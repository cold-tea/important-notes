AGGREGATE FUNCTION -->>
	Return single value and works in a row.
	Eg: min, max, avg etc.
	
SUBQUERIES -->>
	Can be applied on SELECT, WHERE and FROM keywords
	Types
		Single Valued subqueries - Return single value (Can be applied on where = )
		Multiple Valued subqueries - Return multiple values (Can be applied on where in)
		Correlated subqueries - subqueries referencing the table or table alias from enclosing scope
		Multicolumn subqueries - subqueries returning more than one column
		Inline views - query used as a table providing the alias and performing further actions
		
INDEXES -->>
	Scanning table with millions of rows are often time consuming..
	Indexes are those schema objects which increments the performance of sql queries
	The most famous way of indexing is call B-Tree indexing. Where objects are stored in tree structure
	For say if index is create in salary column then the rows from value 1000 - 5000 will be stored in one leaf, 5000-10000 in another leaf and so on.
	Eg:
		CREATE INDEX index_name on table_name(column_name)
	Index referencing more than one column is called composite index.
	If multiple indexes are created for certain table then it will decrease the performance such that the DML operations performed should again recalculate where those value should be placed
	Explaing plan feature can be used to evaluate whether the indexes are being used or not during query execution

DATA DICTIONARY -->>
	USER_{name} - These view contains the info about the objects owned by the current connected user
	ALL_{name} - These view contains the info about the objects owned by the current user and those objects for which the user is given permission
	DBA_{name} - These view contains the info about all objects for all users
	DICT - This view has the metadata regarding all the views available as oracle data dictionary
	V${name} - These view has the dynamic info about various actions and are used primarily by adminstrators
				V$SESSION, V$SQLAREA, V$LOCK, V$INSTANCE etc.
				
TRANSACTIONS -->>
	COMMIT -
		Commit will commit the transactions made
		Rollback will rollback the changes made after last commit
		The record that is being modified in one session and not yet been committed will be locked such that other session cannot modify it
		There is another keyword called savepoint which marks the point of transaction and the rollback can be performed upto that point.
		syntax -
			(queries.......);   savepoint s1;  (queries.......);   rollback to s1;    -- this will rollback the transaction upto the savepoint s1
	
	
SEQUENCES -->>
	Database object used to generate unique objects
	syntax - 
		create sequence seq_name minvalue 1 maxvalue 99999 start with 1 increment by 1 cache 20
		cache will load the given integer into the memory to optimize the performance
	sequence_name.currval - current value in sequence, sequence.nextval - next value in sequence
	

IDENTITY COLUMNS -->>
	These column can be thought as built in columns with embedded sequence
	Types -
		GENERATED ALWAYS AS IDENTITY     -    If custom value is inserted then it will throw error
		GENERATED BY DEFAULT AS IDENTITY    - If custom value is inserted then it will accept .. the value is only inserted as default if not specified
		GENERATED BY DEFAULT AS IDENTITY (START WITH 100 INCREMENT BY 100) 
		
TRIGGERS -->>
	Triggers execute themselves at certain system event
	Types 
		DML
		DDL
		System Events (startup, shutdown, error messages)
		User Events (logon, logoff etc)
	
SYNONYMS -->>
	Synonyms are alternative names for database objects like table, views etc.
	syntax	
		create or replace synonym syn_name for (table_name/view_name etc).
		create or replace public synonym syn_name for (table_name/view_name etc).
		
		
PARTITIONING -->>
	Allows oracle data to be subdivided in small parts
	Tables are split horizontally
	Rows are divided and assigned to certain partitions based upon the condition on certain columns.
	Partitioning helps optimizer to find out in which partition the data reside and omit other partition from scanning.
	Partition vs Index
		If both are used then database optimizer might use index over partition.
	Types of table partitions
		List Partitioning -
			Here we have made four partitions. Values provides the condition analyzing which the records will be assigned to partition
			CREATE TABLE table_name 
			(columns.......)
			PARTITION BY LIST (column_name) (
				PARTITION parname_1 VALUES (...),
				PARTITION parname_2 VALUES (...),
				PARTITION parname_3 VALUES (...),
				PARTITION parname_4 VALUES (DEFAULT)    -- Value not matching any will be assigned here
			);
			
			Splitting existing partition parname_4 into two partitions
			ALTER TABLE table_name SPLIT PARTITION parname_4 VALUES (....)
			INTO (PARTITION parname_5, PARTITION parname_4);
		Range Partitioning -
			Here we have made four partitions. Values provides the condition analyzing which the records will be assigned to partition
			CREATE TABLE table_name 
			(columns.......)
			PARTITION BY RANGE (column_name) (
				PARTITION parname_1 VALUES LESS THAN (<something>),
				PARTITION parname_2 VALUES LESS THAN (<something>),
				PARTITION parname_3 VALUES LESS THAN (<something>),
				PARTITION parname_4 VALUES LESS THAN (<something>) 
			);
			
			Adding new partition
			ALTER TABLE table_name ADD PARTITION parname_5 VALUES LESS THAN (<something>)
		Hash Partitioning - 
			This method of partitioning is used when there is no specific way to partition the table.
			It just analyzes hash value and perform partitioning
			CREATE TABLE table_name 
			(columns......., PARTITION BY HASH (col_name)) PARTITIONS 4;
			
			ANALYZE TABLE table_name COMPUTE STATISTICS;   -- This will analyze the table and assign record to specific partition
			SELECT table_name, partition_name, num_rows FROM user_tab_partitions WHERE table_name = 'TABLE_NAME';
					-- This will list all partitions and provide info like number of rows etc.
		Interval Partitioning - 
		Composite Partitioning - 
		Automatic list Partitioning - 
		

		
------------------------------------------------------------ DATABASE ADMINSTRATION -------------------------------------
		
ARCHITECTURE -->>
	Made up of three parts - 
		Oracle instance
		Oracle database storage
		Oracle server processes (SPs)
		RDBMS
			Instance
				Process
					PMON
					SMON
					DbWn
					Ckpt
					LGWR
					ARCn
					Optional (ARCn, ASMB, RBAL, Others)
				Memort Structure
					SGA
						Data buffer cache
						Shared pool
						Redo log buffer
						Large pool
						Java pool
						Streams pool
					PGA
			Database
				Data files
				Control files
				Redo log files
				Archived redo log files
				Parameter files
				Password file
				Backup files
				Trace files
				Alert log file
				
				
BACKGROUND PROCESSES -->>
	DATABASE WRITER - 
		Writes data from data buffer cache to the disk
		Dirty  buffer = modified
		Cold buffer = not recently
		Writes the both types of buffers to the disk, such the buffer gets free space for other data.
		Can have more than one database writer according to the need.
		DB_WRITER_PROCESSES parameter can be used to configure more writers.
		When ? -- When a server process cannot find a clean reusable buffer, and when oracle needs to advance the checkpoint i.e,
			the oldest dirty buffer from which the database need to recover during failure i.e, in redo log file
			
	LOG WRITER -
		Redo log buffer is the cyclic buffer, 3 in count
		Responsible for writing from redo log buffer to redo log files
		When ? --
			When a user process commits a transaction
			When the redo log buffer is one-third full
			Before a DBWn process writes modified buffers to disk
			Every 3 seconds
			
	CHECKPOINT PROCESS -
		Checkpoint is a database event which synchronizes the modified data blocks in memory with the data files on disk
		It also updates the datafile header and control file with the latest checkpoint System Change Number (SCN)
		When ? --
			At each switch of the redo log files
			Once the number os seconds defined in the LOG_CHECKPOINT_TIMEOUT is reached
			Once the current redo log file reaches the size
				LOG_CHECKPOINT_INTERVAL * size of IO OS blocks
			Directly by the ALTER SYSTEM SWITCH LOGFILE command
			Directly with the ALTER SYSTEM CHECKPOINT command
			
	SMON (System Monitor) PROCESS -
		Performs recovery at the instance startup
		Clears the temporary unused segment
		
	PMON (Process Monitor) PROCESS - 
		Performs process recovery when a user process fails
			Cleans up the buffer cache
			Free resource that are used by the user process
		Restarts stopped running dispatchers and server processes
		Dynamically registers database service with network listeners
		
	RECO (Recoverer) PROCESS - 
		Used with the distributed database configuration
		Automatically resolves all in-doubt transactions
		
	ARCn (Archiver) PROCESS -
		Copy redo log files to a designated storage device after a log switch occurs
		Runs only in ARCHIVELOG mode
		LOG_ARCHIVE_MAX_PROCESS param to configure the number of archiver process
		
		
MEMORY STRUCTURES -->>
	SGA (Shared Global Area) 
		Program code (PL/SQL code, Java code)
		Cached data shared among users
		Redo log entries
		Information about currently connected sessions etc.
		This is shared across multiple users
		
		BUFFER CACHE -
			holds the copies of data blocks that are read from data files
			shared by all concurrent users
			number of buffers defined by DB_BLOCK_BUFFERS
			size of a buffer based on DB_BLOCK_SIZE
			stores the most recently used blocks
			Data must first come in buffer before the user can read it
			
		SHARED POOL -
			Library cache
				Contains statement text, parsed code and an execution plan, Contains one or more sql area
				Shared SQL area
					contains a parse tree and execution plan for a given SQL statement
			Data dictionary cache
				Contains table and column definitions and privileges
			Result cache
				It stores the result of sql query and the plsql query
				If same query is run multiple times then it gives back the same result
			Size of shared pool SHARED_POOL_SIZE param
			
		REDO LOG BUFFER -
			This is a circular buffer which stores the changes made to the database
			DML, DDLs etc..
			Useful during database recovery
			Size is defined by LOG_BUFFER
			
		LARGE POOL -
			This is an optional memory area which provide large memory allocations
			If not defined then will reduce performance as the shared pool is used instead
			Parallel query operations
			Oracle backup and restore operations
			Make heavy operations use large pool instead of shared pool
			
		JAVA POOL and STREAMS POOL - 
			Used to store java code and java object required for java virtual machine
			Size defined by JAVA_POOL_SIZE
			
			Used to Oracle streams to store buffered queue message and provide memory for oracle streams processes
			Size defined by STREAMS_POOL_SIZE
			
		KEEP BUFFER POOL -
			Data which is frequently accessed can be pinned in the keep buffer pool
			In case if buffer cache is full and the data is to be removed will cause the problem if that data is highly used
			To create keep buffer pool DB_KEEP_CACHE_SIZE
			
		RECYCLE BUFFER POOL -
			Recycle pool is reserved for large tables that experience full table scans that are unlikely to be reread or accessed rarely
			Size defined by DB_RECYCLE_CACHE_SIZE
			
		nK BUFFER CACHE -
			Is used to store oracle data blocks having different size then the default block size (8192k)
			Default block size is denoted by DB_BLOCK_SIZE
			Size defined by DB_NK_CACHE_SIZE
			Eg: DB_2K_CACHE_SIZE = 0M, DB_4K_CACHE_SIZE = 0M etc ...
			
		
	PGA (Program Global Area)
		This area of memory is not shared.
		Users have their own PGA to work in their session
		PGA contains private SQL area for each server processes for each user connection
			That private SQL will be stored in Shared SQL area inside library cache checking if all are running similary query
		Its a private memory region containing data and control information for a server process
		
		
DEDICATED VS SHARED SERVERS -->>
	Dedicated servers -
		One server process is spawned for each user connection
		Consumes lots of memory
		Greater performance
	Shared servers -
		Server processes are shared among multiple connections
		It provides scalability
		Here first the user request is handled through Dispatcher process and is stored in the request & response queues. Then from that shared process performs its actions.
		Nobody is using these servers nowadays.
		In shared servers PGA contains only stack space and SGA contains User global areas. Both are in PGA in case of dedicated servers.
		
ORACLE INSTANCE -->>
	Is loaded into the memory each time database starts (In memory nature)
	Contains shared memory caches (Shared pool, buffer cache, redo buffer etc.)
	Contains Oracle's background processes
	Instance will be removed from the memory once it is shut down (non persistent nature)
	
DATABASE STORAGE -->>
	This is the storage which contains oracle files to store the data
	This makes the physical oracle database
	Microsoft word and microsoft word file is analogous to oracle instance and oracle file
	Data storage is usually - Both are natively supported by oracle
		SAN -  Storage Area Netword (Accessed by fibre network)
		NAS -  Netword Attached Storage 
	Data can also be place in local disk (not recommended - single point of failure)	
		
		
SERVER PROCESSES (SPs) -->>
	Server processes are spawned when user is trying to connect to database.
	Each user will have the dedicated server process unless pooling is used with the help of middleware.
	It's role -
		verify the syntax of sql query that the user is trying to execute
		it then read data from disk and also load it in the buffer cache
		
		
LOGICAL AND PHYSICAL DATA STRUCTURES -->>
	Database (L) - Tablespace (L) - Data Files (P) - Segment (L) - Extent (L) - Data block (L)
	Tablespace can contain one or more data files, whereas one datafile can be mapped to only one tablespace
	
	DATA BLOCK -
		A data block is the smallest unit of storage in an Oracle database
		Common and variable header
			It contains the address of block, general block info, type of segment eg. data or index segment
		Table directory
			Info about table having the row in the block
		Row directory
			Info about the row for which the data is stored
		Row data 
			Actual area in which the data is stored
		Free space
		
	TABLESPACE -
		Three types Permanent, UNDO and Temporary
			Permanent for storing info like tables, indexes etc...
			UNDO for the rollback and data consistency during select
			Temporary for storing the temp data like of join, order by etc.
		DEFAULT tablespaces	
			SYSTEM		- Manage the database, data dictionary and other adminstrative objects, cannot take offline, can store user data but strict no
			SYSAUX		- Introduced after 10g, auxillary to system, some of the stuffs stored in system are now in sysaux table space, cannot take offline
			TEMP		- Temporary table space, used to store temporary data, sort , merge, joins etc
			UNDOTBS1	- Undo data
			USERS		- Used to store all the data created by users by default if the tablespace for that user is not allocated
			EXAMPLE		- This schema is created if the scripts file is ran during oracle installation, sample schemas
			
		Online and Offline tablespace -
			Users can connect and read data from is tablespace is online
			Users cannot read write if tablespace is offline, moving a datafile, recover datafile and tablespace, offline tablespace backup etc.
			Tablespace as default will be in online state
			
		Useful -
			select * from dba_data_files;
			select * from dba_tablespaces;
			select tablespace_name, bytes/1024/1024 blocks_in_mb from dba_free_space where tablespace_name = 'TBS1';
			select tablespace_name, file_name, bytes/1024/1024 blocks_in_mb from dba_data_files;
			
			create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M;
				autoextend on to increase the size if the initial size is full
				next 512k is to increase the datafile size by 512 if data file is full
				maxsize is the size till which the tablespace can be extended upto
			create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 1m;
			alter tablespace tbs1 add datafile '/mnt/san_storage/oradata/orcl/data02.dbf' size 5m;  -- adding datafile
			alter tablespace tbs1 drop datafile '/mnt/san_storage/oradata/orcl/data02.dbf';  -- dropping datafile
			drop tablespace tbs1;  -- dropping the tablespace
			drop tablespace tbs1 including contents and datafiles; -- including datafiles and contents
			alter tablespace tbs1 rename to tbs2;  --renaming the tablespace
			
			-- for renaming the datafile first take tablespace to offline
			alter tablespace tbs1 offline;
			alter database rename file '/mnt/san_storage/oradata/orcl/data01.dbf' to '/mnt/san_storage/oradata/orcl/data11.dbf';
				for this, first rename data01.dbf to data11.dbf in filesystem using mv command and then run this command otherwise error
			alter tablespace tbs1 online;
			
			if datafile is full - either of the three
				alter database datafile '/mnt/san_storage/oradata/orcl/data01.dbf' resize 5m;   -- resizing
				alter tablespace tbs1 add datafile '/mnt/san_storage/oradata/orcl/data02.dbf' size 5m; -- adding
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on; -- create tablespace with autoextend
			
			Tablespace with different block size
				Bigger size needed
					Larger tables that are the target for full table scans
					table with larger objects (LOB's, BLOB's, CLOB's)
					temporary tablespace for sorting
					tables with large rows that might lead to chained/migrated rows
				show parameter db_block_size;
				alter system set db_16k_cache_size=60m scope=both; -- both means spfile and memory
				show parameter db_16k_cache_size;
				create tablespace tbs2 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m blocksize 16k;	
		
		TEMPORARY TABLESPACE MANAGEMENT
			Used during joins, sorts and other temporary operations
			select * from dba_temp_files;
			select tablespace_name, file_name, bytes/1024/1024 blocks_in_mb from dba_temp_files;
			alter database tempfile '/mnt/san_storage/oradata/orcl/temp01.dbf' resize 140m; -- resizing default temp tablespace
			create temporary tablespace temp2 tempfile '/mnt/san_storage/oradata/orcl/temp02.dbf' size 20m autoextend on next 1m maxsize 60m;
			select * from database_properties;  -- this will display various database configuration stuffs like defualt temp tablespaces etc..
			alter database default temporary tablespace temp2;  -- sets defualt temp tablespace as temp02
			
		TEMPORARY TABLESPACE GROUP
			Multiple tablespaces can be assigned to a group such that those groups can be assigned to the users
			Group will be create while first tablespace is added and removed when the last tablespace is removed from the group
			create temporary tablespace temp2 tempfile '/mnt/san_storage/oradata/orcl/temp02.dbf' size 20m autoextend on next 1m maxsize 60m
				tablespace group tempgroup1 ; -- cannot be used with other types of tablespaces
			select * from dba_tablespace_groups; -- lists all the groups
			alter tablespace temp2 tablespace group tempgroup1;
			select * from database_properties;
			alter database default temporary tablespace tempgroup1;  -- sets defualt temp tablespace as the group of temp tablespaces
			
		LOCALLY VS DICTIONARY MANAGED TABLESPACE
			In dictionary managed tablespace, all the extent related info is stored in the dictionary for the tablespace
				This is slower approach as the dictionary table is to be queried to fetch the information
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M
					extent management dictionary default storage (initial 50k next 50k minextents 2 maxextents 50); -- only for 10g and before
					
			In dictionary managed tablespace, all the extent related info is stored within the tablespace itself
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M
					extent management local autoallocate; -- must use locally managed 11g onwards
					
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M
					extent management local uniform size 128k; -- must use locally managed 11g onwards
					
					
		UNDO DATA
			It is the copy of pre modified data that is captured for every transaction that changes data.
			Why ?
				Rollback transactions
				Support read consistent queries
				Support flashback operations
				Recover from failed transactions
			How long does undo data stay ?
				User commits the transaction
				User rollbacks the transaction
				User execute DDL statements (create, drop, alter, rename)
				User session terminated abnormally (transaction rollbacks)
				User session terminates normally with an exit (transaciton commits)
			Where does undo data stay ?
				Stored in undo segments/undo tablespace
				Only 1 active tablespace for an instance
				They are owned by the user sys
			With 11g, automatic undo management is the default mode
				UNDO_MANAGEMENT = AUTO / MANUAL; show parameter undo_management;
			An auto extended UNDO tablespace named UNDOTBS1 is automatically created while creating database with DBCA.
			SYSTEM tablespace is used , if no UNDO tablespace is defined.
			UNDO retention period is managed very effectively. UNDO retention period is the number of seconds the committed undo data is retained.
			Only one undo table sapce can be online at a time
			Useful
				alter tablespace undotbs add datafile '/mnt/san_storage/oradata/orcl/undotbs01.dbf' size 5m;  -- adding datafile
				create undo tablespace undotbs02 datafile '/mnt/san_storage/oradata/orcl/undotbs02.dbf' size 10m reuse autoextend on next 512k maxsize 200M;
					reuse option is used to use the datafile if already present
				alter system set undo_tablespace=undotbs2; -- changing the used undo tablespace
				select segment_name, owner, tablespace_name, status from dba_rollback_segs;
			CONFIGURING UNDO RETENTION PERIOD
				UNDO_RETENTION - 1800
				ALTER SYSTEM SET UNDO_RETENTION = 2400
				After a transaction is committed, undo data is no longer needed for rollback. But still it is persisted for consistent read purposes.
				Furthermore, the success of several oracle flashback feature also depend upon the availability of older undo information
				Useful
					show parameter undo_retention;
					alter system set undo_retention=2400;
			RETENTION GUARANTEE
				This guarantee that the data exists for the specified amount of time (UNDO_RETENTION) in the undo tablespace
				If the space is not available in undo tablespace then it will fails any other transaction but still persist the data in the tablespace.
				alter tablespace undotbs1 retention guarantee; -- guarantee the retention (needs the space)
				alter tablespace undotbs1 retention noguarantee; -- doesnot guarantee
				select tablespace_name, retention from dba_tablespaces;  -- check for the retention policy
				
REDO CONCEPTS -->>
	Redo log files enable the oracle server or DBA to redo transactions if a database failure occurs
	Only purpose is to enable recovery
	Redo log entries from buffer is stored in the redo log files	
	Management
		Stores redo log files as a group, in which one group contains many redo log files stored in different disk
		The data is written in circular fashion
		Current log file
			The redo log file to which LGWR is actively writing
		Active log file
			Log files required for instance recovery
			Active log files cannot be overwritten by LGWR until ARCn has archived the data when archiving is enabled
		Inactive log file
			Log files no needed for instance recovery
	Useful
		select * from v$logfile;
		select * from v$log;
		alter database add logfile member '/mnt/san_storage/oradata/orcl/redo01b.log' to group 1;
		alter database add logfile member '/mnt/san_storage/oradata/orcl/redo02b.log' to group 2;
		alter database add logfile member '/mnt/san_storage/oradata/orcl/redo03b.log' to group 3;
			now each group has 2 log files
			by default their status will be invalid as the logwriter has not used them and will be changed once used
			Manually we can switch the logfiles to see the result quickly.   -- alter system switch logfile;
		alter database drop logfile group 3;  -- group 3 log files are deleted, for this the happen the group should be in inactive status
		alter database add logfile group 3 ('/mnt/san_storage/oradata/orcl/redo03.log', '/mnt/san_storage/oradata/orcl/redo03b.log') size 10m reuse;
			add group and the logfile within them dissimilar with the above way of addition in which we already have the group
			reuse option is used to use the datafile if already present
			
			
			
				
---------------ORACLE INSTALLATION PROCESS -----------------------------------------
	Steps:
		i) Edit /etc/hosts file and add 	127.0.0.1	<eg:oel7.lab.local>	<eg:oel7>
			this is to resolve domain from ip and viceversa, run hostname command to resolve what to put in second parameter

		ii) Resolve oracle kernel parameters as well as the group that is to be added for the oracle functioning like groupadd -g oinstall, 				groupadd -g dba, useradd -g oinstall -G dba oracle passwd oracle... This all will be done by below command
				yum install oracle-rdbms-server-12cR1-preinstall
				yum install oracle-database-server-12cR2-preinstall
			group and user are stored in /etc/group and /etc/passwd file respectively. command -->> id oracle --- will also display
		iii) Now create directory where oracle is to be installed. For this login as root user and enter commands
			mkdir -p /u01/app/oracle    --- oracle binary file will be stored here i.e, oracle software
			chown -R oracle:oinstall /u01
			chmod -R 775 /u01

			mkdir -p /mnt/san_storage/oradata --- oracle datafiles, redofiles etc will be stored here i.e, datafiles
		iv) Now change /home/oracle/.bash_profile
			export ORACLE_HOSTNAME=oel7.lab.local i.e, hostname
			export ORACLE_UNQNAME=orcl
			export ORACLE_BASE=/u01/app/oracle
			export ORACLE_HOME=$ORACLE_BASE/product/12.1.0.2/dbhome_1
			export ORACLE_SID=orcl
			export PATH=/usr/sbin:$PATH
			export PATH=$ORACLE_HOME/bin:$PATH

		v) Now set SELINUX to be permissive, this will print warning instead of enforcing security
			/etc/selinux/config     SELINUX=permissive
			run command -->> setenforce permissive ---- to immediately show the effect otherwise restart to see the change
		vi) If network firewalls are up and running stop them so they don't interfere
			service iptables stop
			chkconfig iptables off
		vii) Create the folder where oracle is to be downloaded
			With root user - 
				mkdir -p /orasoft
				chown -R oracle:oinstall /orasoft

	After installation --
		Creating a database instance
			Goto dbhome/bin folder and run "netca"
			Go on and add the listener, default port of 1521 can be used as the listener's port
			After finishing it -
				run 'lsnrctl' command, after that 'status' command will show what listener is currently running
		Creating a database
			Database can also be created in manual for which we have to create everything like spfile (parameters) in dbs folder,
				database itself (
					create database orcl
					darafile '..../example.dbf' size 300m autoextend on
					sysaux datafile '............' ......... on
					default tablespace user_data datafile '............' 
					undo tablespace undotbs datafile '.....................'
					logfile group 1 '......./redo1.redo' size 100m,
							group 2 '......../redo2.redo' size 100m;
				) save this  sql in some location, then connect to idle instance of database with admin and then startup nomount
				  after this run that sql file and all other sql files, 
				datafiles etc...
				This method is hectic , so prefer below options
			Goto dbhome/bin folder and run "dbca"       //// In case of upgrade run "dbua"
			While creating database select advanced mode and do accordingly
			For simplicity Create as container database is not checked, if checked then pluggable database will be created too if selected accordingly
			Oracle enterprise manager is the web interface provided through which we can manage our database as administrator
			During the setup of recovery area checking the archiving will archive the redo logs automatically
			Storage area can be setup which must be the secured one
			After filling up all the required details, the database creation and the instance creation step will begin.
		view /etc/oratab file for all the services running in the host
			orcl:/u01/app/oracle/product/12.2.0/dbhome_1:N, here N means the instance will not run automatically after reboot
			
	
		
		
		
---------------- ORACLE LINUX FIXING SWAP SIZE ----------------------------------------------------------------

	1 check what swap size
	#swapon –s
	Filename                                Type            Size    Used    Priority
	/dev/xvda3                              partition       2097144 0       -1
	  
	2 Create a file that you’ll use for swap with dd command with   as root
	dd if=/dev/zero of={/swapfile path} bs={size of swap}  count=1048576
	Example:-
	[root@localhost]#dd if=/dev/zero of=/home/swapfile bs=6048 count=1048576
	Note :- Above command will be create the 6Gb swapfile on /home location as  swapfile
	3 Set up a Linux swap area
	#mkswap /home/swapfile
	4: Enabling the swap file
	#swapon /home/swapfile
	#swapon –a
	5 status of add swap
	[root@myrem12c em12cBP1]# swapon -s
	Filename                                Type            Size    Used    Priority
	/dev/xvda3                              partition       2097144 0       -1
	/home/swapfile                          file            6097144 0       -2
	6 Update /etc/fstab   (this must to be done )
	#vi /etc/fstab
	/home/swapfile        none             swap   sw      0 0  
	Note  :- Add above line ending with  the file otherwise ones you restart the server swap partition not be mounted to the system

	Then retry the installation
		
		
		
---------------------- USEFUL LINUX COMMANDS (CENTOS) --------------------------------	
	
	wget http://get.geo.opera.com/pub/opera/linux/1216/opera-12.16-1860.x86_64.rpm
	lsnrctl status;	lsnrctl start;	lsnrctl stop;
	lsnrctl > status; lsnrctl > start; lsnrctl > stop;
	via sqlplus >
		start and stop the instance -
			alter system register;   >> force instance to connect to the listener automatically
			shutdown - nomount - mount - open
			startup
				nomount - 
					only instance is started, not connected to the database, used for database creation and recovery purpose
				mount -
					taking the instance and associating with the database
					only the dba can access database at this stage, no user can access, still closed
					for enabling and disabling archive log mode
					for renaming data files etc.
					users cannot access the database at this stage
					adding, dropping, renamind redo log files
					performing database recovery
				open -
					all users can access the database
			shutdown
				immediate -
					terminate all executing sql query, and disconnects the user
					uncomitted changes are rollbacked
					issues the checkpoints and closes all the databases
				transactional -
					waits for all the transactions to complete
					prevents user from starting new transactions and creating sessions
					oracle then perform checkpoint, disconnects all the users and close
				normal -
					wait for all transactions to complete
					can start new transactions by the connected user but doesnot allow new user to connect
					coloses the database at the end after performing checkpoint
				abort -
					everything will be stopped
					instance recovery should be performed later

		parameters related -
			show parameter sga_target;
			show parameter sga_; >> list all the params with sg_ text as substring
			alter system set sga_target=1000m scope=memory/spfile/both;  >> memory -- cleared after restart, spfile -- stored in spfile reflects after restart, both -- both
			create pfile='/home/oracle/my_pfile.ora' from spfile;    >> this will copy all the parameters from spfile so that we can view it
			Parameter file are of two type :
				pfile and spfile, pfile is stored as string whereas spfile is stored as binary
				pfile -> $ORACLE_HOME/dbs/init.ora
				spfile - > $ORACLE_HOME/dbs/spfile[sid].ora
		create user and permissions -
			create user user_name identified by password;   >> if connected then displays create session privilege error
			grant create session to user_name;    >> if create table is executed - insufficient privilege
			grant unlimited tablespace, create table to user_name; 
			drop user user_name;   >> only if the schema does not owns anything otherwise
			drop user user_name cascade;
			create role role_name;
			grant (roles.....) to role_name;
			grant role_name to user_name;
		data dictionary -
			v$logfile;    >> view all redo log files
			dba_data_files;    >> view data files, <file_name, tablespace_name> shows the datafile belonging to tablespace
			v$controlfile;		>> view control file.  >> if multiplexing is enabled then it will be stored in multiple places
			v$session;
			dba_tablespaces;
		troubleshoot using alert log -
			It contains all warnings, errors generated by oracle
			cd $ORACLE_BASE/diag/rdbms/orcl/orcl/trace   >> here alert_orcl.log
		storage - 
			tablespace is a collection of datafiles used as the storage
			dba_tablespaces -
				system - it is the tablespace where oracle data dictionary are stored.
			dba_data_files;    >> view data files, <file_name, tablespace_name> shows the datafile belonging to tablespace	
			create tablespace tbspc_name datafile '/mnt/san_storage/oradata/dfile01.dbf' size 20m autoextend on;
			drop tablespace tbspc_name including contents and datafiles;
		control files -
			The database control file maintains information about the physical structure of the database
			The control file is critical to recovering a data file because the control maintains the System Control Number (SNC) for each data file
			Multiplexing control file will eradicate the single point of failure issue
			CREATE CONTROLFILE is used to manually create a controlfile
			ALTER DATABASE BACKUP CONTROL FILE TO TRACE for backup
		
		
		
		
	