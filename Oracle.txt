------------------------------------------------------------ DATABASE BASIC -------------------------------------
ORACLE DATA TYPES -->>
	Everything is varchar2
		This mind set is injurious
		It will degrade the performance of database
		Doing so, the feature specific to certain data types won't be utilized
	DUMP()
		Dump function if applied to any column in a table will display the bytes occupied by the data type
		It will also display each alphabet or digits in ASCII format
	Character data types
		Char and NChar are space padded if the size specified is not reached	
		Varchar and Nvarchar are of varying length	
		One character takes one byte of storage
		Varchar2 is best to use then Char
		Char, Varchar2 are stored in the default character set of the database
		Nchar, NVarchar2 are stored in the national character set of the database (Unicode)
		Finding character set
			select * from nls_database_parameters where parameter in
			('NLS_CHARACTERSET', 'NLS_NCHAR_CHARACTERSET');
			NLS_CHARACTERSET = AL32UTF8 -- default character set ( 1 byte per character)
			NLS_NCHAR_CHARACTERSET = AL16UTF16 -- national character set ( 2 byte per character)
		Varchar vs Varchar2
			At present if varchar is used then it will be directly converted to varchar2
			Both are same at the present
			Varchar is in experimentation and will soon change in future
		Raw data type
			Store row binary data (array of bytes)
			raw(64) -- 64 bytes can be stored in given example
	Number data types
		Number(p,s)
			-- p (precision), it means the total number of digits in the number
			-- s (scale), digits to the right of the decimal place
		Example
			123456.78 for number is correct
			123456.78 for number(6) is correct. Result = 123457
			123456.78 for number (8,2) is correct
			123456.78 for number(7,2) is incorrect. Error
			123456.78 fro number(8,3) is incorrect. Error
		Number type aliases
			int -> number(38)
			smallint -> number(38)
			decimal(p,s) -> number(p,s)
			float -> float(126)
			real -> float (63)
		Binary float and Binary double
			Utilize hardware acceleration when performing calculations
			Governed by IEEE 754 compliant
			Can store very small or large value than number data type
			In performance it can out perform number data type
			Not suitable for financial calculations
			Not precise in comparision with number data types
			Can store infinity --> while inserting use 12f/0 OR 12d/0 like this syntax
	Date and Time data types
		Usage
			Date - Date/time values with precision of one second
			Timestamp(n) - Date/time values with upto n digits of fractional second precision, if not specified default to 6
			Timestamp(n) with timezone - same as timestamp but also supports timezone
			Timestamp(n) with local timezone - Convert the time acc to the local timezone specified
		Formatting date and timestamp
			to_char(date, 'format-string') as formatted_date
			String in the date format
				to_char(date, 'MONTH DD, YYYY "at" HH24:MI') as formatted_date
		Extract function
			This function can be used either with date or timestamp value
			Example
				extract (year from systimestamp) as year
				extract (month from systimestamp)as month
				extract (day from systimestamp) as day
				extract (hour from systimestamp) as hour...
		Getting current date and time
			sysdate - current date and time (server time zone)
			current_date - current date/time in the session (session time zone)
			systimestamp - similar to sysdate but contains fractional seconds
		Functions for use with dates and times
			trunc(sysdate) - returns date only, omits time
			last_day(date) - date i.e, last day of the month
			next_day(date, 'FRIDAY') as my_date - returns date of next friday
			sysdate + 1 -> add 1 day to the current sysdate i.e, tomorrow
			sysdate - 0.5 -> subtract half a day from current date
			add_months(date, num_of_months)
			select months_between(date '2014-4-08', date '2001-08-24') as diff from dual; -- 151.48
		Interval date types
			Stores period of time
			Types
				Interval Year to Month & Day to Second
					Interval Year(<<precision>>) to month
					Interval day(<<precision>>) to second(<<precision>>)
					precision by default is 2 i.e, can store data of 99 years 
					Eg (year to month literal):
						employement_length interval year(2) to month -- this is column in some table
						(employement_length) values(INTERVAL '4-3' year(2) to month)
					Eg (day to second literal)
						values (INTERVAL '<days> <hh>:<mi>:<sec>.<ff>' day (<precision>) to second (<precision>)
	Large Object data types
		Three types
			CLOB		-- Character data in the default character set (can store very long); like extended form of varchar2 data type
			NCLOB		-- Character data in the national character set; like extended form of varchar2 data type
			BLOB		-- Binar data (images, documents, etc.)
		Tables can contain multiple LOB columns
		Maximum size = (4 gigabytes - 1) * (database block size of the tablespace in which this lob column belong to)
		Cannot be part of an index
		Cannot be used in order by, group by or aggregate functions
		Oracle LONG and LONG RAW data types are also present but are deprecated since Oracle 8i
		How data is stored
			lob column generally stores a pointer to a LOBINDEX
			data is stored in LOBSEGMENT 
				LOBSEGMENT contains many chunks which contains data and LOBINDEX points to those chunks
				LOBSEGMENT can also be stored in the entirely different tablespace
		Creating	
			Create table tbl_name (
				.......,
				syllabus blob null
			)
			lob (syllabus) store as [securefile|basicfile] (
				tablespace tblspc_name
				disable storage in row
				chunk 8192
				nocache
				compress high
			)
			lob...........;
		Securefile vs Basicfile
			Basicfile -  Original storage implementation for large objects
			Securefile - Offers additional features like data de-duplication and data compression
				Advanced compression requires license to use it
				select * from v$parameter where name = 'db_securefile';
					NEVER - using Securefile will result to an error
					PERMITTED - LOBs are allowed to be created as Securefile
					PREFERRED - LOBs are created as securefile by default (12c)
					ALWAYS - Always created as Securefile
					IGNORE - The Securefile keyword is ignored
				LOB file must be stored in tablespace with ASSM (automatic segment space management) to use securefile option
		Storage in row option	
			enable storage in row
				first 4000 bytes stored inline with row data and remaining data separately in LOBSEGMENT
				need not use LOBINDEX for lookup
		Chunk options
			smallest unit of data that Oracle works with for LOB values
			must be multiple of the tablespace data block size
			if greater chunk size is used then the space might get wasted if the data cannot fill the chunk
		Data dedeplication
			available only if Securefile is used
			used with advanced compression
			if data are same then won't be stored in separately (one copy is stored)

AGGREGATE FUNCTION -->>
	Return single value and works in a row.
	Eg: min, max, avg etc.
	
SUBQUERIES -->>
	Can be applied on SELECT, WHERE and FROM keywords
	Types
		Single Valued subqueries - Return single value (Can be applied on where = )
		Multiple Valued subqueries - Return multiple values (Can be applied on where in)
		Correlated subqueries - subqueries referencing the table or table alias from enclosing scope
		Multicolumn subqueries - subqueries returning more than one column
		Inline views - query used as a table providing the alias and performing further actions
		
INDEXES -->>
	Scanning table with millions of rows are often time consuming..
	Indexes are those schema objects which increments the performance of sql queries
	The most famous way of indexing is call B-Tree indexing. Where objects are stored in tree structure
	For say if index is create in salary column then the rows from value 1000 - 5000 will be stored in one leaf, 5000-10000 in another leaf and so on.
	Eg:
		CREATE INDEX index_name on table_name(column_name)
	Index referencing more than one column is called composite index.
	If multiple indexes are created for certain table then it will decrease the performance such that the DML operations performed should again recalculate where those value should be placed
	Explaing plan feature can be used to evaluate whether the indexes are being used or not during query execution

DATA DICTIONARY -->>
	USER_{name} - These view contains the info about the objects owned by the current connected user
	ALL_{name} - These view contains the info about the objects owned by the current user and those objects for which the user is given permission
	DBA_{name} - These view contains the info about all objects for all users
	DICT - This view has the metadata regarding all the views available as oracle data dictionary
	V${name} - These view has the dynamic info about various actions and are used primarily by adminstrators
				V$SESSION, V$SQLAREA, V$LOCK, V$INSTANCE etc.
	Useful
		show errors;
		select * from user_errors;
				
TRANSACTIONS -->>
	COMMIT -
		Commit will commit the transactions made
		Rollback will rollback the changes made after last commit
		The record that is being modified in one session and not yet been committed will be locked such that other session cannot modify it
		There is another keyword called savepoint which marks the point of transaction and the rollback can be performed upto that point.
		syntax -
			(queries.......);   savepoint s1;  (queries.......);   rollback to s1;    -- this will rollback the transaction upto the savepoint s1
	
	
SEQUENCES -->>
	Database object used to generate unique objects
	syntax - 
		create sequence seq_name minvalue 1 maxvalue 99999 start with 1 increment by 1 cache 20
		cache will load the given integer into the memory to optimize the performance
	sequence_name.currval - current value in sequence, sequence.nextval - next value in sequence
	

IDENTITY COLUMNS -->>
	These column can be thought as built in columns with embedded sequence
	Types -
		GENERATED ALWAYS AS IDENTITY     -    If custom value is inserted then it will throw error
		GENERATED BY DEFAULT AS IDENTITY    - If custom value is inserted then it will accept .. the value is only inserted as default if not specified
		GENERATED BY DEFAULT AS IDENTITY (START WITH 100 INCREMENT BY 100 CACHE 20) 
		
INVISIBLE COLUMNS -->>
	Invisible columns are not displayed by default. Treats the table as if the column does not exist at all
	Usage
		CREATE TABLE products ( prod_id INT, count INT INVISIBLE );
		ALTER TABLE products MODIFY ( count VISIBLE );
		
COMPRESSION -->>
	SELECT SEGMENT_NAME, BYTES/1024/1024 SIZE_MB FROM USER_SEGMENTS;
	SELECT TABLE_NAME, COMPRESSION, COMPRESS_FOR FROM USER_TABLES;
	By default table are not compressed
	Explicit keyword should be given to indicate table compression
	
	Basic
		For basic compression to work properly table should be created with direct load
		DML operation should be minimal or the table should be at read only for it to perform better
		CREATE TABLE DEMO_COMPRESS_ COMPRESS BASIC AS SELECT * FROM DEMO_NOCOMPRESS; -- direct load of the oracle table
		CREATE TABLE DEMO_COMPRESS_CONVENTIONAL COMPRESS BASIC AS SELECT * FROM DEMO_NOCOMPRESS WHERE 1=0; -- conventional table creation
		INSERT INTO DEMO_COMPRESS_CONVENTIONAL SELECT * FROM DEMO_NOCOMPRESS; 
				-- entering data into the table previously created (size might be greater compared to above)
		CREATE TABLE DEMO_COMPRESS (.........) COMPRESS;
		CREATE TABLE DEMO_COMPRESS (.........) COMPRESS BASIC;
		CREATE TABLE DEMO_COMPRESS (.........) ROW STORE COMPRESS BASIC;
		ALTER TABLE DEMO_COMPRESS COMPRESS; -- won't compress already present data
		ALTER TABLE DEMO_COMPRESS NOCOMPRESS;
		ALTER TABLE DEMO_COMPRESS MOVE COMPRESS; -- if table contains uncompressed data then it will compress those too
		ALTER TABLE DEMO_COMPRESS MOVE NOCOMPRESS;
	Advanced
		This supports often DML operations in the table as well
		CREATE TABLE DEMO_COMPRESS (.........) ROW STORE COMPRESS ADVANCED;
		CREATE TABLE DEMO_COMPRESS_OLTP COMPRESS FOR OLTP AS SELECT * FROM DEMO_NOCOMPRESS WHERE 1=0;
		CREATE TABLE DEMO_COMPRESS_OLTP_DIRECT COMPRESS FOR OLTP AS SELECT * FROM DEMO_NOCOMPRESS;
	Findings
		Uncompressed table size -- 63mb
		For basic direct and conventional I got the same size -- 57mb
		For oltp direct and conventional and advanced I got the same size -- 63mb
		
TABLE CLUSTERS -->>
	A table cluster is a group of tables that share common columns and store related data in the same blocks
	When tables belong to the cluster then the segment will only be created in the name of cluster
	Data of those tables will be stored in the cluster segment
	Check these
		SELECT * FROM USER_SEGMENTS; -- segment name and type of segment
		SELECT * FROM USER_TABLES; -- if the table belong to cluster
		SELECT * FROM USER_CLUSTERS; -- view the cluster
	To perform required operations to create clusters and tables
		i) Create a cluster with a key
			CREATE CLUSTER CL_EMP_DEPT (DPT_ID NUMBER(2));
		ii) Create an index on cluster. This is for the access of table rows
				-- Rows are retrieved either with index or full table scan
				-- If tables shares segment the full table scan won't work
				-- That's why index should be created
				CREATE INDEX IDX_CL_EMP_DEPT ON CLUSTER CL_EMP_DEPT;
		iii) Create tables
				CREATE TABLE DEPT (DEPT_ID NUMBER(2), DEPT_NAME VARCHAR2(40)) CLUSTER CL_EMP_DEPT (DEPT_ID);
				CREATE TABLE EMP (EMP_ID NUMBER(4), EMP_NAME VARCHAR2(100), DEPT_ID NUMBER(2)) CLUSTER CL_EMP_DEPT (DEPT_ID);
				-- Rowid of both dept and emp having same dept no will be same
	Reduces disk access as the data is stored in the same block
	While performing joins, if tables present in joins are clustered then the query will work quick
	Save space as the cluster key won't be stored repeatedly
	Not beneficial if too much DML will ran in the table
	NOTE:: Also read about hash clusters, this can also be used instead of index clusters
	
ATTRIBUTE-CLUSTERED TABLES -->>
	An attribute-clustered table is a heap-organized table that stores data in close proximity on disk based on user-specified clustering directives
	It is only available for direct path load not for conventional DML loading
	Types
		i) The CLUSTERING ... BY LINEAR ORDER directive orders data in a table according to specified columns
		ii) The CLUSTERING ... BY INTERLEAVED ORDER directive orders data in one or more tables using a special algorithm, similar to a Z-order function
	Usage
		Problem
			CREATE TABLE T1(C1 NUMBER, C2 DATE);
			INSERT /*+ APPEND */ INTO T1 SELECT ABS(MOD(DBMS_RANDOM.RANDOM,1000000)), SYSDATE FROM DUAL CONNECT BY LEVEL < 1000000;
			CREATE INDEX IC1 ON T1(C1);
			SELECT BLOCKS FROM USER_TABLES WHERE TABLE_NAME='T1'; --let 2536
			SELECT INDEX_NAME, NUM_ROWS, CLUSTERING_FACTOR FROM USER_INDEXES WHERE INDEX_NAME='IC1'; 
				-- here clustering_factor will be too high nearly equal to no. of rows
			SET AUTOTRACE TRACE EXPLAIN
			SELECT * FROM T1 WHERE C1 BETWEEN 1 AND 100000;
				-- table access full will be used as the index won't matter because of greater branching factor
			SELECT /*+ INDEX(T1 IC1) */ * FROM T1 WHERE C1 BETWEEN 1 AND 100000;  
				-- forceful use of index will be deplete the cost further more than above
		Solution
			CREATE TABLE T1(C1 NUMBER, C2 DATE) CLUSTERING BY LINEAR ORDER (C1);
				-- this will place similar records nearer in the disk
				-- reduces the clustering factor tremendously
	Dig more about others
	
TEMPORARY TABLES -->>
	A temporary table holds data that exists only for the duration of a transaction or session
	Useful for the buffering of records
	Table in the session is only private to them
	The data need not to be persisted
	Another session cannot view the data of another session
	No overhead of maintaining redo and writing to data blocks
		-- redo recovery is not possible for temporary table
		-- data is stored in the temporary tablespace
	Rows visible
		i) Till the end of transaction -> When commit is performed then data is removed from temp table
		ii) Till the end of session -> When user logs out
	Global temp table is visible to all session but the data in it is not visible to others
	Indexes can be created. Indexes thus created are also temporary
	View or Trigger can also be created in the temporary tables
	Useful in data guard
		TEMP_UNDO_ENABLED (False in Primary, True in Standby)
		if false - stored in undo tablespace
		if true - stored in temp tablespace
		if stored in temp tablespace then redo need not to be generated
		DML and calculations can also be performed in temp tablespace
		as the standby database is readonly, DML can only be performed in temp tablespace, useful for reporting
	Usage
		CREATE GLOBAL TEMPORARY TABLE DEMO_GLOBAL_TEMP (ID INT, NAME VARCHAR2(100)) ON COMMIT PRESERVE ROWS;
		CREATE GLOBAL TEMPORARY TABLE DEMO_GLOBAL_TEMP_DL (ID INT, NAME VARCHAR2(100)) ON COMMIT DELETE ROWS;
		
		
TRIGGERS -->>
	Triggers execute themselves at certain system event
	Types 
		DML
		DDL
		System Events (startup, shutdown, error messages)
		User Events (logon, logoff etc)
	
SYNONYMS -->>
	Synonyms are alternative names for database objects like table, views etc.
	syntax	
		create or replace synonym syn_name for (table_name/view_name etc).
		create or replace public synonym syn_name for (table_name/view_name etc).
		
		
PARTITIONING -->>
	Allows oracle data to be subdivided in small parts
	Tables are split horizontally
	Rows are divided and assigned to certain partitions based upon the condition on certain columns.
	Partitioning helps optimizer to find out in which partition the data reside and omit other partition from scanning.
	Partition vs Index
		If both are used then database optimizer might use index over partition.
	Types of table partitions
		List Partitioning -
			Here we have made four partitions. Values provides the condition analyzing which the records will be assigned to partition
			CREATE TABLE table_name 
			(columns.......)
			PARTITION BY LIST (column_name) (
				PARTITION parname_1 VALUES (...),
				PARTITION parname_2 VALUES (...),
				PARTITION parname_3 VALUES (...),
				PARTITION parname_4 VALUES (DEFAULT)    -- Value not matching any will be assigned here
			);
			
			Splitting existing partition parname_4 into two partitions
			ALTER TABLE table_name SPLIT PARTITION parname_4 VALUES (....)
			INTO (PARTITION parname_5, PARTITION parname_4);
		Range Partitioning -
			Here we have made four partitions. Values provides the condition analyzing which the records will be assigned to partition
			CREATE TABLE table_name 
			(columns.......)
			PARTITION BY RANGE (column_name) (
				PARTITION parname_1 VALUES LESS THAN (<something>),
				PARTITION parname_2 VALUES LESS THAN (<something>),
				PARTITION parname_3 VALUES LESS THAN (<something>),
				PARTITION parname_4 VALUES LESS THAN (<something>) 
			);
			
			Adding new partition
			ALTER TABLE table_name ADD PARTITION parname_5 VALUES LESS THAN (<something>)
		Hash Partitioning - 
			This method of partitioning is used when there is no specific way to partition the table.
			It just analyzes hash value and perform partitioning
			CREATE TABLE table_name 
			(columns......., PARTITION BY HASH (col_name)) PARTITIONS 4;
			
			ANALYZE TABLE table_name COMPUTE STATISTICS;   -- This will analyze the table and assign record to specific partition
			SELECT table_name, partition_name, num_rows FROM user_tab_partitions WHERE table_name = 'TABLE_NAME';
					-- This will list all partitions and provide info like number of rows etc.
		Interval Partitioning - 
		Composite Partitioning - 
		Automatic list Partitioning - 
		--- THESE ARE ALL DISCUSSED DETAIL IN OPTIMIZATION SECTION ---
		
DIRECTORY -->>
	create directory dir_name as 'location';
		directory cannot be renamed
	any LOB file with size greater than 4000 byte will be stored in directory
	
IMPORT AND EXPORT -->>
	This is solely for SQL developer
	For import and export using different file format use tools like SQL DEVELOPER. Which contains some built in feature to support
	this task.
	Hints can be used for this purpose
		SELECT /*csv*/ * FROM scott.emp;
		SELECT /*xml*/ * FROM scott.emp;
		SELECT /*html*/ * FROM scott.emp;
		SELECT /*delimited*/ * FROM scott.emp;
		SELECT /*insert*/ * FROM scott.emp;
		SELECT /*loader*/ * FROM scott.emp;
		SELECT /*fixed*/ * FROM scott.emp;
		SELECT /*text*/ * FROM scott.emp;
	Simply set sqlformat variable and the query output will be in desired format
		SET SQLFORMAT csv;
		SELECT * FROM scott.emp; -- Press f5 to run as script output
		SET SQLFORMAT; -- this will transit in the default format
	
	
SQL LOADER -->>
	It is used to load the content of file in the table
	Data is loaded permanently
	Control file is the heart of the sql loader
		OPTIONS (ERRORS=10, LOAD=100, SILENT(HEADER,FEEDBACK),SKIP=1)
		TRUNCATE - flush all data and load in it
		REPLACE - delete the data first and then insert
			replace and truncate are similar but truncate is more efficient
		INSERT - expects an empty table and generate error if data is already present
		APPEND - starts appending data in the table
			
	Useful (eg.)
		control file -
			OPTIONS (SKIP=1) -- skip the header i.e, first row (if not required the remove this line)
			LOAD DATA
			INFILE 'student.csv' -- name of the text file containing data (if this is in sqlldr as data=student.csv the this infile line can be omitted)
			APPEND  -- append , truncate
			INTO TABLE STUDENT
			fields terminated by ","
			(
				id,
				name 
			)
			
			
			OPTIONS (SKIP=1)
			LOAD DATA          -- here infile is not mentioned as data param is mentioned through sqlldr
			TRUNCATE
			INTO TABLE STUDENT
			WHEN ID <> '5000'
			FIELDS TERMINATED BY ","
			(
				id,
				name
			)
			
			
			OPTIONS (SKIP=1)
			UNRECOVERABLE		-- the history won't be written in the redo log files, beware while using it, perform immediate backup, similar to NOLOGGING
			LOAD DATA       
			INFILE 'student.csv'
			BADFILE 'student_bad.csv'
			DISCARDFILE 'student_discard.dsc'    -- here all params is mentioned so only control=student.ctl can be mentioned in sqlldr      
			TRUNCATE
			INTO TABLE STUDENT
			WHEN ID <> '5000'
			FIELDS TERMINATED BY ","
			TRAILING NULLCOLS	-- treat the field if not present in data file at the end as null
			(
				id,
				name "trim(:name)"
			)
			
			inline data (i.e, data is in the control file itself)
			LOAD DATA       
			INFILE *   
			TRUNCATE
			INTO TABLE STUDENT
			(
				id sequence,
				name position(1:6) char, -- similar to name position(1) char(6)
				price position(7:9) integer external, -- price position(*) integer external(3) -- follows above column
				item_loc position(8:13) char
				the_date sysdate
			)
			begindata
			table 100njdemo
			sofa  200ikdemo
			chairs50 nydev
			
			
			for fixed length file
			(
				id position(1:6) char(100),
				name position(7:10) char "UPPER(:item_name)",
				ins_date sysdate
			)
			
		calling through terminal
			sqlldr userid=sandesh/sandesh control=student.ctl data=student.csv bad=student_bad.bad log=student_log.log
			sqlldr userid=sandesh/sandesh control=student.ctl data=student.csv bad=student_bad.bad log=student_log.log discard=student_discard.dsc
				--either provide data, control, bad, discard file in sqlldr command or in the control file
	
EXTERNAL TABLES -->>
	Data won't be stored inside datbase
	Data is stored in files
	Similar concept as sqlloader is brought in the external table
		Control file is not present here like in sqlloader
	Data is not loaded permanently like sql loader
	Directory name, datafile, field seperator and record seperator is required for this
	Directory student_directory is created and the read write access was given to the user
	Data Pump, Oracle Loader, Oracle HDFS, Oracle Hive are the access drivers used in external table
		Oracle HDFS is to load data from Hadoop DFS
		Oracle Hive is to load data from Apache hive database
	USING DATA PUMP
		Loading
			create table student_data_pump1
			(
				stu_id number,
				stu_name varchar2(200),
				stu_address varchar2(200)
			)
			organization external
			(
			type oracle_datapump
			default directory student_directory
			access parameters (   -- can skip access parameters section it is optional
				logfile another_dir: 'student.log'
				badfile 'student.bad' -- in the default directory
			)
			location ('student_pump.dmp'));
		Unloading
			create table student_data_pump2
			organization external
			(
			type oracle_datapump
			default directory student_directory
			access parameters (
				encryption enabled
				compression enabled
				logfile 'file.log'
			)
			location ('student_pump.dmp'))
			parallel 2  -- this means the degree, must mention two files in location section two use this otherwise default 1 is used
			as select * from student;
			
			here dmp file is created, student_data_pump table is created which points to the student_pump.dmp file
				if dump file is deleted then the table will be invalid, it won't display any data
		
	USING ORACLE LOADER
		create table student_external
		(
			id number, name varchar2(100) -- this list need not contain every column specified in below optional field section
		)
		organization external   -- specifies that it is external table
		(
		type oracle_loader  -- datapump can be used if the data is to be exported to file from the table (example mentioned above)
		default directory student_directory   -- default directory, if not specified in location() then this dir will be used
		access parameters
		(
		records delimited by newline
		badfile 'ext_student.bad'
		discardfile 'ext_discard_student.dsc'
		logfile 'ext_student.log'
		skip 1
		fields terminated by ','
		-- load when (id = 1000) -- not to load that id with value of 1000
		--reject rows with all null fields
		lrtrim -- trim the space if specified as char(4000) in below fields section to only insert required value in above table created
		missing field values are null
		(id, name)   -- optional (fields)
		)
		location ('student.csv')      -- default directory is scanned, for using other explicitly use location(another_dir_name:'student.csv')
		)
		reject limit unlimited  -- optional
		;
	
		records delimited and field terminated are compulsary
		NOTE: remove all those comments inside query otherwise it will throw error (data catridge error)
		
		
		
		if in external table more column is required than that listed in file then we can perform column transform
		create table student_external
		(
			id number, name varchar2(100), name_dummy varchar2(200), dummy_const
		)
		organization external
		(
		type oracle_loader
		default directory student_directory
		access parameters
		(
		records delimited by newline
		badfile 'ext_student.bad'
		discardfile 'ext_discard_student.dsc'
		logfile 'ext_student.log'
		skip 1
		fields terminated by ','
		load when (id = 1000)
		--reject rows with all null fields
		lrtrim
		missing field values are null
		(id, name),
		column transforms -- in above external table defn we have more columns
		(
			name_dummy from concat (name, ' DUMMY_VAL '),
			dummy_const from constant '1'
			-- another_col_name from lobfile (column_in_fields_section) from dir_name -- this will populate from mentioned lob file
		)
		)
		location ('student.csv')
		)
		reject limit unlimited
		;
		
		
TABLE LOCKS -->>
	Types - DML, DDL, Internal
	Shared locks are shared and can be acquired by multiple users at a time whereas exclusive locks can only be acquired by a single user
	Important
			S	X
		S	yes yes
		
		X	yes no
		Exclusive exclusive is not allowed
		This means: reader won't block reader, reader won't block writer, writer won't block reader but writer will block writer
	Lock modes - Share / Row share, Share update, Exclusive / Row Exclusive / Share Row Exclusive
		Share / Row share - Multiple users
		Share update - Multiple users
		Exclusive / Share Row Exclusive - Single user (specifify NOWAIT such that it won't wait for lock if already locked by other users)
		Row Exclusive - Multiple users
	Row level implicit locks
		These are the implicit locks automatically placed by database
		When one session performs any DML in one row then other session and users cannot modify it unless committed or rollbacked
	Explicit locks
		Different lock modes are used for the explicit locks
		Those modes are used for explicitly locking the tables in different way
	Practical
		lock table student in share mode; 
				-- can be acquirred by multiple users
				-- if the lock is acquired by only one user then it can modify the table. Other user cannot modify
				-- if the lock is with more than one then no one can modify the table only read is possible
		lock table student in share update mode;
				-- can be acquirred by multiple users
				-- if the lock is with more than one then also the table can be modified, but same row cannot me modified 
		lock table sandesh.student in exclusive mode nowait;
				-- can be acquirred by only one user
				-- if any type of lock is present with that object then exclusive lock cannot be made
				-- other user can read the data but cannot perform any manipulation
		lock table sandesh.student in row exclusive mode;
				-- can be acquirred by only one user
				-- if the lock is with more than one then also the table can be modified, but same row cannot me modified 
				
		SHARE
			SHARE permits concurrent queries but prohibits updates to the locked table.
		ROW SHARE
			ROW SHARE permits concurrent access to the locked table but prohibits users from locking the entire table for exclusive access. 
			ROW SHARE is synonymous with SHARE UPDATE, which is included for compatibility with earlier versions of Oracle Database.
		SHARE UPDATE
			See ROW SHARE.
		EXCLUSIVE
			EXCLUSIVE permits queries on the locked table but prohibits any other activity on it.
		ROW EXCLUSIVE
			ROW EXCLUSIVE is the same as ROW SHARE, but it also prohibits locking in SHARE mode. 
			ROW EXCLUSIVE locks are automatically obtained when updating, inserting, or deleting.
		SHARE ROW EXCLUSIVE
			SHARE ROW EXCLUSIVE is used to look at a whole table and to allow others to look at rows in the table but 
				to prohibit others from locking the table in SHARE mode or from updating rows.
				
PSEUDO-COLUMNS -->>
	Columns that are automatically present in oracle table
	These columns are not manipulated by the user
	Five types of pseudo columns
		rownum, rowid, level, currval, nextval
		currval, nextval is for sequences
		rowid, rownum, level is for tables
			rowid is the physical address
			rownum is just the sequential numbers
			level used in hierarchical report used in connect by clause
				connect by displays hierarchical records i.e, in tree structure
				level will display who belongs to the top etc.
			

GENERATING OUTPUT FILES -->>
	To spool the record or data is the way to solve this problem
		spool /home/oracle/Documents/spool/student_spool.csv;
		set heading off;
		set feedback off;
		set termout off;
		set pages 30 lines 200;
		set colsep ",";      -- if this line not used then tab will be field seperator
		prompt Student Details;
		prompt --------------------;
		prompt -----------------------------------------;
		select * from sandesh.student;
		set heading on;
		set feedback on;
		set termout on;
		spool off;
		
WITH CLAUSE -->>
	Usage of subqueries can be minimized
	If there are many levels of subqueries then the performance will drastically deteriorate
	If there are more than one exactly same subquery used in a query then all query will take time to execute
		This is solved with 'with' clause in which the subquery will only be computed once
	Usage
		select * from student where id in (select 9720 from dual);

		with val as (select 9720 val_column from dual)
		select * from student where id in (select val_column from val); 
		
CONNECT BY CLAUSE -->>
	Used in hierarchical queries
	(Primary key or Unique key) and foreign key column is used for this; no any constraint on both column also works but their must be some relation
	Top - Down hierarchy and Bottom - Up hierarchy
	Top-Down		PRIOR PK = FK | FK = PRIOR PK
	Bottom-Up		PRIOR FK = PK | PK = PRIOR FK
	Usage 
		for table employee with (employee_id pk column and manager_id fk column self referencing employee_id)
		select employee_id, last_name, job_id, manager_id, level
		from employees
		start with employee_id = 101	-- either at a top or bottom determined by prior keyword
		connect by prior manager_id = employee_id;   -- here employee_id is pk so it is bottom-up hierarchy (101 will be at the bottom of the hierarchy)
			-- i.e, 101 will have the level <last>
		
		select employee_id, last_name, job_id, manager_id, level
		from employees
		start with employee_id = 101	-- either at a top or bottom determined by prior keyword
		connect by prior employee_id = manager_id;   -- here employee_id is pk so it is top-down hierarchy (101 will be at the top of the hierarchy)
			-- i.e, 101 will have the level 1
			
VIEW -->>
	A view is a logical representation of one or more tables
	Unlike a table, a view is not allocated storage space
	Operations performed on a view affect data in some base table of the view and are subject to the
		integrity constraints and triggers of the base tables
	A general rule is that an INSERT, UPDATE, or DELETE operation on a join view can modify only one base table at a time
	Syntax 
		Create view as select ................;
		Create view as select ................ WITH READ ONLY; -- no dml can be performed
		Create veiw as select ................ WITH CHECK OPTIONS; -- check if any dml will transform data that will no be contained with the	
			present definition of view. Those operations will be errored out
	DML in views
		Can be performed if
			No distinct
			No aggregate
			key-preserved table
			Not read only
		
MATERIALIZED VIEW -->>
	Database view that stores pre computed results
	Like the result of complex group by, complex aggregate , complex joins result
	For remote database objects we create materialized view in local that's the main purpose
		Without this the performance of whole business will be worst
		Refresh can be done in non peak hour
	Create materialize view is required
	Base table must have a primary key constraint
	If database link is being used then create database link is also required
	Types of mat view
		Normal
			create materialized view view_name
			build [immediate | deferred]    -- immediate is the default ; with deferred the data will be populated only after first refresh
			refresh [fast | complete | force]
			on [commit | demand]   -- instead of these time interval can also be provided
			[[enable | disable} query rewrite]  -- disable in default
				-- with this enabled all the aggregation and complex task is precomputed beforehand and during refresh data is inserted quickly
			as select ....;
		Pre-Built
			create materialized view view_name
			on prebuilt table
				-- with prebuilt option, name of mat view and base table name must be same
				-- if the base table is already referenced by other mat view then also it will throw error
			refresh [fast | complete | force]
			on [commit | demand]
			[[enable | disable} query rewrite]
			as select ....;
	Types of refresh
		complete - Truncate and insert fresh copy from base table (takes longer time)
		fast - mat view log file should be present, otherwise it fails; if has more than one base table then log file should be there for all tables
				for shorter refresh interval fast refresh is used
				to create mat log - NOTE: create materialized view log on student;
		force - fast refresh is attempted, if fails then complete refresh is done, log file will be read for the any changes made and the change
				is applied
	Types or refresh events
		commit - refresh happens on every commits on base table
		demand - execute dbms_mview.refresh('student_mat_view'); -- executing this
				in general - exec dbms_mview.refresh('view_name', 'c') -- c means complete refresh
		periodically - providing the time in mat view definition
	Usage
		create materialized view view_name 
		as
		statement;
		
		create materialized view student_mat_view
		as
		select * from student where id <= 5000;    -- complex query should be written, easy just for demo
			-- If the table student is dropped now, then also the record in student_mat_view still persists unlike view
			-- If the table student changes then also the change won't get reflect in the mat view
			-- Mat view should be updated manually once the base table changes
			
		create materialized view mv_emp
		refresh complete			-- whole data will be refreshed
		start with sysdate         -- refresh starts from today
		nex trunc(sysdate) + 1		-- next refresh is after 1 day (i.e, each day it will refresh)
		as select ............;
	Alter
		alter materialized view view_name refresh complete;
		alter materialized view view_name refresh complete start with sysdate;
		alter materialized view view_name refresh on demand complete;
		
TYPE -->>
	A custom type can be created and used as the field type for oracle table
	Usage
		create or replace type global_type_san as varray(10) of varchar2(200);
		create table tbl_varray (id number, varr_type global_type_san);
		select * from tbl_varray;
			-- varr_type has data like HR.GLOBAL_TYPE_SAN('1', 'Sandesh')
		insert into tbl_varray values (1, global_type_san(1, 'Sandesh'));
		select t1_alias.id, t2_alias.column_value from tbl_varray t1_alias, table(t1_alias.varr_type) t2_alias;
			-- use table keyword to find out the cartesian product of the id column and global_type_san column which contains multiple value
					and can be considered as the table in itself

		
------------------------------------------------------------ ORACLE PL/SQL -------------------------------------

PL/SQL -->>
	It is a procedural language i.e, we can write step by step process in it
	It is case insensitive prog. language
	It is platform independent
	It is not ANSI standard
	It uses blocks
		Each block makes a single request to db server
	select * from user_source;  -- this contains all the code required for the build of plsql procedure
	ARCHITECTURE
		PLSQL BLOCK is divided into procedural statements and SQL statements
			Procedural statements are executed in PL/SQL engine
			SQL statements in Oracle server
		PLSQL block architecture
			DECLARE -- optional section; contains variables, constants, cursors, exception, pragma, type, local subprograms etc.
				-- local subprograms must at the end during declaration or PRAGMA can also be at the end
			BEGIN	-- Mandatory section (procedural statements + sql statements)
			EXCEPTION -- optional section (procedural statements + sql statements)
			END;
		Types of block
			Nameless block (Anonymous block)
			Named block
				Named block are stored in db in compiled form
	DBMS_OUTPUT.PUT_LINE -- for printing
	DBMS_OUTPUT.PUT -- for printing in single line
	EXECUTE DBMS_OUTPUT.ENABLE
	EXECUTE DBMS_OUTPUT.DISABLE
	Before printing the message serveroutput should be enabled
		set serveroutput on;
		show serveroutput;
		
		set autoprint on; -- this will print the value of variable automatically without once the plsql block is compiled
	All variable declared inside declare section are local variables..
	To use the global variable bind variable can be used while calling from plsql
	Global variables are also called, host, environment and global variable.
	
DBMS_OUTPUT -->>
	ENABLE			--DBMS_OUTPUT.PUT.ENABLE;
	DISABLE			--DBMS_OUTPUT.DISABLE;
	PUT				--DBMS_OUTPUT.PUT('');
	PUT_LINE		--DBMS_OUTPUT.PUT_LINE('');
	NEW_LINE		--DBMS_OUTPUT.NEW_LINE();
	GET_LINE		--DBMS_OUTPUT.GET_LINE(input varchar out, int status out);    -- no need to use them  -- input is from previous put_line
	GET_LINES		--DBMS_OUTPUT.GET_LINES(inputs varchar out, int statuses out);  -- no need to use them -- input is previous multiple put_line
	
	
ANONYMOUS BLOCK -->>
	Useful
		declare
			some_var varchar2(100) := 'Some dummy value written';
			some_const number default 3.14;
		begin
			some_const := 7;
			dbms_output.put_line(some_var || ' and PI: ' || some_const);
		end;
		/
		
	Using bind variable
		variable person_name varchar2(20);
		execute :person_name := 'Sandesh Pokhrel';

		begin
			dbms_output.put_line('Value of bind variable person_name is: ' || :person_name);
		end;
		/
			
	Using type of table column
		declare
			first_name employees.first_name%type;   -- %type and %rowtype are two plsql attributes
			last_name employees.last_name%type;
			v_query varchar2(1000);
		begin
			v_query := 'select first_name, last_name from employees where employee_id = 100';
			select first_name, last_name into first_name, last_name from employees where employee_id = 100;
			dbms_output.put_line('Full name is: ' || first_name || ' ' || last_name);
			
			execute immediate v_query into first_name, last_name;
			dbms_output.put_line('Full name is: ' || first_name || ' ' || last_name);
		end;
		/
		
		declare
			employee_row employees%rowtype;    -- here a table row type is used instead of individuals
		begin
			select first_name, last_name into employee_row.first_name, employee_row.last_name from employees where employee_id = 100;
			select * into employee_row from employees where employee_id = 100;
			dbms_output.put_line('Full name is: ' || employee_row.first_name || ' ' || employee_row.last_name);
		end;
		/
		

	set transaction read only;   -- transaction mode modified  ; this should be the first line of transaction otherwise error
	set transaction read write;  -- default mode ;  this should be the first line of transaction otherwise error
	TCL and DML can be used inside plsql.
	DDL and DCL can be used but should be used dynamically.
	
	Attributes
		Two attributes in plsql
			%type
			%rowtype
			
			
NAMED BLOCK -->>
	Lables can be given anywhere; not only above begin; it's like a marker to indicate for goto
	Nested Nameless block
		declare
			some_integer number := 100;
		begin
			dbms_output.put_line('From outside: ' || some_integer);
			begin
				dbms_output.put_line('From inside: ' || some_integer);
			end;
		end;
		/

	Nested Named block
		declare
			some_integer number := 100;
		begin
			dbms_output.put_line('From outside: ' || some_integer);
			<<XYZ>>
			begin
				dbms_output.put_line('From inside (Named Block) : ' || some_integer);
			end;
		end;
		/
		
	Named block
		<<ABC>>
		declare
			some_integer number := 100;
		begin
			dbms_output.put_line('From outside: ' || some_integer);
			<<XYZ>>
			begin
				dbms_output.put_line('From inside (Named Block) : ' || some_integer);
			end XYZ;
		end;
		/
		
		Loop like implementation using labels
		declare
			v number := 1;
		begin
			dbms_output.put_line('Start: ' || v);
			<<XYZ>>
			dbms_output.put_line('Inside XYZ: ' || v);
			if v>= 5 then
				return;
			end if;
			v := v + 1;
			goto XYZ;
		end;
		/
		
IDENTIFIER -->>
	Max length 30 character
	Allowed characters A-Z,a-z,0-9,_,$,#
	First char must be an alphabet
	Not case sensitive
	Keywords are not allowed as identifiers
	
	
CONTROL STRUCTURES -->>
	Conditional - if-elsif if-else if-elsif-else.. end if;
	Iterative - simple loop: (finite, infinite), while loop, for loop: (incremental, decremental)
	Sequential - goto, null
	
	Relational operators
		<, >, <>, !=, >=, <=, AND, OR
		
	Simple loop
		Infinite
			loop
				......
			end loop;
			
		Finite
			loop
				....
				exit when condition;  -- if condition never matches then also infinite
				OR
				if v_count = 5 then
					exit;
				end if;
			end loop;
			
	While loop
		while con loop
			.....
		end loop;
		
	For loop
		FOR loop_counter IN [REVERSE] lowest_number..highest_number LOOP
		   {...statements...}
		END LOOP;
		
	Goto
		Move the execution control to particular labels
		Move upward or downward within begin particular
		Move upward or downward within exception part
		It cannot move from begin to exception or vice versa
		Cannot move from if condition to outside and vice versa
		Cannot move to other blocks
		
	Null
		It will do nothing
		Hust passes the execution control to next statement
		Usage
			begin
				if 1=1 then
					null;
				end if;
				dbms_output.put_line('After');
			end;
			/
			
OPERATORS -->>
	:= assignment
	.. range
	~=, <>, !=, ^= inequality -- ~= only works in pl/sql
	** exponentation
	=> association
		-- exec proc_name(5); OR exec_proc_name(proc_arg => 5);
		
		
SUBPROGRAMS -->>
	Two types
		PROCEDURE
			Mainly to perform an action
			Allows all DML + TCL statements
			Accepts all modes of parameters
			Accepts in, out, in out parameters
			Donot return any value directly
			Return values through out/in out parameters
			Cannot have return clause
			Cannot be called in the DML statements
			Return statement is optional in the body
			
			create or replace procedure p_name 
			is
			........ (optional)
			begin
			.... (mandatory)
			end;
			/
			
			execute p_name; OR exec p_name();
			
		FUNCTION
			Mainly to perform an calculations
			Allows all DML + TCL statements
			Accepts all modes of parameters
			
			create or replace function f_name return number  --  return type can be anything 
			is
			........ (optional)
			begin
			.... (mandatory)
			end;
			/
			
			exec dbms_output.put_line(f_name); OR exec dbms_output.put_line(f_name());
			select f_name from dual;
			
		PARAMETERS
			IN -> Assignment cannot be performed with the parameter indicated as IN
			OUT -> Value cannot be passed as argument for OUT, if passed then the value will be null automatically
				-- bind variable can be passed as an out param
			IN OUT -> value can be passed as well as it is returned
			
			IN, OUT, IN_OUT, Default args with default keyword and default args with assignment operator
			Function is same, the difference is it can also return the value 
			create or replace procedure proc_with_params 
			(ids in number, returned out number, in_return in out number, default_val in number default 57, def_assign_val in number := 99)
			is
			begin
				dbms_output.put_line('Passed returned is: ' || returned);
				dbms_output.put_line('Passed in + returned is: ' || in_return);
				dbms_output.put_line('Passed id is: ' || ids);
				returned := 200;
				in_return := 700;
				dbms_output.put_line('Modified returned is: ' || returned);
			end;
			/

			variable out_var number;
			variable in_out_var number;
			execute :out_var := 300;
			execute :in_out_var := 200;
			execute proc_with_params(returned => :out_var, in_return => :in_out_var, ids => 100);
			execute dbms_output.put_line(:out_var);
			execute dbms_output.put_line(:in_out_var);
			
			
			NOTE: In case of function if it has out parameter then it cannot be called using select and any other DML statements
					Functions with TCL cannot be called through DML statements
					Function with DML write cannot be called through DML statements
					
PACKAGES -->>
	A database object
	Stored in database
	It has two parts - Specification and Body
	Package spec and body must have the same name
	Package spec is mandatory. Pacakge body may or may not be present. Body cannot be compiled without its specs.
	All the subprograms declared inside the spec must present in the body.
	Body can contain subprograms that is not declared in the spec, those subprograms are local and can only be utilized from within the package.
	Package is a collection of subprograms (procedures + functions)
	There can be zero subprograms in the package. This is like orphan package.Body is not required for this package.
	
	Usage
		-- This is an orphan package. As no subprograms are declare it does not need a body.
		create or replace package orphan_package is
			v_num number := 10;
			cursor c1 is select * from employees;
		end orphan_package;
		/
		execute dbms_output.put_line (orphan_package.v_num);
		
		
		create or replace package emp_package as
			v_global_unlike_in_body number;		-- This is a global variable. Accessed from all subprograms and from the outside also.
			function func_in_package (nums in out number ) return number;
			procedure proc_in_package (nums in out number);
		end emp_package;
		/

		prompt create or replace package body emp_package				-- This is used for printing something in the screen
		create or replace package body emp_package as
			v_global number;		-- Declaring global variable for the whole package's subprograms. Can't be accessed from outside unlike that of specs.
			function func_in_package (nums in out number) return number is
			begin
				nums := nums**2;
				return nums;
			end func_in_package;
			
			-- This procedure is not in the package specification
			-- This is called local procedure as it is accessible for only the package inside this procedure
			-- Declare this procedure above the one which is using it. Otherwise the package won't compile.
			procedure local_procedure is
			begin
				dbms_output.put_line('Local procedure called');
			end local_procedure;

			procedure proc_in_package (nums in out number) is
			begin
				nums := nums*2;
				dbms_output.put_line('Num * 2 = ' || nums);
			end proc_in_package;
			
			-- Initialization block inside the package; This is optional; It must not contain the end statement
			-- This is executed for the first time of the session only
			begin
				dbms_output.put_line('Welcome');
		end emp_package;
		/

		variable var_bind number;
		execute :var_bind := 5;
		execute emp_package.proc_in_package(:var_bind);
		print :var_bind;

		execute dbms_output.put_line ('Printing from function: ' || emp_package.func_in_package(:var_bind));
		print :var_bind;
		
TRIGGERS -->>
	A trigger is a named PL/SQL block stored in the Oracle Database and executed automatically when a triggering event takes place
	Triggers are the db objects fired at the certain event
	Triggers can be disabled or enabled; This can also be dictated during trigger creation
	Triggers can also call the subprograms from the body section
	Disabling triggers for the table
		alter table table_name disable all triggers;
	Commits, rollbacks, savepoint cannot be performed in the triggers. It creates runtime error
	Set transaction also creates run time error
	Commits and rollbacks are valid only if the autonomous_transaction pragma is declared in the trigger
		i.e, pragma autonomous_transaction;  -- declared in the declare section of trigger
	Merge triggers can be created but it won't be fired during merge operation.
	Insert or update trigger will be fired based on the on matched and unmatched condition in the merge statement
	When clause cannot be used for table level triggers
	REFERENCING can only be use if FOR EACH ROW clause is specified
	If multiple triggers are there for same event then they will execute in the reverse order of their creation
	Syntax
		CREATE [OR REPLACE] TRIGGER trigger_name
		{BEFORE | AFTER } triggering_event ON table_name
		[FOR EACH ROW]
		[FOLLOWS | PRECEDES another_trigger]  -- in case of compound trigger
		[ENABLE / DISABLE ]
		[WHEN condition]
		DECLARE
			declaration statements
		BEGIN
			executable statements
		EXCEPTION
			exception_handling statements
		END;
		
	Types of triggers
		Based on level - Statement + Row
		Based on timing - Before + After
		Based on Event - DML + DDL + DCL (DCL - grant , revoke etc.)
			System events
				After Startup + Before shutdown + After logon + After logoff + ServerError
			Instead of trigger - On views only
			Compound trigger
			Cascading trigger
			Recursive trigger
			
	Based on Level
		Statement level
			A statement-level trigger is fired whenever a trigger event occurs on a table regardless of how many rows are affected
			Statement level trigger will have higher precedence than the row-level triggers
			For example, updating 1000 rows in a table, then a statement-level trigger on that table would only be executed once
			Due to its features, a statement-level trigger is not often used for data-related activities like auditing the changes of the data in the associated table
			It’s typically used to enforce extra security measures on the kind of transaction that may be performed on a table
			Usage
				Disallowing the insert on table students from day 20 to 31 of any months
				CREATE OR REPLACE TRIGGER students_insert_failure
					BEFORE INSERT
					ON students
				DECLARE
					l_day_of_month NUMBER;
				BEGIN
					-- determine the transaction type
					l_day_of_month := EXTRACT(DAY FROM sysdate);
				 
					IF l_day_of_month BETWEEN 20 AND 31 THEN
						raise_application_error(-20100,'Cannot insert students from 28th to 31st');
					END IF;
				END;
				/
				
		Row level	
			Row-level triggers fires once for each row affected by the triggering event such as insert, update or delete
			Row-level triggers are useful for data-related activities such as data auditing and data validation
			For After triggers :new.columnname cannot be assigned a value, as the data is already inserted in the table
			For before trigers :new.comumnname can be modified such that the modified value get inserted
			if UPDATING OR DELETEING OR INSERTING can be used to perform different action based upon those event if trigger supports all the actions
			Problems
				Recursion can occur if insert in performed in the insert trigger which keep on firing the trigger
				Mutating trigger if read is also being performed and update is also being performed
					Eg. If in after insert trigger, update is performed in body for the same table
					Soln. Change after to before insert and simply change the value of :new.columnname. The last modified value will be inserted
			Usage
				create or replace trigger tg_student 
				after insert on students
				referencing old as old    -- this two lines can be removed, by default new in new and old is old
				new as new
				for each row
				declare
					v_remarks varchar2(100) := 'PASS';
				begin
					/*
					if :new.marks < v_pass_limit then
						v_remarks := 'FAIL';
					end if;
					*/
					dbms_output.put_line('Hello ' || :new.roll);
					--update students set remarks = v_remarks where roll = :new.roll;
					--insert into students values (:new.roll, :new.full_name, :new.marks, v_remarks);
					:new.remarks := v_remarks;
				end;
				/
				
	Based on events
		DML
			Insert, Update, Delete
		DDL 
			DDL triggers are not created in object like in DML
			DDL triggers are created for schema level or for database level
			These trigger should execute before the event (Preferred). After trigger can also be created
			For flashback and purge trigger cannot be created
			For comment, drop, create, alter, rename triggers can be created 
			Usage
				Raises error if any object is being dropped in the schema   
				create or replace trigger stu_bef_drop 
				before drop on schema				-- schema level trigger
				begin
					raise_application_error(-20001, 'You cannot drop objects');
				end;
				/
				
				Raises error if any object is being dropped in the database 
				create or replace trigger stu_bef_drop 
				before drop on database				-- database level trigger (dba privelege required)
				begin
					raise_application_error(-20001, 'You cannot drop objects');
				end;
				/
		DCL
			Trigger can be created for grant, revoke
			Trigger cannot be created for set role 
			
	System events
		After Logon
			It cannot have before trigger, only after trigger are allowed
			create or replace trigger logon_trigger 
			after logon on schema			-- schema level, database level can also be create if dba 
			begin
				insert into tbl_audit values ('LOGON', sysdate);
			end;
			/
		Before Logoff
			It cannot have after trigger, only before trigger are allowed
			create or replace trigger logff_trigger 
			before logoff on schema			-- schema level, database level can also be create if dba 
			begin
				insert into tbl_audit values ('LOGOFF', sysdate);
			end;
			/
		After Startup
			It cannot have before trigger, only after trigger are allowed
			create or replace trigger startup_trigger 
			after startup on schema			-- schema level, database level can also be create if dba 
			begin
				insert into tbl_audit values ('STARTUP', sysdate);
			end;
			/
		Before Shutdown
			It cannot have after trigger, only before trigger are allowed
			create or replace trigger shutdown_trigger 
			before shutdown on schema			-- schema level, database level can also be create if dba 
			begin
				insert into tbl_audit values ('SHUTDOWN', sysdate);
			end;
			/
		After servererror
			It cannot have before trigger, only after trigger are allowed
			create or replace trigger server_error_trigger 
			after servererror on schema			-- schema level, database level can also be create if dba 
			begin
				insert into tbl_audit values ('SERVERERROR', sysdate);
			end;
			/
			
	Instead of Trigger
		These triggers are only for the views, does not work on tables
		This trigger also work either in statement or in row level
		Main usage of this trigger is to convert the action not permitted in view to redirect it to the base table
		Usage
			create or replace trigger insteadof_trigger
			instead of insert on vw_students
			for each row
			begin
				dbms_output.put_line('Instead oftrigger executing');
				insert into students (roll, full_name, marks, remarks) values (1000, :new.full_name, :new.marks, :new.remarks);
			end;
			/
			
	Conditional Trigger
		These types of trigger provides luxury to use column names with update event
		These can be either on statement level or in row level
		Works only for update
		If additional when clause is used for these trigger then make sure it is row level trigger
		Usage
			create or replace trigger updt_fname_student_trigger
			after update of full_name<,....> on students		-- specifies to work only for full_name in students table
			for each row
			begin
				dbms_output.put_line('Update performed on students.full_name');
			end;
			/
			
			create or replace trigger updt_fname_student_trigger
			after update of full_name<,....> on students		-- specifies to work only for full_name in students table
			for each row
			when (old.full_name = 'Sandesh Pokhrel')  -- either old. or new.
			begin
				dbms_output.put_line('Update performed on students.full_name');
			end;
			/
			
			Multilevel action trigger
			create or replace trigger updt_fname_student_trigger
			after update OR insert OR delete on students		-- specifies to work only for full_name in students table
			begin
				if INSERTING then
					.....
				if DELETING then
					.....
				if UPDATING then
					.....
				end if;
			end;
			/
			
	Recursive Trigger
		Automatic recursion is possible only in case of trigger
		Function and procedure can also do it but it can be done manually
		Eg: if the after update trigger on abc table is performing the update for the same table in body section
		Try preventing this problem from happening
		
	Cascading Trigger
		In this type of trigger, one trigger's body action will invoke another trigger and so on...
		
	Mutating Trigger
		If the trigger body is selecting from the same table and try updating the same then the problem is called mutation
		Try avoiding this the solution is also mentioned above
		This can be avoided either using autonomous_transaction pragma in the trigger which will just create the separate transaction
		OR
		Avoid performing the action that causes mutation
		
	Compound Trigger
		If there are multiple triggers for the same event, same level, same timing
		In default the trigger will execute in the reverse order of their creation
		To override the default scenario follows <trigger_name> is used during trigger creation to follow which trigger during execution
		See the trigger syntax at the top to clarify the solution
		
			
COLLECTIONS -->>
	Collections that are available in plsql
		VArray					PLSQL / SQL
		Associative Array		PLSQL			--- also called as Index-by-tables
		PL/SQL Table			PLSQL / SQL
		PLS/SQL Record			PLSQL
		Nested Table			PLSQL
		Object Table			SQL
		
	Using SQL to create type
		create or replace type v_type as varray(10) of varchar2(500);
		
	Methods
		Count	-	Total element
		Limit	-	Size of the collection (Limit is for VArray only)
		First	-	First element index
		Last	-	Last element index in array
		Prior	-	Index before the specified index (PLS_integer returned in case of varray and nested table); for others pls_integer, varchar2 etc returned
		Next	-	Index after the specified index (PLS_integer returned in case of varray and nested table); for others pls_integer, varchar2 etc returned
		Delete	-	Delete all (also supported on varray; other type not supported), delete(n) at that index, delete(n,m) delete from  n to m
			-- varray is always dense so cannot delete like delete(n), delete(n,m) 
		Extend	-	Can be used with varray and nested tables. Not with associative arrays
					-- append single or multiple elements to the collection.
					-- array must be initialized to use the extend
					EXTEND, which adds a single NULL instance.
					EXTEND(n) which adds multiple NULL instances. The number of instances is specified by n.
					EXTEND(n,m) which appends n copies of instance m to the collection.
		Exists	-	Check whether the element exists or not
		
	Multiset operations (Operations are all i.e, not union only, it is union all)
		Union, Union Distinct, Intersect, Intersect Distinct, Except, Except Distinct
		SQL EQUIVALENT
		Multiset Union = Union all
		Multiset Union all = Union all
		Multiset Union Distinct = Union
		Multiset Intersect = NA
		Multiset Intersect Distinct = Intersect
		Multiset Except = NA
		Multiset Except Distinct = Minus
		
	VArray
		VArray is varying array
		It is fixed size collection
		Multiset operators does not work with varray
		Upper limit size is fixed
		Populated sequentially starting with the subscript '1'
		This collection type is always dense, i.e. we cannot delete any array elements. Varray can be deleted as a whole, or it can be trimmed from the end.
		Since it always is dense in nature, it has very less flexibility.
		It is more appropriate to use when the array size is known and to perform similar activities on all the array elements.
		The subscript and sequence always remain stable, i.e. the subscript and count of the collection is always same.
		They need to be initialized before using them in programs. Any operation (except EXISTS operation) on an uninitialized collection will throw an error.
		It can be created as a database object, which is visible throughout the database or inside the subprogram, which can be used only in that subprogram.
		Three way to declare all are same
			type t_name is varray(5) of number;
			type t_name2 is array(5) of student.full_name%type;
			type t_name3 is varrying array(5) of student%rowtype;
			-- varray, array, varrying array all are same
		Usage
			create or replace procedure proc_collection_demo
			is
				type varr_type is varray(10) of number;
				v_type varr_type;
			begin
				dbms_output.put_line('Welcome to varray startup');
				v_type := varr_type(10,20,30,40,50);
				dbms_output.put_line('At third index: v_type(3): ' || (v_type(3) + v_type(2)));
				dbms_output.put_line(v_type.last);
				dbms_output.put_line(v_type.first);
				dbms_output.put_line(v_type.limit);
				dbms_output.put_line(v_type.count);
				dbms_output.put_line(v_type.prior(2));
				dbms_output.put_line(v_type.next(2));
				v_type.extend(1,5);	-- extends the element at index 5 once
				v_type.extend;		-- extends the element by appending null at the end
				v_type.extend(2);	-- extends the element by appending null two times
				v_type.delete;		-- deletes everything from the collection
			end;
			/
			
			Bulk collect should be used in order to enter more than one value in array at once
			create or replace procedure proc_collection_demo2
			is
				type varr_type is varray(10) of students%rowtype;
				type varr_type2 is varying array(10) of students.full_name%type;
				v_type varr_type;
				v_type2 varr_type2;
			begin
				dbms_output.put_line('Welcome to varray startup');
				select * bulk collect into v_type from students;
				for i in 1..v_type.count loop
					dbms_output.put_line(v_type(i).full_name);
				end loop;
				
				select full_name bulk collect into v_type2 from students;
				for i in 1..v_type2.count loop
					dbms_output.put_line(v_type2(i));
				end loop;
			end proc_collection_demo2;
			/
			
			In this example it uses varr_type column of table tbl_varray which is type of varray(10) of varchar2(200)
			create or replace procedure proc_collection_demo3
			is
				varr_type tbl_varray.varr_type%type;
			begin
				select varr_type into varr_type from tbl_varray where id = 1;
				varr_type.extend(1,2);
				dbms_output.put_line(varr_type(3));
			end;
			/
			
			In this example tbl_varray is looped which contains the type as a column of varray(10)
			Here varray of table column type is created; column itself is the type column and the bulk collect is performed
			create or replace procedure proc_collection_demo3
			is
				varr_type tbl_varray.varr_type%type;
				type varr_type2 is varray(20) of tbl_varray.varr_type%type;
				v_type varr_type2;
			begin
				select varr_type bulk collect into v_type from tbl_varray where rownum <= 20;
				select varr_type into varr_type from tbl_varray where id = 1;
				varr_type.extend(1,2);
				dbms_output.put_line(varr_type(3));
				
				for i in (select * from tbl_varray) loop
					dbms_output.put_line('Row---------------');
					for j in i.varr_type.first..i.varr_type.last loop
						dbms_output.put_line('Fetching: ' || i.varr_type(j));
					end loop;
				end loop;
			end proc_collection_demo3;
			/
			
	PLSQL Record
		A PL/SQL record is a composite data structure which consists of multiple fields; each has its own value
		It cannot be initialized
		Usage
			create or replace procedure proc_collection_demo2
			is
				type v_arr is record (eno number);
				v_type v_arr;
				--v_type v_arr := v_arr(); -- cannot be initialized
			begin
				null;
			end;
			/
			
			Record created and being used by table
			create or replace procedure proc_collection_demo2
			is
				type v_rec is record (name students.full_name%type);  -- record (name varchar2(200)) also allowed
				type v_tbl is table of v_rec;
				v_tbl_type v_tbl;
			begin
				select full_name bulk collect into v_tbl_type from students;
				for i in 1..v_tbl_type.count loop
					dbms_output.put_line('Hello: ' || v_tbl_type(i).name);
				end loop;
			end;
			/
			
			This example uses record and associative array concept
			DECLARE
			  CURSOR c1 IS SELECT hedis_tablename FROM lkp_hedis_counts;
			  vquery VARCHAR2(500);
			  v_tbl_without_in VARCHAR2(50);
			  name   VARCHAR2(20); 
			  type type_rec is record (query VARCHAR2(500), status char);
			  v_rec type_rec;                                                
			  TYPE tbl_type IS table OF type_rec INDEX BY VARCHAR2(50);
			  v_type tbl_type;
			BEGIN                       
			  FOR hed_tblname IN c1 LOOP
					v_tbl_without_in := SubStr(hed_tblname.hedis_tablename, 1, InStr(Lower(hed_tblname.hedis_tablename), '_in', -1) - 1);
					vquery := 'rename ' || v_tbl_without_in || ' to ' || hed_tblname.hedis_tablename;
					v_rec.query := vquery; v_rec.status := 'C'; 
					begin
					  EXECUTE IMMEDIATE vquery;
					EXCEPTION WHEN OTHERS THEN
					  v_rec.status := 'E';
					END;
					v_type(v_tbl_without_in) := v_rec;
			  END LOOP;
			  name := v_type.FIRST; 
			  Dbms_Output.put_line('Statement executed ------------------------------------');
			  WHILE name IS NOT null LOOP 
				Dbms_Output.put_line(name || ' : ' || v_type(name).query || ' -->> ' || v_type(name).status);
				name := v_type.NEXT(name); 
			  END LOOP;
			EXCEPTION WHEN OTHERS THEN
			  Dbms_Output.put_line('Some error!!! Check this whole block..');
			END;
			/
		
		
			
	PLSQL Table | Nested Table
		The Nested table has no upper size limit.
		Since the upper size limit is not fixed, the collection, memory needs to be extended each time before we use it. We can extend the collection using 'EXTEND' keyword.
		Populated sequentially starting with the subscript '1'.
		This collection type can be of both dense and sparse, i.e. we can create the collection as a dense, and we can also delete the individual array element randomly, which make it as sparse.
		It gives more flexibility regarding deleting the array element.
		It is stored in the system generated database table and can be used in the select query to fetch the values.
		The subscript and sequence are not stable, i.e. the subscript and the count of the array element can vary.
		They need to be initialized before using them in programs. Any operation (except EXISTS operation) on the uninitialized collection will throw an error.
		It can be created as a database object, which is visible throughout the database or inside the subprogram, which can be used only in that subprogram.
		Usage
			Example using all multiset operations
			create or replace procedure proc_collection_demo2
			is
				type v_arr is table of number;
				v_type v_arr := v_arr(1,2,1,3,2,4,3,4,5);
			begin
				dbms_output.put_line('ALL VALUES--------------------------');
				for i in v_type.first..v_type.last loop
					dbms_output.put_line(v_type(i));
				end loop;
				
				dbms_output.put_line('UNION VALUES--------------------------');
				v_type := v_type multiset union all v_type;
				for i in v_type.first..v_type.last loop
					dbms_output.put_line(v_type(i));
				end loop;
				
				v_type := v_arr(1,2,1,3,2,4,3,4,5);
				dbms_output.put_line('UNION DISTINCT VALUES--------------------------');
				v_type := v_type multiset union distinct v_type;
				for i in v_type.first..v_type.last loop
					dbms_output.put_line(v_type(i));
				end loop;
				
				v_type := v_arr(1,2,1,3,2,4,3,4,5);
				dbms_output.put_line('INTERSECT VALUES--------------------------');
				v_type := v_type multiset intersect v_type;
				for i in v_type.first..v_type.last loop
					dbms_output.put_line(v_type(i));
				end loop;
				
				v_type := v_arr(1,2,1,3,2,4,3,4,5);
				dbms_output.put_line('INTERSECT DISTINCT VALUES--------------------------');
				v_type := v_type multiset intersect distinct v_type;
				for i in v_type.first..v_type.last loop
					dbms_output.put_line(v_type(i));
				end loop;
				
				v_type := v_arr(1,2,1,3,2,4,3,4,5);
				dbms_output.put_line('EXCEPT VALUES--------------------------');
				v_type := v_type multiset except  v_arr(1,2);
				for i in v_type.first..v_type.last loop
					dbms_output.put_line(v_type(i));
				end loop;
				
				v_type := v_arr(1,2,1,3,2,4,3,4,5);
				dbms_output.put_line('EXCEPT DISTINCT VALUES--------------------------');
				v_type := v_type multiset except distinct v_arr(1,2);
				for i in v_type.first..v_type.last loop
					dbms_output.put_line(v_type(i));
				end loop;
			end;
			/
			
	
	Associative Array | Index-by-table
		Index-by-table is a collection in which the array size is not fixed. 
		Unlike the other collection types, in the index-by-table collection the subscript can consist be defined by the user
		The subscript can of integer or strings. At the time of creating the collection, the subscript type should be mentioned.
		These collections are not stored sequentially.
		They are always sparse in nature.
		The array size is not fixed.
		It cannot be initialized.
		They cannot be stored in the database column. They shall be created and used in any program in that particular session.
		They give more flexibility in terms of maintaining subscript.
		The subscripts can be of negative subscript sequence also.
		They are more appropriate to use for relatively smaller collective values in which the collection can be initialized and used within the same subprograms.
		They need not be initialized before start using them.
		It cannot be created as a database object. It can only be created inside the subprogram, which can be used only in that subprogram.
		BULK COLLECT cannot be used in this collection type as the subscript should be given explicitly for each record in the collection.
		Three types of data types are supported for index: VARCHAR2, PLS_INTEGER, BINARY_INTEGER
		Usage
			create or replace procedure proc_collection_demo2
			is
				type v_arr is table of number index by pls_integer;
				v_type v_arr; 
				--v_type v_arr := v_arr(1,2,3,4)	-- cannot be initialized; index must provided manually
			begin
				v_type(1) := 4;
				dbms_output.put_line(v_type(1));
			end;
			/
			
EXIT, RETURN, BREAK, CONTINUE -->>
	Return
		It is mandatory inside the function
		Return is used to get the control out of the block
		Return; in procedure is valid. It will take control out from the procedure
		Return; is not valid in the function as the function must return a value
		Statement after return won't be executed
		
	Exit
		Exit will return out of the loop.
		
	Continue
		It is added in latest versions. Valid in oracle 12C.
		This will check the value and continue the loop exiting once for the condition matched
		
	Break
		Not a keyword in oracle
		
JAVA PROCEDURE -->>
	------ STUDY MORE -----
	
	
------------------------------------------------------------ ORACLE QUERY OPTIMIZATION -------------------------------------

BASIC SQL OPTIMIZATION -->>
	Normalization
		Normalization reduces the redundant data in the table
		This will help reduce the volume of table being fetched from the query
		Skinny tables = faster queries
		First normal form
			No repeating fields i.e, two phone number fields
		Second normal form
			Must be in first normal form
			No attribute associated with partial key column
				i.e, in case of composite key column should depend on whole key not the partial key
		Third normal form
			Must be in first and second normal form
			All non key column must directly depend on primary key only
		Dividing the larger tables into facts and dimensions tables. Dimensions table contains all 
	Infrequent columns
		Only add those columns in the table which will be used in the select clause
		Separate out infrequently used columns into a separate table
	Nulls and space
		Nulls take up 1 byte of space if present between the columns
		Nulls will take 0 byte space if present at the end of the columns
	Neaten up SQL code
		Spend less time figuring the code
	Intersect vs Inner join
		Intersect performs better than corresponding inner join
		Intersect has its own distinct too where as explicit distinct should be mentioned in case of join
	Minus vs Left join
		Minus performs better than corresponding left join
		Minus has its own distinct too where as explicit distinct should be mentioned in case of join
	Correlated subquery
		Outer query column(s) coded within subquery
		Generally performs poorly
		Try avoiding correlated subqueries
	In vs Exists
		If inner query is small as compared to outer than use IN
		If inner query is large as compared to outer than use EXISTS
	Multi-Column IN condition vs ANDs/ORs
		Multi-column IN performs better than ANDs/ORs
		Readable too
	The WITH clause
		Readable
		Puttine subquery that runs multiple times in a query within the WITH clause reduces the time significantly
	Append hint
		Best on tables with a lot of deletes
		Normally while inserting the writer search for the vacant block created after deletion of record.
			This will degrade the performance as significant amount of time is wasted for that search
		Append hint suggest not to search for those vacant space and directly append record at the end
	On clause vs Where clause
		No significant runtime reduction
		Use ON clause for join criteria
		Use where clause for subsetting criteria
	Order of tables on from clause
		Tables with fewer rows first
		May significantly reduce execution time
		
INDEX -->>
	Index solves the problem of full table scan problem
		i.e, every row of the table should be visited if the index is not defined
	Creating index for every columns in table
		Waste of space. Can get heavier than data itself
		If whole table is read generally then index won't help
		Index and where clause goes hand in hand so the if created for some column it might no get used
		Index will only be used if <=15% of data is returned
		Inserts, updates and deletes can go slower as not only the data is inserted but also index gets updated
	B-Tree Index
		This is the most general purpose index
		It is upside down tree like structure in which the bottom nodes are leaves and traversal happens from the top
	Bitmap Index
		Stored as a series of bits (0 or 1) indicating row incusion/exclusion
		Used if distinct column values is very much less than total number of rows
		Eg, Gender column might contains only two values wheres there might me millions of row present
		Binary AND OR can be performed if and and or are used in column having bitmap index defined
	Function based Index	
		For Eg, if UPPER(name) is to be used often instead of name only then the index thus created are function base index
		Different complex calculation can also be used but the exact calculation is to be used in where clause
	Index-Organized tables
		Combines table and index into one object in contrast to btree indexes in which they are stored separately
		Stored based on the primary key
		Not generally used in practice
	Bitmap join indexes
		Performs the joins up-front
		Inner join between tables should be defined while creating these index
	Gathering statistics
		Oracle uses optimizer statistics to analyze the execution plan that should be adopted
		Gather statistics after creating the indexes
		Otherwise it won't be helpful if oracle does not know the statistics and benefit of using index
		Use DBMS_STATS package to gather statistics
		Avoid older ANALYZE commands
		Oracle will auto gather stats in index column only if the table is not empty
	Index/Partition Interaction
		Partition means to slice the table into smaller once based upon some table value
		Index can also be created in each partitioned table they are called (local partitioned indexes)
		Index across all partitions are called (global non-partitioned indexes)
		Index that use different partitioning scheme than that of underlining table is called (global partitioned indexes)
	Eg
		select * from v$sesstat;
		select * from v$statname;
		exec dbms_stats.gather_table_stats(ownname=>'HR', tabname=>'EMPLOYEES', estimate_percent=>DBMS_STATS.AUTO_SAMPLE_SIZE);
		exec dbms_stats.gather_table_stats(ownname=>'HR', tabname=>'EMPLOYEES', estimate_percent=>100, degree=>DBMS_STATS.DEFAULT_DEGREE);
			-- 100 means gather stats on all rows
			-- degree means the degree of parallelism
		
B-TREE INDEX -->>
	B-Tree index is like an inverted tree with root at the top and leaf at the bottom
	RowId is placed at the leaf of the B-Tree indicating the rows belonging to the particular index branch
	Can be placed on numeric and date data types
	This index can be either simple or composite
		Composite in the sense that it can be placed in more than one column
		If where clause is placed generally in one column than go for simple otherwise composite index
	When to use
		Index <-> where clause
		Gather and inspect where clause that could be used and create index accordingly
		Primary Key
			index created automatically
			prevents accidental data duplication
			index <-> joins
	Create Eg
		Create index index_name on table_name(column_name);
		create unique index ix_name on table_name(unique_col_name);
		create unique index ix_name on tbl_nm(columns...);
		create unique index ix_name on tbl_nm(columns...) parallel;
		create unique index ix_name on tbl_nm(columns...) tablespace tbs_name;
	Useful dictionary tables
		user_tables, all_tables
		all_ind_columns, user_ind_columns
	Gather statistics
		DBMS_STATS package
			GATHER_SCHEMA_STATS - gather stats on all tables in a schema
			GATHER_TABLE_STATS - gather stats on a specific table
		Dict view
			*_tab_statistics, *_tab_col_statistics, *_ind_statistics
	Is the index being used
		Autotrace and Explain plan is used 
		Need plan_table - ask you db administrator (UTLXPLAN.SQL)
		Autotrace execute the sql statement whereas explain plan won't
		Useful
			For autotrace
				set autotrace on explain;
				run_sql_code in here
				set autotrace off;
				view the plan table for the info
			For explain plan
				explain plan set statement_id='TEST1' for select.................;
				
BITMAP INDEX -->>
	It is used if the cardinality of distict values for a column is less comparable to the total rows present
	Distinct column values is very less than total rows in table
	For multiple bitmaps on single sql query AND/OR operation is performed
	When to use
		Used often in data warehouses
		Degree of cardinality is low compared to the total columns
		Don't use on unique columns, use b-tree index instead
		Takes less space than corresponding btree index
	Creating
		create bitmap index on table_name(col_name);
		create bitmap unique index on table_name(col_name); -- this won't work
		create bitmap index on table_name(col_name) parallel;
		create bitmap index on table_name(col_name) tablespace tbs_name;
	Is the index being used
		Same as B-Tree index i.e, explain plan and autotrace
		Instead of range scan in b-tree we can find bitmap index
		If composite index is used with two columns and AND operation is used then we can find logical AND operation between the
			resultant bits of two bitmap index scans
			
ADDITIONAL INDEX TYPES -->>
	Function-Based indexes
		If function is applied to the indexed column then that index is ignored
		If function index is created and same clause is found in where then that index is considered
		Eg:
			where upper(name)='SANDESH'
				if index is created in name column then that index is ignored as upper function is applied to that column
				To solve this issue create function based index. EG:
					create index indx_name on tbl_name(upper(col_name)) parallel;
		Bitmap function based can also be created if required
	Bitmap join indexes
		Bitmap Join Indexes extend this concept such that the index contains the data to support the join query, 
			allowing the query to retrieve the data from the index rather than referencing the join tables. 
			Since the information is compressed into a bitmap, 
			the size of the resulting structure is significantly smaller than the corresponding materialized view.
		Restrictions
			No table can appear twice in the join.
			You cannot create a bitmap join index on an index-organized table or a temporary table.
			The columns in the index must all be columns of the dimension tables.
			The dimension table join columns must be either primary key columns or have unique constraints.
			If a dimension table has composite primary key, each column in the primary key must be part of the join.
		This is used to create index between two table based up some join criteria
		Usually involves a join between fact and dimension table(s)
		Dimension table needs primary key/unique constraint
		Usage
			Create primary key on dimension column
			create bitmap index btm_indx_name on
				fact_tbl_name(B.col_name)  -- here B.col_name is the subsetting criteria present after join in where clause. Index on B.col_name is also useful.
				from fact_tbl_name A, dim_tbl_name B
				where A.col_id=B.col_id; -- ON clause in join
		
	Index-Organized tables	
		Normally,
			index are stored separately from the table
			index stores ROWIDs for the table
		Index-Organized table stores data in B-Tree itself
		Index-Organized table is its own index
		Index-Organized table must have a primary key constraint
		Data access is faster since table and index are same entity
		Usage
			create table tbl_nm (........................, constaint pk_constraint primary key pk_col_name) organization index;
			insert into tbl_nm ......................;
			create index ix_nm on tbl_nm (col_nm) parallel;
			exec dbms_stats.gather_table_stats(ownname=>'hr'
												tabname=>'tbl_nm',
												estimate_percent=>dbms_stats.auto_sample_size,
												degree=>dbms_stats.default_degree);
												
			create table tbl_nm (........................, constaint pk_constraint primary key pk_col_name) organization index
				including col_name_1 overflow;
				-- this means columns after col_name_1 will be stored in overflow regions
				-- that means only columns prior and including to col_name_1 will be treated as a table
				-- this slims down the table
				
PARTITIONS -->>
	Overview
		Smaller tables implies faster query execution
		Smaller in the sense both rows and columns
		Break down table with huge data based up on some logical sections (smaller tables)
		If stored in multiple tables then to perform UNION between them will be nightmare; Partitioning is helpful in this scenario
		Partitioning is done by specifying some column and its value through which partitions are constructed
		Each partition can also be stored in the separate tablespace
		Partitions can specified beforehand of the data i.e, if done by year then partition can be made ready for another new year
		Types of partitions
			Simple: Range, List and Hash
				----- discussed in basic sections ----
			Composite: These are called partitions of partition
				Range-List(e.g, survey_date/respondent_gender,etc.)
					i.e, survey-date - range partition and respondent_gender - list partition
				List-List(e.g, likelihood_purchase/respondent_gender, etc.)
					i.e, likelihood_purchase - list partition and respondent_gender - list partitio
				and so on.....
		Partition-Wise joins
			Oracle can execute queries in parallel based on partitioning scheme
			Joins,
				tables with same partitioning scheme -> full partition-wise joins
				tables with different partitioning scheme -> partial partition-wise joins
				
	Simple Partitions
		List partition
			List partition only allows single column to be the partition key unlike range partition
			Partition can be defined using one or more values i.e,
				values (2004) or values (2004, 2005...);
			Can create partition with non-existent values
			Can make use of DEFAULT keyword for the unhandled values
			Each partition can be on its own tablespace
			Syntax
				create table tb_nm (....) 
				partition by list (col_name) (
					partition p_1 values (12,15,19) tablespace tbs_1,
					partition p_2 values (20),
					partition p_def values (default) tablespace tbs_2
				);
			Dictionary views
				ALL_TAB_PARTITIONS, USER_TAB_PARTITIONS, DBA_TAB_PARTITIONS
			Adding new partition in existing partitioned table 
				alter table tbl_name add partition p_name values (15); --this assumes we have not created default partition
				alter table tbl_name split partition p_def values (2010,2011,2012) into (partition p_012, partition p_def);
					-- if default partition is already present
					-- this will also move the value from default partition to newly created partition if condition satisfies
			Dropping
				Dropping partition as well as the data in it
					alter table tbl_name drop partition p_01; -- be cautious
				Merge partition with adjacent partition
					alter table tbl_name merge partitions p_01, p_02 into partition p_0102;
		Range partition
			Creates partitions using a range of values
			VALUES LESS THAN clause is used for this
			Partitions can be defined using one or more columns 
			MAXVALUE keyword us used for unhandled values similar to DEFAULT in list partition
			Can create partition with non-existent values
			Each partition can be placed in separate tablespace
			Syntax
				create table tb_nm (....)
				partition by range (col_name) (
					partition p_1 values less than (date '2005-01-01'),
					partition p_2 values less than (date '2006-01-01') tablespace tb_1,
					partition p_rest values less than (maxvalue)
				);
			Adding new partition in existing partitioned table 
				alter table tb_nm add partition p_2014 values less than (date '2014-01-01'); -- assume there is no maxvalue partition
				alter table tb_nm split partition p_rest at (date '2015-01-01') into (partition p_2014, partition p_rest); -- if maxvalue exists
			Dropping
				Dropping partition as well as the data in it
					alter table tbl_name drop partition p_1; -- be cautious
				Merge partition with adjacent partition
					alter table tbl_name merge partitions p_1, p_2 into partition p_12; -- adjacent partition merged					
		Interval partition
			This is the extension of range partitioning
			This partitioning scheme automatically creates new partition if the data does not fits any of the existing partition
			Syntax
				create table tb_nm (....)
				partition by range (col_name)
				INTERVAL(NUMTOYMINTERVAL(1, 'MONTH')) (
					partition p_1 values less than (date '2005-01-01'),
					partition p_2 values less than (date '2006-01-01')
				);
				-- here if the value 2006-10-11 is inserted in this table then the new partition will be created where range is upto 2006-11-01
		Hash partition
			In hash partitioning, the database maps rows to partitions based on a hashing 
				algorithm that the database applies to the user-specified partitioning key
			The destination of a row is determined by the internal hash function applied to the row by the database
			Syntax
				CREATE TABLE hash_sales
				( prod_id NUMBER(6)
				, cust_id NUMBER
				, time_id DATE
				, channel_id CHAR(1)
				, promo_id NUMBER(6)
				, quantity_sold NUMBER(3)
				, amount_sold NUMBER(10,2)
				)
				PARTITION BY HASH (prod_id)
				PARTITIONS 2;
			It creates two partitions and hash each key with some hashing function and evenly distributs them in the partitions
		Reference partition
			In reference partitioning, the partitioning strategy of a child table is solely defined with a parent table
			For every record in parent table in some partition, the record in child table will also be in the same partition
			This decreases the overhead of maintaining partition individually in all tables
			Syntax
				-- This is the parent table created with four partitions ( key need not to be primary key)
				CREATE TABLE orders
				( order_id NUMBER(12),
				order_date DATE,
				order_mode VARCHAR2(8),
				customer_id NUMBER(6),
				order_status NUMBER(2),
				order_total NUMBER(8,2),
				sales_rep_id NUMBER(6),
				promotion_id NUMBER(6),
				CONSTRAINT orders_pk PRIMARY KEY(order_id)
				)
				PARTITION BY RANGE(order_date)
				( PARTITION Q1_2015 VALUES LESS THAN (TO_DATE('01-APR-2015','DD-MON-YYYY')),
				PARTITION Q2_2015 VALUES LESS THAN (TO_DATE('01-JUL-2015','DD-MON-YYYY')),
				PARTITION Q3_2015 VALUES LESS THAN (TO_DATE('01-OCT-2015','DD-MON-YYYY')),
				PARTITION Q4_2015 VALUES LESS THAN (TO_DATE('01-JAN-2006','DD-MON-YYYY'))
				);
				
				-- This is the child created with the partition strategy as reference
				CREATE TABLE order_items
				( order_id NUMBER(12) NOT NULL,
				line_item_id NUMBER(3) NOT NULL,
				product_id NUMBER(6) NOT NULL,
				unit_price NUMBER(8,2),
				quantity NUMBER(8),
				CONSTRAINT order_items_fk
				FOREIGN KEY(order_id) REFERENCES orders(order_id)
				)
				PARTITION BY REFERENCE(order_items_fk);
			
	Composite Partitions
		Partition contained within a partition is called composite partition
		can create partition with non-existent values
		Sub-partition template can be created which saves time by preventing to create each subpartition by hand
		Can override subpartition template as necessary
		Make sure that partition and subpartition column are present in where clause in vast majority -> partition pruning
		Dictionary view
			user_tab_subpartitions, all_tab_subpartitions
			In subpartition_name column, name is displayed as partname_subpartname
		Types of composite partition
			List/List - List partitions with list subpartitions
			Range/List - Range partitions with list subpartitions
			Oracle 10gR1/R2: range partition with List/Hash subpartitions
			Oracle 11gR1: Range/List partition with Range/List/Hash subpartitions
			Oracle 11gR2: Range/List/Hash partition with Range/List/Hash subpartitions
		Eg List/List
			create table tbl_nm (..........)
			partition by list (col_integers) 
			subpartition by list (col_gender)
			subpartition template (	-- this is for subpartition
				subpartition sp_male values ('M', 'm'),
				subpartition sp_female values ('F', 'f'),
				subpartition sp_other values (DEFAULT)
			)
			(		-- this is for partition
				partition p_1 values (12,13,14),
				partition p_2 values (15,16)
			);
		Eg Range/List
			create table tbl_nm (..........)
			partition by range (survey_date) 
			subpartition by list (likelihood_purchase)
			subpartition template (	-- this is for subpartition
				subpartition sp_male values ('M', 'm'),
				subpartition sp_female values ('F', 'f'),
				subpartition sp_other values (DEFAULT)
			)
			(		-- this is for partition
				partition p_1 values less than (date '2019-01-01'),
				partition p_2 values less than (date '2020-01-01')
			);
		Overriding subpartition
			create table tbl_nm (..........)
			partition by range (survey_date) 
			subpartition by list (likelihood_purchase)
			subpartition template (	-- this is for subpartition
				subpartition sp_male values ('M', 'm'),
				subpartition sp_female values ('F', 'f'),
				subpartition sp_other values (DEFAULT)
			)
			(		-- this is for partition
				partition p_1 values less than (date '2019-01-01')(
					subpartition sp_male values ('M'),
					subpartition sp_female values ('F'),
					subpartition sp_other values (DEFAULT)
				),
				partition p_2 values less than (date '2020-01-01')
			);
		Adding subpartition
			alter table tbl_nm
				modify partition p_2
				add subpartition p_default values (DEFAULT);
				
PARTITION/INDEX INTERACTION -->>
	Partition means to slice the table into smaller once based upon some table value
	Index can also be created in each partitioned table they are called (local partitioned indexes)
	Index across all partitions are called (global non-partitioned indexes)
	Index that use different partitioning scheme than that of underlining table is called (global partitioned indexes)
	Global Non-Partitioned Index
		Same as the index learned earlier
		This does not care about any partitions, simply the index will be create in the whole table
		create index indx_name on tbl_nm(col_nm..);
		user_indexes, user_ind_columns
	Local partitioned indexes
		Inherits same partitioning scheme as table itself
		create index index_name on tbl_nm(col_nm) local;
		user_ind_partitions
			if status column of this table is unusable then that index is not being used
			change in partition cause this type of issue
			i.e, if the new partition is added or splitted then the index in that can be unusable
			to make usable
				alter index indx_name rebuild partition new_unusable_partname;
	FROM BOOK
		Let the partitioned table be
		create table tb_nm (....)
			partition by range (col_name) (
				partition p_1 values less than (date '2005-01-01'),
				partition p_2 values less than (date '2006-01-01') tablespace tb_1,
				partition p_rest values less than (maxvalue)
			);
		Local partitioned index
			In a local partitioned index, the index is partitioned on the same columns, with the same number of partitions and the same partition bounds as its table
			Each index partition is associated with exactly one partition of the underlying table
			The database automatically synchronizes index partitions with their associated table partitions
			Partition maintenance is simplified
			Local Prefixed index
				These index has the key same to the underlying partitioned table or if composite then contains preceeding columns
				Record bound is same as the partition
				While generating execution plan, partition pruning always happens
				Syntax
					create index idx_prefixed_example on tb_nm(col_name) local;
			Local non-prefixed index
				These index does not need to have same key as the partition key
				Record bound is same as the partition
				While generating execution plan, partition pruning might not always happens. This depends upon the predicate used
				Syntax 
					create index idx_non_prefixed_example on tb_nm(other_col_name) local;
		Global Non partitioned index
			These are the indexes which we always see
			Syntax
				create index idx_global_nopart_example on tb_nm(<other_col OR same_col>);
		Global partitioned index
			These indexes are independent of the partition scheme of the underlying table
			Syntax
				CREATE INDEX idx_global_part_example ON tb_nm (channel_id)
				GLOBAL PARTITION BY RANGE (some_col)
				(PARTITION p1 VALUES LESS THAN (3),
				PARTITION p2 VALUES LESS THAN (4),
				PARTITION p3 VALUES LESS THAN (MAXVALUE));
		
							
------------------------------------------------------------ DATABASE ADMINISTRATION -------------------------------------
		
ARCHITECTURE -->>
	Made up of three parts - 
		Oracle instance
		Oracle database storage
		Oracle server processes (SPs)
		RDBMS
			Instance
				Process
					PMON
					SMON
					DbWn
					Ckpt
					LGWR
					ARCn
					Optional (ARCn, ASMB, RBAL, Others)
				Memort Structure
					SGA
						Data buffer cache
						Shared pool
						Redo log buffer
						Large pool
						Java pool
						Streams pool
					PGA
			Database
				Data files
				Control files
				Redo log files
				Archived redo log files
				Parameter files
				Password file
				Backup files
				Trace files
				Alert log file
				
				
BACKGROUND PROCESSES -->>
	DATABASE WRITER - 
		Writes data from data buffer cache to the disk
		Dirty  buodified
		Cold buffer = ffer = mnot recently
		Writes the both types of buffers to the disk, such the buffer gets free space for other data.
		Can have more than one database writer according to the need.
		DB_WRITER_PROCESSES parameter can be used to configure more writers.
		When ? -- When a server process cannot find a clean reusable buffer, and when oracle needs to advance the checkpoint i.e,
			the oldest dirty buffer from which the database need to recover during failure i.e, in redo log file
			
	LOG WRITER -
		Redo log buffer is the cyclic buffer, 3 in count
		Responsible for writing from redo log buffer to redo log files
		When ? --
			When a user process commits a transaction
			When the redo log buffer is one-third full
			Before a DBWn process writes modified buffers to disk
			Every 3 seconds
			
	CHECKPOINT PROCESS -
		Checkpoint is a database event which synchronizes the modified data blocks in memory with the data files on disk
		It also updates the datafile header and control file with the latest checkpoint System Change Number (SCN)
		When ? --
			At each switch of the redo log files
			Once the number os seconds defined in the LOG_CHECKPOINT_TIMEOUT is reached
			Once the current redo log file reaches the size
				LOG_CHECKPOINT_INTERVAL * size of IO OS blocks
			Directly by the ALTER SYSTEM SWITCH LOGFILE command
			Directly with the ALTER SYSTEM CHECKPOINT command
			
	SMON (System Monitor) PROCESS -
		Performs recovery at the instance startup
		Clears the temporary unused segment
		
	PMON (Process Monitor) PROCESS - 
		Performs process recovery when a user process fails
			Cleans up the buffer cache
			Free resource that are used by the user process
		Restarts stopped running dispatchers and server processes
		Dynamically registers database service with network listeners
		
	RECO (Recoverer) PROCESS - 
		Used with the distributed database configuration
		Automatically resolves all in-doubt transactions
		
	ARCn (Archiver) PROCESS -
		Copy redo log files to a designated storage device after a log switch occurs
		Runs only in ARCHIVELOG mode
		LOG_ARCHIVE_MAX_PROCESS param to configure the number of archiver process
		
		
MEMORY STRUCTURES -->>
	SGA (Shared Global Area) 
		Program code (PL/SQL code, Java code)
		Cached data shared among users
		Redo log entries
		Information about currently connected sessions etc.
		This is shared across multiple users
		
		BUFFER CACHE -
			holds the copies of data blocks that are read from data files
			shared by all concurrent users
			number of buffers defined by DB_BLOCK_BUFFERS
			size of a buffer based on DB_BLOCK_SIZE
			stores the most recently used blocks
			Data must first come in buffer before the user can read it
			
		SHARED POOL -
			Library cache
				Contains statement text, parsed code and an execution plan, Contains one or more sql area
				Shared SQL area
					contains a parse tree and execution plan for a given SQL statement
			Data dictionary cache
				Contains table and column definitions and privileges
			Result cache
				It stores the result of sql query and the plsql query
				If same query is run multiple times then it gives back the same result
			Size of shared pool SHARED_POOL_SIZE param
			
		REDO LOG BUFFER -
			This is a circular buffer which stores the changes made to the database
			DML, DDLs etc..
			Useful during database recovery
			Size is defined by LOG_BUFFER
			
		LARGE POOL -
			This is an optional memory area which provide large memory allocations
			If not defined then will reduce performance as the shared pool is used instead
			Parallel query operations
			Oracle backup and restore operations
			Make heavy operations use large pool instead of shared pool
			This also stores UGA section in case of shared server architecture
			
		JAVA POOL and STREAMS POOL - 
			Used to store java code and java object required for java virtual machine
			Size defined by JAVA_POOL_SIZE
			
			Used to Oracle streams to store buffered queue message and provide memory for oracle streams processes
			Size defined by STREAMS_POOL_SIZE
			
		KEEP BUFFER POOL -
			Data which is frequently accessed can be pinned in the keep buffer pool
			In case if buffer cache is full and the data is to be removed will cause the problem if that data is highly used
			To create keep buffer pool DB_KEEP_CACHE_SIZE
			
		RECYCLE BUFFER POOL -
			Recycle pool is reserved for large tables that experience full table scans that are unlikely to be reread or accessed rarely
			Size defined by DB_RECYCLE_CACHE_SIZE
			
		nK BUFFER CACHE -
			Is used to store oracle data blocks having different size then the default block size (8192k)
			Default block size is denoted by DB_BLOCK_SIZE
			Size defined by DB_NK_CACHE_SIZE
			Eg: DB_2K_CACHE_SIZE = 0M, DB_4K_CACHE_SIZE = 0M etc ...
			
		
	PGA (Program Global Area)
		This area of memory is not shared.
		Users have their own PGA to work in their session
		PGA contains private SQL area for each server processes for each user connection
			That private SQL will be stored in Shared SQL area inside library cache checking if all are running similary query
		Its a private memory region containing data and control information for a server process
		
	Useful
		show parameter SGA_TARGET; -- total memory allocated for SGA
		show parameter PGA_AGGREGATE_TARGET; -- total memory allocated for PGA
		select * from v$pgastat; -- info regarding PGA
		select * from v$sgastat; -- info regarding SGA
		SELECT VALUE/1048576 FROM V$PGASTAT WHERE NAME='maximum PGA allocated'; -- max space allocated for PGA since the start of database (MAXIMUM PGA ALLOCATED)
		MEMORY_TARGET 
			if this is set then auto mem management will be done between SGA and PGA
			MEMORY_TARGET = SGA_TARGET + MAX(PGA_AGGREGATE_TARGET, MAXIMUM PGA ALLOCATED) -- set memory target as given to estimate the memory required
		MEMORY_MAX_TARGET
			set this variable forseeing the value that might be required in the future
		if MEMORY_TARGET is set to some value then set SGA_TARGET and PGA_AGGREGATE_TARGET as 0. Such that there is no restriction on sizes during auto mem management.
		V$MEMORY_DYNAMIC_COMPONENTS
			-- shows the current sizes of all dynamically tuned memory components
		V$MEMORY_TARGET_ADVICE
			-- view tuning advice for the MEMORY_TARGET during auto memory management
		
		
		
DEDICATED VS SHARED SERVERS -->>
	Dedicated servers -
		One server process is spawned for each user connection
		Consumes lots of memory
		Greater performance
		Listener is responsible for spawning the server process per user connection
			Once the server process is spawned client will directly interact through it without going through listener
		In case of DS UGA is stored in PGA
			For dedicated server, I will *always* have the same process - its dedicated to me - so I can keep my state in the PGA
	Shared servers -
		This is alternative to dedicated servers to prevent high usage of server resource
		In this case, listener will fork a dispatcher process which is light weight process
			Dispatcher process will then put the request in queue which will then served by shared server
			The response from shared server is then put into response queue and then sent to respective client
		Server processes are shared among multiple connections
		It provides scalability
		Here first the user request is handled through Dispatcher process and is stored in the request & response queues. Then from that shared process performs its actions.
		Nobody is using these servers nowadays.
		In shared servers PGA contains only stack space and SGA contains User global areas (UGA). Both are in PGA in case of dedicated servers.
			So since I might be allocated to *any* process, I need somewhere where they can *all* see my current memory state (UGA). So it sits in the SGA.
			
		How I make it happen:
			Update TNS entry to contain SHARED in server field instead of DEDICATED
			alter system set dispatchers = '(PROTOCOL=TCP) (DISPATCHERS=4)';
			alter system set shared_servers = 5;
			ps -ef | egrep ora_d[0-9]{3}   -- in bash gives the total dispatchers running
			ps -ef | egrep ora_s[0-9]{3}   -- in bash gives the total shared servers running
		
ORACLE INSTANCE -->>
	Is loaded into the memory each time database starts (In memory nature)
	Contains shared memory caches (Shared pool, buffer cache, redo buffer etc.)
	Contains Oracle's background processes
	Instance will be removed from the memory once it is shut down (non persistent nature)
	One instance is mounted to only one database at a time.
		In 12c multi tenant architecture many pluggable database are there for single container database. 
	
DATABASE STORAGE -->>
	This is the storage which contains oracle files to store the data
	This makes the physical oracle database
	Microsoft word and microsoft word file is analogous to oracle instance and oracle file
	Data storage is usually - Both are natively supported by oracle
		SAN -  Storage Area Netword (Accessed by fibre network)
		NAS -  Netword Attached Storage 
	Data can also be place in local disk (not recommended - single point of failure)	
	Different types of files
		Parameter file - 
			This file stores the info about various database characteristics
			Without this file database cannot be taken to nomount stage
			Types of parameter files
				TNSNames.ora
					Alias to connect to database
				Listener.ora
					Tells in which port listener will be listening
				spfile<DBName>.ora
					This is called database parameter file and is the most important
					During the startup it will search for the parameter file
					Previously it is called init<DBName>.ora
						This is the string file which contains key value pair
						As this is the string file, two or many file can be created and location can be sent during database startup.
							This will create multiple point of truth and will be confusing.
					Three parameters are enough to have in param file to start the database
						Control file, db_block_size, db_name
		
SERVER PROCESSES (SPs) -->>
	Server processes are spawned when user is trying to connect to database.
	Each user will have the dedicated server process unless pooling is used with the help of middleware.
	It's role -
		verify the syntax of sql query that the user is trying to execute
		it then read data from disk and also load it in the buffer cache
		
PARALLEL PROCESSES -->>
	The execution of many SQL statements can be parallelized. 
	The degree of parallelism is the number of parallel execution servers that can be associated with a single operation.
	ps -ef | egrep ora_p[0-9]{3}   -- listing out the parallel servers running (after p009 it will be p00a)
	parallel_min_servers -- displays the minimum parallel servers running
	select * from v$px_process; -- table to display the parallel services running
	select * from v$process; -- info about all currently active processes
		
		
LOGICAL AND PHYSICAL DATA STRUCTURES -->>
	Database (L) - Tablespace (L) - Data Files (P) - Segment (L) - Extent (L) - Data block (L)
	Tablespace can contain one or more data files, whereas one datafile can be mapped to only one tablespace
	
	DATA BLOCK -
		A data block is the smallest unit of storage in an Oracle database
		Common and variable header
			It contains the address of block, general block info, type of segment eg. data or index segment
		Table directory
			Info about table having the row in the block
		Row directory
			Info about the row for which the data is stored
		Row data 
			Actual area in which the data is stored
		Free space
		
	TABLESPACE -
		Three types Permanent, UNDO and Temporary
			Permanent for storing info like tables, indexes etc...
			UNDO for the rollback and data consistency during select
			Temporary for storing the temp data like of join, order by etc.
		DEFAULT tablespaces	
			SYSTEM		- Manage the database, data dictionary and other adminstrative objects, cannot take offline, can store user data but strict no
			SYSAUX		- Introduced after 10g, auxillary to system, some of the stuffs stored in system are now in sysaux table space, cannot take offline
			TEMP		- Temporary table space, used to store temporary data, sort , merge, joins etc
			UNDOTBS1	- Undo data
			USERS		- Used to store all the data created by users by default if the tablespace for that user is not allocated
			EXAMPLE		- This schema is created if the scripts file is ran during oracle installation, sample schemas
			
		Online and Offline tablespace -
			Users can connect and read data from is tablespace is online
			Users cannot read write if tablespace is offline, moving a datafile, recover datafile and tablespace, offline tablespace backup etc.
			Tablespace as default will be in online state
			
		Bigfile and Smallfile tablespace - 
			An Oracle database can contain both bigfile and smallfile tablespaces.
			System default is to create the traditional smallfile tablespace.
			The SYSTEM and SYSAUX tablespaces are always created using the system default type.
			Bigfile tablespaces are supported only for locally managed tablespaces with automatic segment-space management.
			
			Smallfile
				By default table space is smallfile tablespace
				Smallfile tablespace can contain multiple datafiles
				To alter for resizing and autoextend on, we need to do it for each datafile. Cannot be done in tablespace level
				i.e, alter database datafile '/mnt/san_storage/oradata/orcl/data01.dbf' resize 5m;
					 alter tablespace tbs1 resize 5m; 
						--  the error will be ORA-32773: operation not supported for smallfile tablespace TBS1
			Bigfile
				The bigfile tablespace simplifies large database tablespace management by reducing the number of datafiles needed.
				The bigfile tablespace simplifies datafile management with Oracle-managed files and Automated Storage Management (ASM) by eliminating the need for adding new datafiles and dealing with multiple files.
				The bigfile tablespace allows us to create a bigfile tablespace of up to eight exabytes (eight million terabytes) in size, and significantly increase the storage capacity of an Oracle database.
				The bigfile tablespace follows the concept that a tablespace and a datafile are logically equivalent.
				For creating bigfile tablespace we should explicitly define the type
				Bigfile tablespace can only contain one file
				Altering on tablespace level is also allowed in here
			
		Useful -
			select * from dba_data_files;
			select * from dba_tablespaces;
			select tablespace_name, bytes/1024/1024 blocks_in_mb from dba_free_space where tablespace_name = 'TBS1';
			select tablespace_name, file_name, bytes/1024/1024 blocks_in_mb from dba_data_files;
			
			create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on reuse next 512k maxsize 200M;
				autoextend on to increase the size if the initial size is full
				next 512k is to increase the datafile size by 512 if data file is full
				maxsize is the size till which the tablespace can be extended upto
				reuse option to use the datafile if exists
			create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 1m;
			alter tablespace tbs1 add datafile '/mnt/san_storage/oradata/orcl/data02.dbf' size 5m;  -- adding datafile
			alter tablespace tbs1 drop datafile '/mnt/san_storage/oradata/orcl/data02.dbf';  -- dropping datafile
			drop tablespace tbs1;  -- dropping the tablespace
			drop tablespace tbs1 including contents and datafiles; -- including datafiles and contents
			alter tablespace tbs1 rename to tbs2;  --renaming the tablespace
			
			-- for renaming the datafile first take tablespace to offline
			alter tablespace tbs1 offline;
			alter database rename file '/mnt/san_storage/oradata/orcl/data01.dbf' to '/mnt/san_storage/oradata/orcl/data11.dbf';
				for this, first rename data01.dbf to data11.dbf in filesystem using mv command and then run this command otherwise error
			alter tablespace tbs1 online;
			
			if datafile is full - either of the three
				alter database datafile '/mnt/san_storage/oradata/orcl/data01.dbf' resize 5m;   -- resizing
				alter tablespace tbs1 add datafile '/mnt/san_storage/oradata/orcl/data02.dbf' size 5m; -- adding
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on; -- create tablespace with autoextend
			
			Tablespace with different block size
				Bigger size needed
					Larger tables that are the target for full table scans
					table with larger objects (LOB's, BLOB's, CLOB's)
					temporary tablespace for sorting
					tables with large rows that might lead to chained/migrated rows
				show parameter db_block_size;
				alter system set db_16k_cache_size=60m scope=both; -- both means spfile and memory
				show parameter db_16k_cache_size;
				create tablespace tbs2 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m blocksize 16k;	
		
		TEMPORARY TABLESPACE MANAGEMENT
			This is the tablespace in which temporary data are stored like sort, groupby, etc.
			By default user are assigned to temp tablespace.
			We can create other temp table space and alter user to belong to that temp tablespace.
			Used during joins, sorts and other temporary operations
			select * from dba_temp_files;
			select tablespace_name, file_name, bytes/1024/1024 blocks_in_mb from dba_temp_files;
			alter database tempfile '/mnt/san_storage/oradata/orcl/temp01.dbf' resize 140m; -- resizing default temp tablespace
			create temporary tablespace temp2 tempfile '/mnt/san_storage/oradata/orcl/temp02.dbf' size 20m autoextend on next 1m maxsize 60m;
			select * from database_properties;  -- this will display various database configuration stuffs like defualt temp tablespaces etc..
			alter database default temporary tablespace temp2;  -- sets defualt temp tablespace as temp02
			
		TEMPORARY TABLESPACE GROUP
			Multiple tablespaces can be assigned to a group such that those groups can be assigned to the users
			Group will be create while first tablespace is added and removed when the last tablespace is removed from the group
			create temporary tablespace temp2 tempfile '/mnt/san_storage/oradata/orcl/temp02.dbf' size 20m autoextend on next 1m maxsize 60m
				tablespace group tempgroup1 ; -- cannot be used with other types of tablespaces
			select * from dba_tablespace_groups; -- lists all the groups
			alter tablespace temp2 tablespace group tempgroup1;
			select * from database_properties;
			alter database default temporary tablespace tempgroup1;  -- sets defualt temp tablespace as the group of temp tablespaces
			
		LOCALLY VS DICTIONARY MANAGED TABLESPACE
			In dictionary managed tablespace, all the extent related info is stored in the dictionary for the tablespace
				This is slower approach as the dictionary table is to be queried to fetch the information
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M
					extent management dictionary default storage (initial 50k next 50k minextents 2 maxextents 50); -- only for 10g and before
					
			In dictionary managed tablespace, all the extent related info is stored within the tablespace itself
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M
					extent management local autoallocate; -- must use locally managed 11g onwards
					
				create tablespace tbs1 datafile '/mnt/san_storage/oradata/orcl/data01.dbf' size 10m autoextend on next 512k maxsize 200M
					extent management local uniform size 128k; -- must use locally managed 11g onwards
					
					
		UNDO DATA
			It is the copy of pre modified data that is captured for every transaction that changes data.
			
			Why ?
				Rollback transactions
				Support read consistent queries
				Support flashback operations
				Recover from failed transactions
			How long does undo data stay ?
				User commits the transaction
				User rollbacks the transaction
				User execute DDL statements (create, drop, alter, rename)
				User session terminated abnormally (transaction rollbacks)
				User session terminates normally with an exit (transaciton commits)
			Where does undo data stay ?
				Stored in undo segments/undo tablespace
				Only 1 active tablespace for an instance
				They are owned by the user sys
			With 11g, automatic undo management is the default mode
				UNDO_MANAGEMENT = AUTO / MANUAL; show parameter undo_management;
			An auto extended UNDO tablespace named UNDOTBS1 is automatically created while creating database with DBCA.
			SYSTEM tablespace is used , if no UNDO tablespace is defined.
			UNDO retention period is managed very effectively. UNDO retention period is the number of seconds the committed undo data is retained.
			Only one undo table sapce can be online at a time
			Useful
				alter tablespace undotbs add datafile '/mnt/san_storage/oradata/orcl/undotbs01.dbf' size 5m;  -- adding datafile
				create undo tablespace undotbs02 datafile '/mnt/san_storage/oradata/orcl/undotbs02.dbf' size 10m reuse autoextend on next 512k maxsize 200M;
					reuse option is used to use the datafile if already present
				alter system set undo_tablespace=undotbs2; -- changing the used undo tablespace
				select segment_name, owner, tablespace_name, status from dba_rollback_segs;
			CONFIGURING UNDO RETENTION PERIOD
				UNDO_RETENTION - 1800
				ALTER SYSTEM SET UNDO_RETENTION = 2400
				After a transaction is committed, undo data is no longer needed for rollback. But still it is persisted for consistent read purposes.
				Furthermore, the success of several oracle flashback feature also depend upon the availability of older undo information
				Useful
					show parameter undo_retention;
					alter system set undo_retention=2400;
			RETENTION GUARANTEE
				This guarantee that the data exists for the specified amount of time (UNDO_RETENTION) in the undo tablespace
				If the space is not available in undo tablespace then it will fails any other transaction but still persist the data in the tablespace.
				alter tablespace undotbs1 retention guarantee; -- guarantee the retention (needs the space)
				alter tablespace undotbs1 retention noguarantee; -- doesnot guarantee
				select tablespace_name, retention from dba_tablespaces;  -- check for the retention policy
				
REDO CONCEPTS -->>
	Redo log files enable the oracle server or DBA to redo transactions if a database failure occurs
	Only purpose is to enable recovery
	Redo log entries from buffer is stored in the redo log files	
	Management
		Stores redo log files as a group, in which one group contains many redo log files stored in different disk
		The data is written in circular fashion
		Current log file
			The redo log file to which LGWR is actively writing
		Active log file
			Log files required for instance recovery
			Active log files cannot be overwritten by LGWR until ARCn has archived the data when archiving is enabled
		Inactive log file
			Log files no needed for instance recovery
	Useful
		select * from v$logfile;
		select * from v$log;
		alter database add logfile member '/mnt/san_storage/oradata/orcl/redo01b.log' to group 1;
		alter database add logfile member '/mnt/san_storage/oradata/orcl/redo02b.log' to group 2;
		alter database add logfile member '/mnt/san_storage/oradata/orcl/redo03b.log' to group 3;
			now each group has 2 log files
			by default their status will be invalid as the logwriter has not used them and will be changed once used
			Manually we can switch the logfiles to see the result quickly.   -- alter system switch logfile;
		alter database drop logfile group 3;  -- group 3 log files are deleted, for this the happen the group should be in inactive status
		alter database add logfile group 3 ('/mnt/san_storage/oradata/orcl/redo03.log', '/mnt/san_storage/oradata/orcl/redo03b.log') size 10m reuse;
			add group and the logfile within them dissimilar with the above way of addition in which we already have the group
			reuse option is used to use the datafile if already present
			
	REDO LOGS ARCHIVAL
		Redo log files enable the oracle server to redo transactions if a db failure occurs
		ARCn process is responsible for archiving redo log files
		ARCn process only works if the database has archive log mode enabled
		Useful
			archive log list;   -- to view the log mode of the database
			shutdown immediate;
			startup mount;
			alter database noarchivelog;  // alter database archivelog;
			alter database open;
			USE_DB_RECOVERY_FILE_DEST as archive destination points to the fast recovery area.
			select thread#, sequence#, name from v$archived_log; -- view archived logs
			
			
USER MANAGEMENT -->>
	Database user account is a way to organize the ownership of database objects and access rights.
	Each user account has
		A unique username,
		An authentication method
		A temporary tablespace
		A default tablespace
		Account status
		Initial consumer group
		User profile
	Default administrative accounts 
		SYS - all privileges, can perform anything, owns database dictionary
		SYSTEM - powerful account, cannot perform backup and recovery, cannot perform database upgrade
		DBSNMP - Used by oracle enterprise manager (graphical tool provide by oracle for performing admin tasks) to monitor the database
		SYSMAN - Used by oracle enterprise manager to perform administrative tasks
	Useful
		select * from session_privs;   -- to check the privileges
		create user user_name identified by password;
		create user user_name identified by password password expire;
		create user john profile default identified by password profile default default tablespace USERS temporary tablespace TEMP account unlock;
		grant connect to user_name/role;
		grant create tablquota 5m on tblsp_name;  -- altering the quota on the tablespace
		grant select any to user_name [with admin option]; -- with admin option guarantees that the created user can grant that privilege to others
		alter user user_name  table to user_name; -- 'any' clause means the user can select tables from all schema
		select * from dba_ts_quotas;   -- check the space of user in tab
		lespace
		
		GOTO ---create user and permissions--- SECTION FOR MORE QUERIES
		More on (USERS AND PERMISSIONS) -- Database security concepts
		
	SYSTEM VS OBJECT PRIVILEGE - 
		Object privilege is the action that are allowed to be operated on certain users.
			SELECT, UPDATE, DELETE, ALTER, EXECUTE
			grant <object_privilege> on <object> to <grantee clause> -- general
			grant delete on john.student to tom [with grant option]; -- perform delete on student table of john to tom user
			........ [with grant option] --- privilege will cascade on revoke
		System privilege are others studied
			......[with admin option] -- with grant option is only for object privilege
			on revoking the privilege, the revoke won't cascade in case of [with admin option] for system privilege
			
	DATABASE-WIDE SYSTEM PRIVILEGE - 
		Any keyword is used to provide database wide system privilege
		Any system privilege is powerful
		grant create any table to user_name; -- can create table in any schemas
		grant alter any table to user_name;
		grant drop any view to user_name;
		grant execute any procedure to user_name;
		The SELECT ANY TABLE, INSERT ANY TABLE, UPDATE ANY TABLE, or DELETE ANY TABLE system privileges
			
	USER PROFILE - 
		Profiles are used to set limits on database resource
		Each user can have only one profile and is by default set to 'default' profile
		Profiles are used to control resource consumption, manage account status and password expiration
		Resources
			CPU, Disk, Connect Time, Idle Time, Concurrent sessions, Private SGA, etc....
		Useful
			select * from dba_profiles;    -- list of profiles and the list of resource names
			select * from dba_users;  -- list of users and the profile they are in
			create profile profile_name limit 
				sessions_per_user 2       -- can open two sessions max
				idle_time 5					-- can remain idle for 5 min then session is closed
				connect_time 10;			-- can stay connected for 10 min then session is closed
			create profile password_prf limit
				password_life_time 180
				password_grace_time 7
				password_reuse_time unlimited
				password_reuse_max unlimited
				failed_login_attempts 10
				password_lock_time 1
				password_verify_function verify_function_11g;    -- many more parameters to set
				
				
NETWORKING CONCEPTS -->>
	Oracle net service runs boths in the client and server.
	When a user tries connecting to the database then the request first goes through the net service running in client and then to the listener.
		client (oracle net) ---- listener ---- server (oracle net)
	Once the connection is established, then the user process will directly interact with the server process without listener.
	listener.ora file is present in the oracle server which has the info regarding the network related configuration parameters.
		this file must contain protocol, host and port number. Service name is optional and should be listed if multiple services are running
	Listener can be configured using following approaches
		enterprise manager, oracle net manager, oracle net configuration assistant, command line
		'netca' command can be used to run oracle net configuration assistant
	tnsnames.ora file is used to bundle up the host, protocol, port and service name information such that those parameter can be replaced with the name during connection
		we can directly add the tns entry inside tnsnames.ora file
		'netmgr' tool can be launched which also let us add the entry graphically
		
		
DATABASE LINKS -->>
	Database link or dblink is the schema object which let us connect to the other database remotely
		eg: from local branch to the headquarters host
	Other database need not be some, it can be two different databases
	create database link dblink_name
		connect to schema_name_to_connect_to identified by password_name_of_that_remote_schema
		using 'tns_entry_name';       -- creating the database link, this is private database link
	select * from table_name@dblink; -- selecting from the remote database
	select * from dba_db_links;  -- selecting the dblinks present in the database
	create public database link dblink_name
		connect to schema_name_to_connect_to identified by password_name_of_that_remote_schema
		using 'tns_entry_name';  -- public database link, can be used by anybody on that database
	We must have database backup before the time in which we are restoring the database
		and the archived redo log for the period between the SCN of the backups and the target SCN (System Change Number)


TERMINATING SESSION -->>
	When a session is terminated, any active transactions of the session are rolled back.
	Resources held by the session (such as locks and memory areas) are immediately released and available to other sessions.
	Useful
		ALTER SYSTEM KILL SESSION '7,15'; -- 7=system identifier and 15=serial number
		ALTER SYSTEM KILL SESSION '7,15' NOREPLAY; -- noreplay makes sure that the session won't be recovered again
		DBMS_SERVICE.DISCONNECT_SESSION can also be used to terminate the session.
			-- specify DBMS_SERVICE.NOREPLAY for the disconnect_option to make it unrecoverable
			DEMO:
				BEGIN
					DBMS_SERVICE.DISCONNECT_SESSION(
					service_name => 'sales.example.com',
					disconnect_option => DBMS_SERVICE.NOREPLAY);
				END;
				/
		select * from V$SESSION; -- provides session info (session is ACTIVE if it is making SQL calls)
		
		
MONITORING DATABASE -->>
	Errors and Alerts
		The trace file and alert log contain information about errors
		Each server and background process can write to an associated trace file
		When an internal error is occurred, the info is written to the corresponding trace file
		Alert log is a chronological log of messages and errors
			Trace file info is also written in the alert log
			Goto given trace file to view the issue
		Location: DIAGNOSTIC_DEST
				  BACKGROUND_DUMP_DEST
				  CORE_DUMP_DEST
	    Control size of alert log file by deleting the file and archiving it otherwise system will keep on appending the content.
		Control size of trace file with MAX_DUMP_FILE_SIZE - UNILIMITED, <some_number><K,M,G>
			File will be segmented if that given file size is reached
		Background processes always write to a trace file when appropriate
			For ARCn process it is possible to control what and how much to be written using parameter LOG_ARCHIVE_TRACE param
		Regardless of param SQL_TRACE background processes can write to trace file altering its session
			ALTER SESSION SET SQL_TRACE TRUE;
			DBMS_SESSION, DBMS_MONITOR -- use them to control SQL tracing for a session
	Server-Generated Alerts
		A server-generated alert is a notification from the Oracle Database server of an impending problem
		A server-generated alert may contain suggestions for correcting the problem
		Notifications are also provided when the problem condition has been cleared
		Alerts are automatically generated when a problem occurs or when data does not match expected values for metrics, such as the following:
			• Physical Reads Per Second
			• User Commits Per Second
			• SQL Service Response Time
		Server-generated alerts can be based on threshold levels or can issue simply because an event has occurred
			The value of these levels can be customer-defined or internal values, and some alerts have default threshold levels which can be changed if appropriate
			Eg: tablespace usage exceeds 85% etc.
		Some example not base on threshold issues
			• Snapshot Too Old
			• Resumable Session Suspended
			• Recovery Area Space Usage
	Monitoring Performance
		Locks
			Locks are mechanisms that prevent destructive interaction between transactions accessing the same resource
			To monitor locks:
				1. Run the catblock.sql, which creates lock views.
				2. Run the utllockt.sql script, which uses the views created by catblock.sql to display, in a tree fashion, the sessions in the system that are waiting for locks and
						the locks that they are waiting for.
			Useful
				V$LOCK, DBA_BLOCKERS, DBA_WAITERS, DBA_DDL_LOCKS, DBA_DML_LOCKS, DBA_LOCK, DBA_LOCK_INTERNAL, V$LOCKED_OBJECT, V$SESSION_WAIT, V$SYSSTAT, V$RESOURCE_LIMIT,
				V$SQLAREA, V$LATCH
	Monitoring quarantined objects
		Object quarantine isolates an object that has raised an error and monitors the object for impacts on the system.
		Some process might cause database to abort. Such resources might be isolated in memory such that database can continue to run
		V$QUARANTINE view store the objects that are currently quarantined
		
		
CONTROL FILE MANAGEMENT -->>
	Every Oracle Database has a control file, which is a small binary file that records the physical structure of the database.
	Without the control file, the database cannot be mounted and recovery is difficult.
	specify control file names using the CONTROL_FILES initialization parameter 
	Reading control file:
		strings controlfile_name.extension -- use string to read binary file
	Every Oracle Database should have at least two control files, each stored on a different physical disk
		This is called control file multiplexing
	It is important to backup control file whenever database structure changes
		• Adding, dropping, or renaming data files
		• Adding or dropping a tablespace, or altering the read/write state of the tablespace
		• Adding or dropping redo log files or groups
	The main determinants of the size of a control file are the values set for the
		MAXDATAFILES, MAXLOGFILES, MAXLOGMEMBERS, MAXLOGHISTORY, and MAXINSTANCES params in CREATE DATABASE statement
	Initially control files are created during database creation phase
		names of the control files are mentioned in CONTROL_FILES param
		if files already exists then use CONTROLFILE REUSE clause with the CREATE DATABASE statement
	Creating control file
		Must create new control files in certain situations.
		It is necessary to create new control files in the following situations:
			• All control files for the database have been permanently damaged
			• If database name is to be changed
		Useful
			CREATE CONTROLFILE
				SET DATABASE prod
				LOGFILE GROUP 1 ('/u01/oracle/prod/redo01_01.log',
								/u01/oracle/prod/redo01_02.log'),
						GROUP 2 ('/u01/oracle/prod/redo02_01.log',
								'/u01/oracle/prod/redo02_02.log'),
						GROUP 3 ('/u01/oracle/prod/redo03_01.log',
								'/u01/oracle/prod/redo03_02.log')
				RESETLOGS
				DATAFILE '/u01/oracle/prod/system01.dbf' SIZE 3M,
						'/u01/oracle/prod/rbs01.dbs' SIZE 5M,
						'/u01/oracle/prod/users01.dbs' SIZE 5M,
						'/u01/oracle/prod/temp01.dbs' SIZE 5M
				MAXLOGFILES 50
				MAXLOGMEMBERS 3
				MAXLOGHISTORY 400
				MAXDATAFILES 200
				MAXINSTANCES 6
				ARCHIVELOG;
		If after losing control files we donot know what to include in those logfile and datafile
			SELECT MEMBER FROM V$LOGFILE; -- listing all log files
			SELECT NAME FROM V$DATAFILE; -- listing all data files
			SELECT VALUE FROM V$PARAMETER WHERE NAME = 'control_files'; -- listing control files
		If no such above list is present then
			try to locate all datafiles and redo files that constitute the database
		Backing up control file
			ALTER DATABASE BACKUP CONTROLFILE TO '/oracle/backup/control.bkp'; -- backup control file to a binary file
			ALTER DATABASE BACKUP CONTROLFILE TO TRACE; -- creates trace file which can be used to reproduce the control file later on
				-- the location of trace file is mentioned in the alert log file
				-- found in: /u01/app/oracle/diag/rdbms/orcl/orcl/trace/orcl_ora_19528.trc
		During crash
			Place all data files and redo log files in the desired location
			Open database in NOMOUNT
			Issue CREATE CONTROLFILE statement
			Specify RESETLOGS clause if you have lost any redo log groups in addition to control file
			Edit CONTROL_FILES initalization param to indicate all of the control files
			If new control file was create using RESETLOGS
				must specify USING BACKUP CONTROL FILE -- recovers the lost online redo logs, archive redo log files, or data files
				i.e, recover database using backup control file;
			Then open the database
				ALTER DATABASE OPEN;
				ALTER DATABASE OPEN RESETLOGS;
			On restoring control file
				https://web.stanford.edu/dept/itss/docs/oracle/10gR2/backup.102/b14191/osrecov005.htm
	NOTE:
		If all datafile is not mentioned in the control file and resetlogs is performed then the recovery is not possible
		In that case the left datafile and tablespace needs to be dropped
		If more datafile is mentioned in control file than in reality then that entry will be removed from the control: no problem
		
		
REDO LOG MANAGEMENT -->>
	The most crucial structure for recovery operations is the redo log
	They are preallocated files that store all changes made to the database as they occur
	Every instance of an Oracle Database has an associated redo log to protect the database in case of an instance failure
	Redo Threads
		When speaking in the context of multiple database instances, the redo log for each database instance is also referred to as a redo thread
		In typical configurations, only one database instance accesses an Oracle Database, so only one thread is present
		This way the contention is removed from the database
	Redo Contents
		These are the records or redo entries in the redo log files
		Redo entries record data that can be used to reconstruct all changes made to database including undo segments
		Whenever commit is performed content from redo buffer is written to the redo log files and assigns a system change number (SCN) to identify the redo records
	The redo log file that LGWR is actively writing to is called the current redo log file	
	Redo log files that are required for instance recovery are called active redo log files
	Redo log files that are no longer required for instance recovery are called inactive redo log files
	Log switches and log sequence numbers
		A log switch is the point at which the database stops writing to one redo log file and begins writing to another
		It can be configured to occur at regular intervals
		It can also be forced to happen manually
			Configure ARCHIVE_LAG_TARGET to define when to switch and starts archival if mode is archivelog
		Oracle Database assigns each redo log file a new log sequence number every time a log switch occurs
	Planning redo logs
		Multiplexing can be performed in redo log files, allowing two identical copies to be stored in separate locations
		Multiplexing is implemented by creating groups of redo log files
		Whenever LGWR cannot write to a member of a group, the database marks that member as INVALID
			and writed error message to LGWR trace file and to database alert log
		Planning block size of redo log files
			Unlike the database block size, which can be between 2K and 32K, redo log files always default to a block size of physical sector size of disk
			With higer block size there could be increased redo wastage
			Useful
				SELECT name, value FROM v$sysstat WHERE name = 'redo wastage';
				select blocksize from v$log;
			specify the block size of online redo log files with the BLOCKSIZE keyword in
				CREATE DATABASE, ALTER DATABASE and CREATE CONTROLFILE statements
		Useful
			ALTER DATABASE orcl ADD LOGFILE
				GROUP 4 ('/u01/logs/orcl/redo04a.log','/u01/logs/orcl/redo04b.log')
				SIZE 100M BLOCKSIZE 512 REUSE; -- adds a new group 4 of redo logs to the database
			ALTER DATABASE
				ADD LOGFILE ('/oracle/dbs/log1c.rdo', '/oracle/dbs/log2c.rdo') SIZE 100M; -- adds a new group of redo logs to the database
			ALTER DATABASE ADD LOGFILE MEMBER '/oracle/dbs/log2b.rdo' TO GROUP 2; -- add member to the group 2; size is determined by the existing member's size of that group
			ALTER DATABASE ADD LOGFILE MEMBER '/oracle/dbs/log2c.rdo' TO ('/oracle/dbs/log2a.rdo', '/oracle/dbs/log2b.rdo'); -- add member to group by using other members of the group
	Relocating and Renaming Redo log members
		SHUTDOWN IMMEDIATE
		move the redo logs
			mv /diska/logs/log1a.rdo /diskc/logs/log1c.rdo
			mv /diska/logs/log2a.rdo /diskc/logs/log2c.rdo
		STARTUP MOUNT
		Rename
			ALTER DATABASE
				RENAME FILE '/diska/logs/log1a.rdo', '/diska/logs/log2a.rdo'
					TO '/diskc/logs/log1c.rdo', '/diskc/logs/log2c.rdo';
	Dropping
		Dropping group
			Instance must require two groups of redo log files
			Drop a redo log group only if it is inactive
				if current group needs to be dropped then perform log switching first
			SELECT GROUP#, ARCHIVED, STATUS FROM V$LOG;
			ALTER DATABASE DROP LOGFILE GROUP 3;
		Dropping members
			An instance always require at least two valid groups of redo log files
			A redo log file becomes INVALID if the db cannot access it
			A redo log files becomes STALE if database suspects it is not complete or correct
			Member can only be deleted if it is not the part of active or current redo logs
			Make sure the group in which member belongs is archived
			ALTER DATABASE DROP LOGFILE MEMBER '/oracle/dbs/log3c.rdo';
	Forcing log switches
		ALTER SYSTEM SWITCH LOGFILE;
	Clearing a redo log file
		A redo log file might become corrupted while the database is open, and ultimately stop database activity because archiving cannot continue
		ALTER DATABASE CLEAR LOGFILE GROUP 3; -- clears log files in redo log group number 3
		ALTER DATABASE CLEAR UNARCHIVED LOGFILE GROUP 3; -- clears redo log file and disallow archiving
	Precedence of force logging settings
		FORCE LOGGING and NOLOGGING can be set at various levels such as database, tablespace or database objects
		When FORCE LOGGING  is made in the database level then it doesn't matter in what logging mode are tablespaces and database objects
		
		
MANAGING ARCHIVED LOGS -->>
	While in mount stage
		ALTER DATABASE ARCHIVELOG; -- to archive log mode
		ALTER DATABASE NOARCHIVELOG; -- to no archive log mode
		ALTER DATABASE ARCHIVELOG MANUAL; -- archiving of inactive groups of filled redo log files must be done manually otherwise db will be suspended manually
		ALTER SYSTEM ARCHIVE LOG ALL; -- manually archive filled redo log files while in manual mode
		
DATA DICTIONARY -->>
	----Check in basic section-----
	

------------------------------------------------------------ DATABASE BACKUP AND RECOVERY -------------------------------------	

COMPLETE VS INCOMPLETE RECOVERY -->>
	Restore to a previous point in time (PITR). ---incomplete
	Restore to a current point in time. --complete
	
POINT IN TIME RECOVERY (PITR) -->>
	Combining the full backup taken at certain period of time + the archived redo log files after the backup will restore the database.
	Archive log mode should be enabled
		the archives are stored in DB_RECOVERY_FILE_DEST parameter size defined by DB_RECOVERY_FILE_DEST_SIZE

	DATABASE FAILURE CATEGORIES -
		Human failure
			resolved by flashback and oracle data pump
		Machine failure (instance failure but healthy database)
			HA technologies and oracle RAC
		Disk failure (healthy instance but failure in database)
			Oracle RMAN and hot backups
		Data center failure (whole data center failure)
			DR (Disaster Recovery) technologies and oracle data guard
			
	Physical vs Logical data backups
		Physical
			Copying the files like Data files, Control files, Archived redo logs to other locations
			Oracle RMAN (Recovery Manager) is used for physical backups
		Logical
			Exporting database objects like tables, indexes, etc..
			Oracle data pump is used for this
			
	Oracle backup technologies
		User-managed hot backups
			Manually copy oracle data files to a separate location to create a physical backup of the database
			Can be done while database is running
			Incremental backups cannot be done with this approach
			Not used that much at recent time
		Oracle RMAN
			Comprehensive framework for physical backup and recovery
			Invoked using rman command-line utility
			supports incremental backups
			Creates bulletproof backups
		Data pump
			Enables portability of data and metadata from one db to another
			support filtering during export/import
			Suppoert interchangeability between different versions
		Flashback
			provides way to view past states of the entire database or specific objects
			Usually invoked through 'flashback' command
			
	Enable archivelog mode
		show con_name;  -- make sure to be connected to container database
		alter pluggable database all open;  -- to open all pluggable database after alter database open;
		goto ---redo sections---	
		
	Fast recovery area	
		Select * from v$database;
		Disk or file system available to manage the backup or log files
		It is oracle managed directory or file system
		Usually stores archived redo logs, flashback logs, RMAN backups
		The archives are stored in DB_RECOVERY_FILE_DEST parameter size defined by DB_RECOVERY_FILE_DEST_SIZE
		
	USER MANAGED BACKUPS
		Manually copying oracle data files to a separate location
		start by enabling the database online 'backup mode'   -- alter database begin backup;
		finish by taking the database out of 'backup mode'		-- alter database end backup;
		select * from v$backup; -- check if backup is active or not
		i)  Then perform copy commands and copy the required data files etc
				If the storage device is enterprise level oracle filesystem then various useful operation are available like cloning etc.
			ii) Now the backup of control files and archived logs.
				control file backup ----  alter database backup controlfile to 'controlfile_backup_destination/control_file_backup.ctl'
				archive redo log backup ---- alter system archive log current;
												select thread#, sequence#, name from v$archived_log;
			iii) Performing backup  --
				use operating system utility to perform the backup
				first copy the backup taken previously (datafiles, controlfiles and archived redo logs)
				after that the backup is done using the recover utility of sqlplus
				steps
					shutdown immediate;
					startup mount;
					set autorecovery on;
					show parameter log_archive_format;  -- format of the archive redo files
					recover database;   -- displays no recovery required message if no datafiles are corrupted and no recovery is needed.
					alter database open;
					
	ORACLE RMAN (Recovery Manager)
		RMAN is the utility recommended by oracle than others methodology.
		RMAN is used both for backup and the restoration
		By default stored in fast recovery area
		RMAN command line is used for everything instead of other hectic os commands.
		RMAN copy only the used block in contrary with OS level commands
			if the datafile is only one-third full then also the cp commands copy the entired file
			RMAN copy only one-third of the total file
		RMAN can detect if any block is corrupted in the database during backup and can fix the block
		RMAN provides the concept of incremental backup which is extremely important feature
		With RMAN we can validate the backup to make sure we can perform recovery with that backup
		Parallelism is the feature of RMAN, with which we can accomplish the task in parallel
		Recovery catalog can be utilized with RMAN which is very helpful for storing metadata instead using control files 
		RMAN can only be used if it is at least in mount stage
		Multi destination backup is possible by setting up the redundancy levels
		Performs online database backup
		Supports two types of backups - full and incremental backups
			full - backing up everything
			incremental - copy changed data blocks
			levels of incremental backup
				level 0 - similar to the full backup
				level 1 - cumulative and deferential, two types
		Backup made by RMAN is called backup set, and can only be restored using RMAN
		Useful
			rman target=/; -- target = <sid> -- / = default instance
			rman target=sys/sagarmatha@orcl
				now rman is opened like sqlplus RMAN>
				show all; -- show all the RMAN parameters -- its result are given below
					using target database control file instead of recovery catalog
					RMAN configuration parameters for database with db_unique_name ORCL are:
					CONFIGURE RETENTION POLICY TO REDUNDANCY 1; # default    -- how many level 0 backups should RMAN retain
					CONFIGURE BACKUP OPTIMIZATION OFF; # default
					CONFIGURE DEFAULT DEVICE TYPE TO DISK; # default    -- by default store in a disk rather than tapes etc.
					CONFIGURE CONTROLFILE AUTOBACKUP ON; # default		-- instruct to backup control and sp file during backup
					CONFIGURE CONTROLFILE AUTOBACKUP FORMAT FOR DEVICE TYPE DISK TO '%F'; # default
					CONFIGURE DEVICE TYPE DISK PARALLELISM 1 BACKUP TYPE TO BACKUPSET; # default
					CONFIGURE DATAFILE BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # default
					CONFIGURE ARCHIVELOG BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # default
					CONFIGURE MAXSETSIZE TO UNLIMITED; # default
					CONFIGURE ENCRYPTION FOR DATABASE OFF; # default
					CONFIGURE ENCRYPTION ALGORITHM 'AES128'; # default
					CONFIGURE COMPRESSION ALGORITHM 'BASIC' AS OF RELEASE 'DEFAULT' OPTIMIZE FOR LOAD TRUE ; # default
					CONFIGURE RMAN OUTPUT TO KEEP FOR 7 DAYS; # default
					CONFIGURE ARCHIVELOG DELETION POLICY TO NONE; # default
					CONFIGURE SNAPSHOT CONTROLFILE NAME TO '/u01/app/oracle/product/12.2.0/dbhome_1/dbs/snapcf_orcl.f'; # default
					
				list backup; -- list all the backups
				list backup summary; -- list all the backups in brief
				list backup of database summary; -- list extremely less backup summary only of datafiles and the associated tag
					-- list backup summary; will list individual tags of all archived, control files etc too
				list backup of controlfile;
					
				command for manually configuring those params	
					CONFIGURE RETENTION POLICY TO REDUNDANCY 1;
					CONFIGURE RETENTION POLICY TO RECOVERY WINDOW OF 7 DAYS;  -- retain backup for 7 days and delete older
					
				Useful	
					backup database plus archivelog format '\location\%u'; -- %u denotes to use the same file name
					backup incremental level 0 database plus archivelog; -- same as above but it is incremental backup
					backup database plus archivelog;  -- instructs RMAN to take backup of everything
					backup database tag 'DAILY_BACKUP' plus archivelog;  -- use tags to identify different backups
					backup database root;  -- only backup the root (CDB), don't backup any PDBs
					backup pluggable database plg_db_name;  -- only backup pluggable db with name plg_db_name
					
					restore controlfile from '/location.bkp';
					restore database from tag=''; -- it restores using all incremental levels
					recover database; -- it also applies archived logs
					
					
					if connected to pluggable database
						rman plg_db_name/password@service
						backup database plus archivelog; -- will only backup the connected pdb
					Incremental
						backup incremental level 0 database tag 'inc_level0' plus archivelog;
						backup incremental level 1 database tag 'inc_level1' plus archivelog; -- least amount of data block backed up, differentiating
						backup incremental level 1 cumulative database tag 'inc_level1' plus archivelog;
							cumulative back ignores all other backups and only cares about last full backup
							it takes more space than differential backup
							it takes more time to execute
							it should not analyze more backup sets
		---STUDY RMAN AGAIN---					
							
							
	ORACLE DATA PUMP						
		Data pump is done using the utility 'expdp'
		Useful
			Export
				expdp system/sagarmatha schemas=sandesh dumpfile=mydump01.dmp; -- file is dumped in the default oracle directory
				expdp system/sagarmatha schemas=<comma separated list of schemas> dumpfile=mydump01.dmp; -- exporting from multiple schemas
				create directory my_exp_dir1 as '/u01/app/oracle/admin/orcl/my_export_directory'; -- will not create file system dir if not exists
				grant read,write on directory my_exp_dir1 to public;
				expdp system/sagarmatha schemas=sandesh dumpfile=mydump01.dmp directory=my_exp_dir1; -- exporting to the directory created
				expdp system/sagarmatha schemas=sandesh dumpfile=mydump01.dmp directory=my_exp_dir1 logfile=mylog01.log; -- with the log file
					-- logfile is the exact mirror of the messages during the expdp command run
				
				alter session set container=cdb$root; -- setting container database as the connected one
				create pluggable database demo_PDB admin user pdb_main identified by pdb_admin file_name_convert('/u01/app/oracle/oradata/orcl12c/pdbseed/',
					'/u01/app/oracle/oradata/orcl12c/demo_pdb/'); -- creating a pluggable database
				alter pluggable database demo_pdb open;  -- opening the pluggable database;
				alter session set container=demo_pdb;
				expdp system/sagarmatha@localhost:1521/demo_pdb directory=directory_name dumpfile=demo_full.dmp; -- export content of complete pdb
				expdp sandesh/sandesh@orcl tables=EMP,DEPT directory=TEST_DIR dumpfile=EMP_DEPT.dmp logfile=expdpEMP_DEPT.log
					-- exporting table only
				expdp sandesh/sandesh@orcl schemas=SCOTT directory=TEST_DIR dumpfile=SCOTT.dmp logfile=expdpSCOTT.log
					-- exporting whole schema
				expdp system/sagarmatha@orcl full=Y directory=TEST_DIR dumpfile=DB10G.dmp logfile=expdpDB10G.log
					-- full database export
				
			Import
				The exported file does not depend upon the oracle version, once exported can be imported by any other system
				impdp success/success directory=my_exp_dir dumpfile=sandesh_stu.dmp logfile=success_stu_imp.dmp remap_schema=sandesh:success;
					-- this will work without specifying schemas or tables attribute; using remap_schema option
				impdp sandesh/sandesh directory=my_exp_dir dumpfile=sandesh_stu.dmp logfile=sandesh_stu_imp.log tables=student table_exists_action=append;
					-- table import with the action to append if exist
				impdp system/sagarmatha@localhost:1521/orcl directory=my_exp_dir1 dumpfile=sandesh01.dmp logfile=sandesh01_IMPORT.log;
					-- if the data already exist then it will throw the error stating data is already present
				impdp system/sagarmatha@localhost:1521/orcl directory=my_exp_dir1 dumpfile=sandesh01.dmp logfile=sandesh01_IMPORT.log remap_schema=sandesh:success;
					-- remap schema will remap the content of (x:y) x schema obj in dump file to the y schema
				impdp system/sagarmatha@localhost:1521/orcl directory=my_exp_dir1 dumpfile=sandesh01.dmp logfile=sandesh01_IMPORT.log remap_schema=sandesh:success
					INCLUDE=USER,ROLE_GRANT,SYSTEM_GRANT,TABLE,INDEX;
					-- this specify the objects to include during import
					-- user grants, system grants are also applied to the user
				impdp system/sagarmatha@localhost:1521/orcl directory=my_exp_dir1 dumpfile=sandesh01.dmp logfile=sandesh01_IMPORT.log remap_schema=sandesh:success
					EXCLUDE=VIEW;
					-- exclude view and import everything
					
			Advanced object filtering
				expdp system/sagarmatha@localhost:1521/orcl schemas=sandesh dumpfile=mydump01.dmp directory=my_exp_dir1 logfile=mylog01.log
					INCLUDE=TABLE:\""IN ('MY_FIRST_TABLE','ANOTHER_TABLE')\"", VIEW:\""IN ('MY_FIRST_VIEW')\"";
					-- double quotation is used to escape how the linux file system treats double quotes
					-- using parameter file will solve this problem
				expdp system/sagarmatha@localhost:1521/orcl parfile=/home/Documents/params.par
					-- providing parameter file as an argument and all other necessary arguments are passed via param file
			Monitoring and Controlling
				expdp system/sagarmatha schemas=sandesh dumpfile=mydump01.dmp directory=my_exp_dir1 logfile=mylog01.log full=y;
					-- full=y refers that this export is going to run for sufficiently long period of time
				select * from dba_datapump_jobs; -- tracking the jobs that are running as export or import data pump
				expdp system/sagarmatha@localhost:1521/orcl attach=SYS_EXPORT_FULL_01
					-- SYS_EXPORT_FULL_01 this is the running datapump job_name. dba_datapump_jobs table has the job info and name
					-- entering above command will open the expdp prompt attached to that job
					-- this provides the various info regarding the job
					-- 'kill_job' command in expdp prompt will terminate the job
					
	ORACLE FLASHBACK
		With flashback we can be at the previous timeline of database with the single command.
		Flashback doesnot work in case of DDL statements like altering table structure etc.
		If whole table is dropped then the recycle bin feature can be used
		Requirements
			Flashback query	
				Uses data stored in undo tablespace so requires the proper undo retention policy
				select * from student as of timestamp to_timestamp('13-DEC-19 02.28.32.771409000 PM'); -- data at time stamp before modification
				create table old_student as select * from student as of timestamp to_timestamp('13-DEC-19 02.28.32.771409000 PM');  -- restoring old data
				flashback table student to timestamp (systimestamp - interval '60' second);  
					-- flashback table before 60 second
					-- if row movement is not enabled then this query will throw error
					alter table student enable row movement; -- altering student table to enable row movement
						-- row id is not preserved by flashback query to enabling row movement in the table will solve this problem
			Flashback database
				Uses flashback logs stored in FRA
				To enable to flashback database
					shutdown immediate;
					startup mount;
					archive log list;  -- make sure that database is running with archive log mode enabled
						-- logs used by flashback database is also stored in db_recovery_file_dest;
							db_recovery_file_dest is also used by archived redo logs and RMAN backup
					show parameter db_flashback_retention_target;  -- length of time flashback database supports
					alter database flashback on;
					alter database open;
					alter pluggable database all open;
					
				To restore using flashback database
					shutdown immediate;
					startup mount;
					flashback database to timestamp to_timestamp ('timestamp','format');
						OR
					flashback database to timestamp to_timestamp ('timestamp');
						OR
					SCN (System change number) can also be used for flashing back the database
					alter database open resetlogs;
						-- resetlogs or noresetlogs must be used
					
		If whole table is dropped then the recycle bin feature can be used
			show parameter recyclebin; -- this should be on
			flashback table student to before drop;  -- recreate the table from before drop operation
			select * from dba_recyclebin;   -- view the content of recycle bin
			select * from user_recyclebin;   -- view the content of recycle bin from users
			purge recyclebin;  -- clearing recyclebin (user specific; if ran with sys then also remove object belonging to sys)
			purge dba_recyclebin; -- clears everything from recyclebin
			drop table student purge; -- purge will make sure that the object won't move to recycle bin once deleted
			
		Creating restore point
			create restore point DB_IS_OK guarantee flashback database;  -- running from root will create the point for all pdbs and cdb a restore point for the flashback database
			select * from v$restore_point; -- table containing SCN, restore point name and timestamp to restore the database to
			flashback database to restore point DB_IS_OK; -- DB_IS_OK is restore point name
			
			
			
------------------------------------------------------------ ORACLE 12C  -------------------------------------		

MULTITENANT ARCHITECTURE -->>
	Runs multiple oracle databases under the same Oracle instance
	Makes running multiple Oracle databases on a single server more cost effective and easier to manage
	Database consolidation and multitenant architecture
	Simple management and self service
	Rapid provisioning of new databases
	Manage multiple DBs as a single DB
	Dedicated pre 12c servers 
		Assigning single server to single application
			i.e, we have single dedicated server containing of instance, resources and database for each application
		More servers to manage
		DR and HA - complex  (Disaster Recovery and High Availability)
		More licenses to buy
		Cannot share resource easily in case workload on one is greater than another
		More servers = more CPU cores = more money to spend on
	Pre 12c instance consolidations
		Single application will have a dedicated database and instance but other resources will be shared among them
		If one application is added then one database should also be added alongside with the instance
		Movement of one database to another server is really complex procedure to follow
	Pre 12c schema consolidations	
		One server, running one database but has many schemas per application running
		Logically separate but not physically separates as schemas are simply logical objects
		If same application ran by two different branches then cloning schema twice will create lot of trouble like in public synonyms etc.
		Moving one schema to another server is also difficult as they are not physically separated
		Logical copy technology like data pump should be used which is no fast comparatively
	With 12c
		Single server, single instance, multiple databases
		Each application will have each database
		Backup, restoration, migration is often easier
		Contains CDB (Container Database) and PDB (Pluggable Database)
			CDB acts as a container to the many pluggable databases
			CDB can be called as a super database with many sub (pluggable) databases
			CDB contains pluggable databases (PDBs) and a root container which contains the set of oracle provided objects etc..
			
	Architecture
		Contains a container database
		Container database consists of root container, a seed pdb and zero or many PDBs
		Container db has the container id of 0
		Root container contains redo logs, control files, archived logs, flashback and all other tablespaces etc.. Root container has the container id of 1
		Seed pdb is the type of pdb which is used to create other pd. seed pdb has the container id of 2. It does not serve any application wise purpose.
			It is named PDB$SEED
		Undo tablespace is only contained in root container
			-- This is only till oracle 12cR1. In Oracle Database 12c Release 1 all containers in an instance shared the same undo tablespace.
			-- Now the undo tablespace can be configured with local mode on; In this case all pdbs will have undo datafile contained separately
			-- In Oracle 12c Release 2 each container in an instance can use its own undo tablespace
			-- Local undo mode is the default mode in newly created databases
			-- Switching to shared mode
				CONN / AS SYSDBA
				SHUTDOWN IMMEDIATE;
				STARTUP UPGRADE;
				ALTER DATABASE LOCAL UNDO OFF;
				SHUTDOWN IMMEDIATE;
				STARTUP;
			
		System, sysaux and temp is present in alls PDBs too
		Root container is named CDB$ROOT with container id 1
		Upto 253 pluggable databases can be present
		Each PDB will have its own server processes as the SPs is spawned for each user connection
			Buffer cache will now track, which PDB does that block belong to
			
	connect 
		show con_name;  -- print the container in which we are connected to	
		sqlplus sys/sagarmatha@localhost:1521/orcl as sysdba; -- here orcl is the cdb name	
		sqlplus my_pdb_admin/password@localhost:1521/my_new_pdb as sysdba; -- to the newly created pdb
	create
		create pluggable database my_new_pdb
		admin user my_pdb_admin identified by password   -- admin user for that pdb
		ROLES = (dba)       -- granting dba roles for the above user, otherwise user won't have any privilege
		DEFAULT TABLESPACE my_tbs  -- default tablespace to use
		DATAFILE '/oradata/orcl/my_new_pdb/mytbs01.dbf' size 50m autoextend on
		file_name_convert=('/oradata/orcl/pdbseed', '/oradata/orcl/my_new_pdb')  -- clone from the seed pdb;
	alter
		alter pluggable database my_new_pdb open; -- opening the created pdb or closed pdb
		alter session set container=me_new_pdb; -- to change the container without logging out and connecting to the new one
		alter pluggable database my_new_pdb open;
		drop pluggable database my_new_pdb <including datafiles / keep datafiles>;
	useful	
		select * from v$pdbs;
		select * from v$container;
	pdb portability
		alter pluggable database my_new_pdb close <immediate>;
		alter pluggable database my_new_pdb unplug into '/storage/pdb.xml'; -- this xml file is called the manifest file, containing the metadata regarding original db
		drop pluggable database my_new_pdb keep datafiles;
		create pluggable database my_new_pdb_replugged  using '/storage/pdb.xml' nocopy tempfile reuse; -- reuse the tempfile and donot copy anything i.e, use all datafiles
		alter pluggable database my_new_pdb_replugged open;
	pdb cloning
		create pluggable database source_pdb
		admin user my_pdb_admin identified by password
		ROLES = (dba)
		DEFAULT TABLESPACE my_tbs
		DATAFILE '/oradata/orcl/my_new_pdb/mytbs01.dbf' size 50m autoextend on
		file_name_convert=('/oradata/orcl/pdbseed', '/oradata/orcl/my_new_pdb');
		alter pluggable database source_pdb open;
		alter pluggable database source_pdb close;
		alter pluggable database source_pdb open read only; -- must be in read only state
		create pluggable database target_pdb from source_pdb file_name_convert('/oradata/orcl/source_pdb/', /oradata/orcl/target_pdb/';
		alter pluggable database target_pdb open;
	data dictionary view
		cdb_,dba_,all_ and user_ dictionary views are present now
		Each pdb has - dba_,all_ and user_ views
		Root container has - cdb_ and all others views
		cdb_ - all objects in all containers
		dba_ - all objects in the database
		all_ - all objects in the db which the user has access to
		user_ - all objects owned by the user
	local and common user
		local users are the local to the database created
		common users are for all databases i.e, cdb and pdbs
		common user created can connect to all the database if the privilege is given
		common user should be prefixed with c##.
		Default scope of grant is only to the pdb from which the grant is performed.
		local user is created using the admin user of the pdb to connect.
			from my_new_pdb >
				create user username identified by password;
		common user is create from root container with sys
			from cdb$root >
				create user c##username identified by passsword;
				grant create session to c##username; -- i.e, the user can only connect to the db from which the grant is performd. in this case cdb$root
				grant create session to c##username container=all; -- now can connect to all containers
				
IN-MEMORY DATABASE -->>
	Row and columnar format differences
		Oracle in memory column store caches data in columnar format comparision to row form in standard buffer cache
		Best during the analytical query ran which highly depends upon the aggregate functions
		Basically data is stored in buffer cache in row format
		Row wise storage is suitable in case of OLTP systems
		In case of OLAP or analytic functions whole row need not be cached
		As row was stored in the disk likewise row is also stored in the buffer cache
		This is not the best scenario in case of analytics technologies
			they tried to summarize column performing complex operations
	

------------------------------------------------------------ MIGRATING NON CDB TO PDB  -------------------------------------

Steps for Conversion. 

Step 1.  Cleanly Shutdown the Non-CDB Database Instance. 

          ==- set environment to NCDB

          ==- sqlplus / as sysdba

          ==- Shutdown immediate 

Step 2.  Once You Have Cleanly Shutdown the Database, Start Up the database in Mount Exclusive Mode and Open the Database in Read-Only Mode. 

          ==- set environment to NCDB

          ==- sqlplus / as sysdba        

          ==- startup mount exclusive

          ==- alter database open read only; 

Step 3. Generate a Pluggable Database Manifest File from the Non-Container Database. 

          ==- set environment to NCDB

          ==- sqlplus / as sysdba        

          ==- exec dbms_pdb.describe (pdb_descr_file=-'E:\app\oracle\manifest\NCDB_manifest_file.xml'); 

Step 4. Shutdown the NON-CDB file Once Step 3 Completes Successfully. 

          ==- Set environment to NCDB

          ==- sqlplus / as sysdba

          ==- shutdown immediate 

Step 5. Start the CDB (CDB) if it’s Not Already Up and Check the Compatibility with CDB.

          ==- set environment to CDB

          ==- sqlplus / as sysdba

          ==- startup (If not up)

          ==- Run below at SQL prompt. 

          SET SERVEROUTPUT ON;

          DECLARE

          Compatible CONSTANT VARCHAR2(3) :=CASE  DBMS_PDB.CHECK_PLUG_COMPATIBILITY 

          (pdb_descr_file =- 'E:\app\oracle\manifest\NCDB_manifest_file.xml')

          WHEN TRUE THEN 'YES'

          ELSE 'NO'

          END;

          BEGIN

          DBMS_OUTPUT.PUT_LINE(compatible);

          END;

          / 

Step 6. Once it Completes Successfully, Query PDB_PLUG_IN_VIOLATIONS View from CDB Database for Any Errors.

          ==- set environment to CDB

          ==- sqlplus / as sysdba

          ==- select name, cause, type, message, status from PDB_PLUG_IN_VIOLATIONS where

                   name='NCDB';  

Note: "There should be no violations reported. If there are any, you need to fix it before proceeding". 

Step 7. Connect to the CDB Where Database has to be Plugged in Using the Noncdb Manifest File and Plug the PDDB12C Database.

          ==- set environment to CDB

          ==- sqlplus / as sysdba         

          ==- CREATE PLUGGABLE DATABASE NCDB USING 'E:\app\oracle\manifest\NCDB_manifest_file.xml'

          COPY

          FILE_NAME_CONVERT = ('E:\APP\ORACLE\ORADATA\NCDB\', 'E:\app\oracle\oradata\CDB1\'); 

          Note: "Below options are supported and you can chose one based on the env" 

1) COPY: The datafiles of noncdb remains intact and it is copied to create PDBs at new locations and keep original datafiles intact at the original location. (This would mean that a noncdb database would still be operational after the creation of a PDB).

2) MOVE: The datafiles of noncdb are moved to a new location to create a PDB. In this case, noncdb database would not be available after a PDB is created.

NOCOPY: The datafiles of noncdb are used to create a PDB2 and it uses same existing location. In this case, a noncdb database would not be available after a PDB is created. 
You can use FILE_NAME_CONVERT parameter to specify the new location of the datafiles while using COPY or MOVE option. 

Step 8. Once Step 7 Completes Successfully, Switch to the PDB Container and Run the "$ORACLE_HOME/rdbms/admin/noncdb_to_pdb.sql".

          ==- set environment to CDB

          ==- sqlplus / as sysdba

          ==- alter session set container=NCDB

          ==- @$ORACLE_HOME/rdbms/admin/noncdb_to_pdb.sql 

Step 9.  Startup the PDB and Check the Open Mode.

          ==- set environment to CDB

          ==- sqlplus / as sysdba

          ==- ALTER PLUGGABLE DATABASE OPEN;

          ==-SELECT name, open_mode FROM v$pdbs;
		  
		  
		  
------------------------------------------------------------ DATABASE SECURITY -------------------------------------

DATABASE AUTHENTICATION -->>
	Authenticating the user before providing access to the database
	When user tries to connect password should be provided beforehand
	Or in the system like web based app, single user can be created for database and multiple users to connect to the web server
	Other modes of authentication
		Username and password authentication
		Operating system based authentication
		Network based authentication (Kerberos)
		Directory based authentication (Active directory users)
	After authentication, authorization and permissions should also be cared about
		
ADVANCED SECURITY CONCEPTS -->>
		Database Auditing
			Make database users accountable for their actions
			i.e, Track the actions performed by authorized user on the actions they are privileged to perform
			Their is built in auditing feature provided in Oracle which can help tracking the unnecessary and suspicious pattern performed in the database
			Allows the tracking of both successful and failed activities
				-- if tried to access the data i.e, not allowed
		Database Encryption
			There two major types of encryption
				Data-at-rest encryption
					Encrypting the data stored in the database itself
					If anyone get hold of the storage devices then also they won't be able to steal any information
					Oracle provides built in feature to for this
						Transparent Data Encryption (TDE)
							TDE with its name suggests the transparent data encryption and decryption
								i.e, decrypt during read
							With this feature enabled encryption-decryption is performed automatically
							Data will be automatically encrypted during writes and automatically decrypted during reads
							It does not care about the type of disk used
							It is easy and convinient way of encryption
				Network encryption
					Data is encrypted at network level
					Incoming and Outgoing data are encrypted
		Roles of DBA in DB Security
			DBA is responsible for implementing security architecture
				Create users and assign permissions
				Granting minimum set of permissions
				Create suitable roles to be granted for the group of people
				Minimize the creation of superusers
				Enable and configure DB auditing.
				Set alert in case of anu unwanted tasks
				Enable encryption; encrypt both data and data backups
				
USERS AND PERMISSIONS -->>
	Goto USER MANAGEMENT section for the further information regarding it
	Users and Schemas are essentially same thing in case of oracle.
		Once the user is created, schema is also created alongside the user
		User cannot be created separately to the schema and vice versa
	SYS	
		Sys user is the most powerful user in the database
		Beware while connecting with the sys user and performing any action in the database
		'as sysdba' clause should be used while connecting with the sys user; for sysdba the schema is sys
		'as sysoper' clause can also be used while connecting with sys; It will restrict from performing some of the tasks; schema is public for sysoper
			With this other user data cannot be viewed
	SYSTEM
		Created automatically during installation and the dba role is granted by default
	Use grant command to assign privilege or role to the user
	Revoke can be used to revoke the permission already granted to the user
	Beware while providing any database-wide system privilege (any keyword); Goto USER MANAGEMENT for futher info
	Tablespace quotas
		Cannot insert any records if the quota for that user is not specified in the tablespace
		ORA-01950: no privileges on tablespace 'tablespace_name'
		alter user user_name quota <1g OR [unlimited]> on tablespace_name;
		grant unlimited tablespace to user_name; -- this is also possible
	Object privilege should be cared out as like the system privilege
		Object privilege is given to the specified database object
		i.e, grant select on user2.table_name to user1;
			 grant execute on user2.proc_name to user1;
			 Grant grant any object privilege to user1; -- user1 can grant any object privilege
	Password Management Policies
		Goto USER MANAGEMENT for futher info; Creating the appropriate profile is discussed
	With [GRANT/ADMIN] Option
		with grant option - is for the object privilege; to whom it is grant can also grant the same level of access
		with admin option - is for the system privilege; same
		Goto USER MANAGEMENT for futher info
	Password Complexity Policy
		It is best to use the complex password for the sake of security enforcement
		Need to run sql file in - $ORACLE_HOME/rdbms/admin/utlpwdmg.sql
			This folder contains other various sql files too
			This script will compile a password verification function in the database
			In case of 12c it will create two functions
				sys.ora12c_verify_function
				sys.ora12c_strong_verify_function
			alter profile profile_name limit password_verify_function ora12c_strong_verify_function;
			alter profile profile_name limit password_verify_function null;
				-- with this password_verify_function is changed from ora12c_verify_function to ora12c_strong_verify_function
	Viewing user, permissions, and policies
		Query dba_users views to see authentication method, account_st, last_time_login etc.
		select * from dba_sys_privs; -- system privilege
		select * from dba_tab_privs; -- object privilege
		select * from dba_role_privs; -- roles assigned i.e, the group of privileges
		select * from session_privs; -- session privilege
	Deleting a user account
		drop user user_name; -- if user_name has no ownership on any objects
		drop user user_name cascade; -- if user_name has ownership
	Local vs Common user
		Refer ORACLE 12C for this
	Seperating Schemas and Users
		Inseperable
		One way to try seperating
			Create everything in one schema that makes sense. For eg, CRM app data in CRM user
			Lock the CRM user after creating everything
			Then create separate different users and provide them privilege accordingly
			
DATABASE ROLES -->>
	Using roles user permissions can be grouped together such that it can be granted to the users
	Built-in roles
		connect -> creating different things
		resource -> creating different things
		dba -> all system privileges with admin option
		pdb_dba
	Creating, assigning and dropping roles
		create role role_name;
		grant <privileges> to role_name;
		create user user_name identified by password_name;
		grant role_name to user_name;
		revoke role_name from user_name;
		drop role role_name;
	Viewing roles
		select * from dba_role_privs; -- user and the role granted to them
		select * from role_tab_privs; -- different object role privilege given to the role
		select * from role_sys_privs; -- different system role privilege given to the role
		select * from role_role_privs; -- role granted to other roles
	Roles with admin option
		if roles is granted with admin option then the user can grant that role to other users
		
DATABASE AUDITING -->>
	Auditing is done to both privileged and non-privileged user about their actions
	Configure audit parameters
		show parameter audit_trail; -- DB means stored in DB level; OS means in OS file
		show parameter audit_sys_operations; -- if sys user should be audited or not
		show parameter audit_file_dest; -- destination used if OS or XML option is used; action done by sys user is by default stored in there ; check it
		select value from v$option where parameter='Unified Auditing';
		unified_audit_trail - query this view to check the different unified source used for editing
			The unified auditing is introduced from oracle 12C
			This dramatically simplified the auditing process
		audit_unified_enabled_policies - query this to view which users or roles have been enabled to be audited by policies
		dba_audit_trail			show all audit entries in the system
		dba_audit_object		show all audit entries in the system for objects
		dba_audit_statement		show audit entries for statements grant, revoke, audit, noaudit, and alter system
		dba_audit_session		show audit entries for the connect and disconnect actions
		 -- In 12c the unified audit trail simplifies viewing and reporting audit information
		 -- Just viewing unified_audit_trail is enough to find out various informations
	Usage
		create audit policy audit_policy1
		privileges create table
		when 'sys_context(''USERENV'', ''SESSION_USER'') = ''USER1'''
		EVALUATE PER SESSION
		CONTAINER = CURRENT;
			-- in current container, can be set to all from root, evaluate per session for USER1 and if create table is done
			
		create audit policy audit_policy2
		actions all on user_name.object_name; 
			-- audit all the actions performed on object object_name
			
		create audit policy audit_policy3
		actions insert on user_name.object_name; 
			-- audit insert actions performed on object object_name
			
		audit policy audit_policy1;
		audit policy audit_policy2;
		audit policy audit_policy3;
			-- enabling audit policies
			-- without this those created audit policies won't work
			
		noaudit policy audit_policy1;
			-- disabling the audit without deleting them
			
		drop policy audit_policy1;
			-- dropping the audit policies
		
		select * from audit_unified_policies;
			-- viewing audit policies created
			
	Using Standard Auditing
		audit select table, insert table, delete table, execute procedure by access whenever not successful;
			-- this query if ran will audit all mentioned actions in the database if not successful
		audit create session whenever not successful;
			-- if connection is not successful
		audit select on user_name.object_name;	
			-- object level auditing
		noaudit select on user_name.object_name;
			-- disable object level auditing
			
		Audit information are stored in dba_audit_trail;
		
	Fine-grained Auditing (FGA)
		All Standard, Unified and Fine-grained auditing are built-in auditing technique create in oracle database
		As the name suggests it allows fine-grained auditing to the columns of the table also
		If just a column is sensitive then why not audit that particular table only
		Usage
			exec dbms_fga.add_policy(object_schema=>'user_name', object_name=>'my_tblname', policy_name=>'check_col2_access',
			audit_column=>'col2', statement_types=>'select, update');
				-- audit for any select and update action performed in column col2 of table my_tblname
				
			dba_audit_policies
				-- check this view for the created FGA policies
			dba_fga_audit_trail
				-- check this view for the audit information triggered by FGA audit policies
			
	Manage the audit trail
		Different aspects should be taken care of regularly for the audit to function properly
		If the space is filled and the audit has no workspace then the database will actually freeze due to its enability to maintain the record
		In case of denial of attack audit table can grow enormously
		SYSTEM tablespace is used to store the audit trail so something that should be taken care about
		Cleanup procedure
			Archiving existing log details of audit table should be done before purging
			This can be called only once, if tried to call again then 'audit trail cleanup initialization has already happened' error will be thrown
			begin
				dbms_audit_mgmt.init_cleanup(
					audit_trail_type => dbms_audit_mgmt.audit_trail_all,
					default_cleanup_interval => 12
				);
			end;
			/
				-- after running this query aud$ table will be moved to sysaux tablespace from system tablespace
				-- after moving to sysaux tablespace it is little safer for space not running up
				
			dbms_audit_mgmt.clean_audit_trail(
				audit_trail_type	=> dbms_audit_mgmt.audit_trail_aud_std,
				use_last_arch_timestamp => false,
				container	=> dbms_audit_mgmt.container_current);
			end;
			/
				-- this will purge the record from standard audit table aud$
				-- aud$ table is the base for all standard auditing views
				
			dbms_audit_mgmt.clean_audit_trail(
				audit_trail_type	=> dbms_audit_mgmt.audit_trail_fga_std,
				use_last_arch_timestamp => false,
				container	=> dbms_audit_mgmt.container_current);
			end;
			/
				-- this will purge the record for FGA audit trail table (dba_fga_audit_trail)
				-- fga_log$ table is the base for dba_fga_audit_trail
				
			dbms_audit_mgmt.clean_audit_trail(
				audit_trail_type	=> dbms_audit_mgmt.audit_trail_unified,
				use_last_arch_timestamp => false,
				container	=> dbms_audit_mgmt.container_current);
			end;
			/
				-- this will purge the record for unified audit trail table (unified_audit_trail)
				-- audsys.aud$unified table is the base for unified_audit_trail
				
DATA ENCRYPTION -->>
	Goto ADVANCED SECURITY CONCEPTS for further clarification
	Data-at-rest encryption will be discussed in this
	For this to work we need to create TDE (Transparent Data Encryption) keyfile 
	Configure the keystore
		mkdir $ORACLE_HOME/wallet	-- any directory will work but creating this
		edit $ORACLE_HOME/network/admin/sqlnet.ora file and add following line
			encryption_wallet_location=
			(source=
			 (method=file)
			 (method_data=
			  (directory=$ORACLE_HOME/wallet)))
		now create password based software keystore
			this requires password from user which is used to protect the keys in the keystore
			administer key management create keystore '$ORACLE_HOME/wallet' IDENTIFIED BY secret_keystore_password; -- for creating keystore	
				-- password can be changed
		goto $ORACLE_HOME/wallet, keystore will be created here after running above query
		now open the wallet created
			administer key management set keystore open identified by secret_keystore_password container=all;
				-- container=all can only be executed if connected in container database
	Create encryption key	
		Set the master encryption key
		This is the key which will be used by TDE method will use during encryption
		administer key management set key identified by secret_keystore_password with backup container=all;
			-- container=all can only be executed if connected in container database
			-- with backup option enables the backup of the key and is required for this type of keystore
	Encrypting data in tables
		It encrypts and decrypts data in SQL layer
		Using this we can try encrypting a single column or some columns according to the needs
		In case of sensitive data stored in certain table we can try encrypting that column only
		Connect as the user i,e. owner for the table to encrypt
			alter table table_name add (new_col varchar2(10) encrypt);
			alter table table_name modify (already_present_tbl encrypt);
			alter table table_name modify (already_present_tbl decrypt);
			desc table_name; -- this will show if table's column is encrypted
			select * from table_name;
				-- if has encrypted column then also the data is displayed as it is
			create index index_name on table_name (encrypted_key_column);
				-- this will throw error
				--  cannot be both indexed and encrypted with salt
			alter table table_name modify (encrypted_key_column encrypt no salt);
				-- disabling salt, now index can be create in that column
				-- column should be encrypted with no salt for the index to be created in that column
			create table table_name (
				first_name varchar2(100),
				last_name varchar2(100),
				id varchar2(10) encrypt using '3DES168'
			);
				-- explicitly specified the encryption technique
			alter table table_name rekey using 'AES128';
				-- changing preciously defined encryption technique
	Encrypting tablespaces
		If many objects needs to be encrypted as a whole then tablespace encryption is useful
		Encrypts data during read and write
		This method encrypts whole tablespace
		Some restriction like index creation failure in column level encryption doesnot occur here
		Usage
			create tablespace tbs_name_encrypt
			datafile '/mnt/san_storage/oradata/oracle/encrypt_01.dbf' size 1m
			encryption using 'AES256'
			default storage (encrypt);
				-- tablespace created with encryption enabled
				
		create table in that tablespace
			create table table_name(....) tablespace tbs_name_encrypt;
			insert into table_name values (....);
	Column vs tablespace encryption
		The advantage of using TDE column is we only have the encryption/decryption overhead when accessing that column
		If there are only 1 or 2 columns to encrypt then column level encryption is best suited
		If performance is critical then it is best suited to go with column level encryption
		We add "salt" (extra data) to the column text before encrypting it. This increases the space used
			-- We have control over the salt, but by default, it will increase the data
		Cannot use column encryption to encrypt foreign key column
		Tablespace-level saves our time analyzing which columns store sensitive data
		Everything's encrypted. So we don't need to debate what to encrypt
	Encrypting data pump backups
		Encrypting the backups is as important as encrypting the data itself
		Oracle directly supports this capabilities
		Usage
			expdp system/sagarmatha@orcl dumpfile=test.dmp schemas=user_name encryption=<all/data_only/metadata_only/encrypted_columns_only>;
				-- all means both metadata and data is encrypted
				-- encrypted_columns_only means only encrypted columns will be encrypted in backup
				-- this used the password from wallet during encryption
			expdp system/sagarmatha@orcl dumpfile=test.dmp schemas=user_name encryption=<all/data_only/metadata_only/encrypted_columns_only>
				encryption_mode=password encryption_password=secret;
				-- if the dump file is imported then that specified custom password should be used in this case
				-- if tried to import without password then error thrown is 'encryption password must be supplied'
				-- encryption_password should also be provided during import
	Encrypting RMAN backups
		RMAN supports three modes of encryption
			Transparent
			Password
			Dual -> Use both transparent or password
		Usage
			rman target /
			Run following
				set encryption on;	-- instructs to use transparent encryption by default
				run
				{
					allocate channel d1 device type disk;
					backup pluggable database orcl tag=transparent_encrypted; -- instructs to backup database orcl
				}
				
				set encryption on identified by 'Secret!' only;	-- password mode encryption
				run
				{
					allocate channel d1 device type disk;
					backup pluggable database orcl tag=password_encrypted; -- instructs to backup database orcl
				}
				
				set encryption on identified by 'Secret!';	-- dual mode encryption
				run
				{
					allocate channel d1 device type disk;
					backup pluggable database orcl tag=dual_encrypted; -- instructs to backup database orcl
				}
	Restore RMAN backups
		Usage
			Running this will throw error because password is not provided for restoration
			run
			{
				alter pluggable database orcl close;
				allocate channel d1 device type disk;
				restore pluggable database orcl from tag=password_encrypted;
				restore pluggable database orcl;
				alter pluggable database orcl open;
			} -- it fails
			
			-- This is password mode backup so the password is to be passed before decrypting
			set decryption identified by 'Secret!';
			run
			{
				alter pluggable database orcl close;
				allocate channel d1 device type disk;
				restore pluggable database orcl from tag=password_encrypted;
				restore pluggable database orcl;
				alter pluggable database orcl open;
			} -- it succeeds
			
			For restoring the wallet based encrypted backup, the wallet must be open before decrypting it
			
	Opening the wallet
		If the wallet is not open and the data is selected from the encrypted table or column it will raise the error message
			'wallet is not open'
		administer key management set keystore open identified by secret_keystore_password container=all;
			-- running it will open the keystore and the above problem is resolved
			
			
		
------------------------------------------------------------ ORACLE INSTALLATION -------------------------------------
	Steps:
		i) Edit /etc/hosts file and add 	127.0.0.1	<eg:oel7.lab.local>	<eg:oel7>
			this is to resolve domain from ip and viceversa, run hostname command to resolve what to put in second parameter

		ii) Resolve oracle kernel parameters as well as the group that is to be added for the oracle functioning like 
					groupadd -g oinstall, groupadd -g dba, useradd -g oinstall -G dba oracle passwd oracle... This all will be done by below command
				yum install oracle-rdbms-server-12cR1-preinstall
				yum install oracle-database-server-12cR2-preinstall
			group and user are stored in /etc/group and /etc/passwd file respectively. command -->> id oracle --- will also display
		iii) Now create directory where oracle is to be installed. For this login as root user and enter commands
			mkdir -p /u01/app/oracle    --- oracle binary file will be stored here i.e, oracle software
			chown -R oracle:oinstall /u01
			chmod -R 775 /u01

			mkdir -p /mnt/san_storage/oradata --- oracle datafiles, redofiles etc will be stored here i.e, datafiles
		iv) Now change /home/oracle/.bash_profile
			export ORACLE_HOSTNAME=oel7.lab.local i.e, hostname
			export ORACLE_UNQNAME=orcl
			export ORACLE_BASE=/u01/app/oracle
			export ORACLE_HOME=$ORACLE_BASE/product/12.1.0.2/dbhome_1
			export ORACLE_SID=orcl
			export PATH=/usr/sbin:$PATH
			export PATH=$ORACLE_HOME/bin:$PATH

		v) Now set SELINUX to be permissive, this will print warning instead of enforcing security
			/etc/selinux/config     SELINUX=permissive
			run command -->> setenforce permissive ---- to immediately show the effect otherwise restart to see the change
		vi) If network firewalls are up and running stop them so they don't interfere
			service iptables stop
			chkconfig iptables off
		vii) Create the folder where oracle is to be downloaded
			With root user - 
				mkdir -p /orasoft
				chown -R oracle:oinstall /orasoft

	After installation --
		Creating a database instance
			Goto dbhome/bin folder and run "netca"
			Go on and add the listener, default port of 1521 can be used as the listener's port
			After finishing it -
				run 'lsnrctl' command, after that 'status' command will show what listener is currently running
		Creating a database
			Database can also be created in manual for which we have to create everything like spfile (parameters) in dbs folder,
				database itself (
					create database orcl
					darafile '..../example.dbf' size 300m autoextend on
					sysaux datafile '............' ......... on
					default tablespace user_data datafile '............' 
					undo tablespace undotbs datafile '.....................'
					logfile group 1 '......./redo1.redo' size 100m,
							group 2 '......../redo2.redo' size 100m;
				) save this  sql in some location, then connect to idle instance of database with admin and then startup nomount
				  after this run that sql file and all other sql files, 
				datafiles etc...
				This method is hectic , so prefer below options
			Goto dbhome/bin folder and run "dbca"       //// In case of upgrade run "dbua"
			While creating database select advanced mode and do accordingly
			For simplicity Create as container database is not checked, if checked then pluggable database will be created too if selected accordingly
			Oracle enterprise manager is the web interface provided through which we can manage our database as administrator
			During the setup of recovery area checking the archiving will archive the redo logs automatically
			Storage area can be setup which must be the secured one
			After filling up all the required details, the database creation and the instance creation step will begin.
		view /etc/oratab file for all the services running in the host
			orcl:/u01/app/oracle/product/12.2.0/dbhome_1:N, here N means the instance will not run automatically after reboot
			
	
		
		
		
---------------- ORACLE LINUX FIXING SWAP SIZE ----------------------------------------------------------------

	1 check what swap size
	#swapon –s
	Filename                                Type            Size    Used    Priority
	/dev/xvda3                              partition       2097144 0       -1
	  
	2 Create a file that you’ll use for swap with dd command with   as root
	dd if=/dev/zero of={/swapfile path} bs={size of swap}  count=1048576
	Example:-
	[root@localhost]#dd if=/dev/zero of=/home/swapfile bs=6048 count=1048576
	Note :- Above command will be create the 6Gb swapfile on /home location as  swapfile
	3 Set up a Linux swap area
	#mkswap /home/swapfile
	4: Enabling the swap file
	#swapon /home/swapfile
	#swapon –a
	5 status of add swap
	[root@myrem12c em12cBP1]# swapon -s
	Filename                                Type            Size    Used    Priority
	/dev/xvda3                              partition       2097144 0       -1
	/home/swapfile                          file            6097144 0       -2
	6 Update /etc/fstab   (this must to be done )
	#vi /etc/fstab
	/home/swapfile        none             swap   sw      0 0  
	Note  :- Add above line ending with  the file otherwise ones you restart the server swap partition not be mounted to the system

	Then retry the installation
		
		
		
---------------------- USEFUL LINUX COMMANDS (CENTOS) --------------------------------	
	
	wget http://get.geo.opera.com/pub/opera/linux/1216/opera-12.16-1860.x86_64.rpm
	lsnrctl status;	lsnrctl start;	lsnrctl stop;
	lsnrctl > status; lsnrctl > start; lsnrctl > stop;
	via sqlplus >
		start and stop the instance -
			alter system register;   >> force instance to connect to the listener automatically
			shutdown - nomount - mount - open
			startup
				nomount - 
					only instance is started, not connected to the database, used for database creation and recovery purpose
				mount -
					taking the instance and associating with the database
					only the dba can access database at this stage, no user can access, still closed
					for enabling and disabling archive log mode
					for renaming data files etc.
					users cannot access the database at this stage
					adding, dropping, renamind redo log files
					performing database recovery
				open -
					all users can access the database
				force -
					first shuts the database with abort and then restart the database
				open recover -
					if media recovery is required, then it will be performed automatically
				open restrict -
					opens the database and restricts the access to only some users who have restricted session privilege with them
					best to have this privilege with dba only
				NOTE:
					srvctl start database;
					****Unable to retrieve Oracle Clusterware home.
					Start Oracle Clusterware stack and try again.

			shutdown
				immediate -
					terminate all executing sql query, and disconnects the user
					uncomitted changes are rollbacked
					issues the checkpoints and closes all the databases
				transactional -
					waits for all the transactions to complete
					prevents user from starting new transactions and creating sessions
					oracle then perform checkpoint, disconnects all the users and close
				normal -
					wait for all transactions to complete
					can start new transactions by the connected user but doesnot allow new user to connect
					waits for all connected users to disconnect
					coloses the database at the end after performing checkpoint
				abort -
					everything will be stopped
					instance recovery should be performed later
			alter databse
				open read only
				open read write - default option
				open resetlogs - during recovery to reintialize archive log number
			alter system
				enable restricted session
					same as 'open restrict'
					need to kill the session if any non-dba (in the sense restricted session privilege is granted) work is going on otherwise the session won't stop
				disable restricted session
				quiesce restricted
					does not allow any non-dba connections
					wait for any active session and once they became inactive the system is quiesced
						VS in case of restriced session the active session won't get disabled automatically, manual task should be performed for this
					session resumes once the system is unquiesced
					SELECT ACTIVE_STATE FROM V$INSTANCE; -- view the state
				unquiesce
					used to unquiesce the database
					SELECT ACTIVE_STATE FROM V$INSTANCE; -- view the state
				suspend
					suspends a database by halting all input and output
					preexisting operations are allowed to complete and any new database access are placed in queue state
					if new instance is started then it won't be in suspended state beware of that (in case of cluster environment)
					can be used for copying the datafiles etc.
					SELECT DATABASE_STATUS FROM V$INSTANCE; -- view the state
				resume
					resumes halted I/O operations
					SELECT DATABASE_STATUS FROM V$INSTANCE; -- view the state

		parameters related -
			show parameter sga_target;
			show parameter sga_; >> list all the params with sg_ text as substring
			alter system set sga_target=1000m scope=memory/spfile/both;  >> memory -- cleared after restart, spfile -- stored in spfile reflects after restart, both -- both
			create pfile='/home/oracle/my_pfile.ora' from spfile;    >> this will copy all the parameters from spfile so that we can view it
			create spfile from memory; -- create spfile using the content of memory
			create spfile<=............> from pfile<=.....>; -- overriding the default by providing the file name to use
			alter system reset; --  clear the initialization parameter value. reverted back to the start up state
			
			Parameter file are of two type :
				pfile and spfile, pfile is stored as string whereas spfile is stored as binary
				pfile -> $ORACLE_HOME/dbs/init.ora
				spfile - > $ORACLE_HOME/dbs/spfile[sid].ora
			select * from v$parameter where name like '%control%'; -- comma concatenated if has many values (session level)
			select * from v$parameter2 where name like '%control%'; -- if has multiple value then splitted into separate lines (session level)
			select * from v$system_parameter where name like '%control%'; -- comma concatenated if has many values (instance level)
			select * from v$system_parameter2 where name like '%control%'; -- if has multiple value then splitted into separate lines (instance level)
			select * from v$spparameter where name like '%control%'; -- contents of spfile
		create user and permissions -
			create user user_name identified by password;   >> if connected then displays create session privilege error
			grant create session to user_name;    >> if create table is executed - insufficient privilege
			grant unlimited tablespace, create table to user_name; 
			drop user user_name;   >> only if the schema does not owns anything otherwise
			drop user user_name cascade;
			create role role_name;
			grant (roles.....) to role_name;
			grant role_name to user_name;
		data dictionary -
			v$logfile;    >> view all redo log files
			dba_data_files;    >> view data files, <file_name, tablespace_name> shows the datafile belonging to tablespace
			v$controlfile;		>> view control file.  >> if multiplexing is enabled then it will be stored in multiple places
			v$session;
			dba_tablespaces;
		troubleshoot using alert log -
			It contains all warnings, errors generated by oracle
			cd $ORACLE_BASE/diag/rdbms/orcl/orcl/trace   >> here alert_orcl.log
		storage - 
			tablespace is a collection of datafiles used as the storage
			dba_tablespaces -
				system - it is the tablespace where oracle data dictionary are stored.
			dba_data_files;    >> view data files, <file_name, tablespace_name> shows the datafile belonging to tablespace	
			create tablespace tbspc_name datafile '/mnt/san_storage/oradata/dfile01.dbf' size 20m autoextend on;
			drop tablespace tbspc_name including contents and datafiles;
		control files -
			The database control file maintains information about the physical structure of the database
			The control file is critical to recovering a data file because the control maintains the System Control Number (SNC) for each data file
			Multiplexing control file will eradicate the single point of failure issue
			CREATE CONTROLFILE is used to manually create a controlfile
			ALTER DATABASE BACKUP CONTROL FILE TO TRACE for backup
		
		password file change -
			orapwd file=orapworcl force=Y format=12.2 sys=Y input_file=orapworcl
				-- force=Y for change the same existing file
				-- input_file param transfers content of previous password file to the new one
				-- sys=Y asks for the password of new sys
			password can also be changed using alter user sys ....... command
			General:
				orapwd FILE=filename [FORCE={y|n}] [ASM={y|n}] [DBUNIQUENAME=dbname]
				[FORMAT={12.2|12}] [SYS={Y|password|external('sys-external-name')}]
				[SYSBACKUP={password|external('sysbackup-external-name')}]
				[SYSDG={password|external('sysdg-external-name')}}]
				[SYSKM={password|external('syskm-external-name')}}]
				[DELETE={y|n}] [INPUT_FILE=input-fname]
		
		
	